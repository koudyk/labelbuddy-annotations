{"text": "Mahmood, Usman and Fu, Zening and Ghosh, Satrajit and Calhoun, Vince and Plis, Sergey\nNeuroimage, 2022\n\n# Title\n\nThrough the looking glass: Deep interpretable dynamic directed connectivity in resting fMRI\n\n# Keywords\n\nDynamic directed connectivity\nInterpretable deep learning\nResting state fMRI\nBrain disorders\n\n\n# Abstract\n \nBrain network interactions are commonly assessed via functional (network) connectivity, captured as an undirected matrix of Pearson correlation coefficients. Functional connectivity can represent static and dynamic relations, but often these are modeled using a fixed choice for the data window Alternatively, deep learning models may flexibly learn various representations from the same data based on the model architecture and the training task. However, the representations produced by deep learning models are often difficult to interpret and require additional posthoc methods, e.g., saliency maps. In this work, we integrate the strengths of deep learning and functional connectivity methods while also mitigating their weaknesses. With interpretability in mind, we present a deep learning architecture that exposes a directed graph layer that represents what the model has learned about relevant brain connectivity. A surprising benefit of this architectural interpretability is significantly improved accuracy in discriminating controls and patients with schizophrenia, autism, and dementia, as well as age and gender prediction from functional MRI data. We also resolve the window size selection problem for dynamic directed connectivity estimation as we estimate windowing functions from the data, capturing what is needed to estimate the graph at each time-point. We demonstrate efficacy of our method in comparison with multiple existing models that focus on classification accuracy, unlike our interpretability-focused architecture. Using the same data but training different models on their own discriminative tasks we are able to estimate task-specific directed connectivity matrices for each subject. Results show that the proposed approach is also more robust to confounding factors compared to standard dynamic functional connectivity models. The dynamic patterns captured by our model are naturally interpretable since they highlight the intervals in the signal that are most important for the prediction. The proposed approach reveals that differences in connectivity among sensorimotor networks relative to default-mode networks are an important indicator of dementia and gender. Dysconnectivity between networks, specially sensorimotor and visual, is linked with schizophrenic patients, however schizophrenic patients show increased intra-network default-mode connectivity compared to healthy controls. Sensorimotor connectivity was important for both dementia and schizophrenia prediction, but schizophrenia is more related to dysconnectivity between networks whereas, dementia bio-markers were mostly intra-network connectivity. \n \n\n# Body\n \n## Introduction \n  \nFunctional connectivity has emerged as a promising tool for understanding the brain\u2019s functional architecture and has been widely used ( ;  ;  ; Van Den Heuvel and Pol, 2010a). Disruptions in the brain\u2019s functional connectivity are often linked to brain disorders evident in patients\u2019 behavior (van den Heuvel and Pol, 2010b). For example, schizophrenic patients have high level of functional dysconnectivity between brain networks ( ;  ;  ;  ; van den Heuvel et al., 2010;  ;  ;  ) and exhibit dysregulated dynamic connectivity across multiple brain networks ( ). Alzheimer\u2019s disease (AD) is also known to disrupt brain dynamics leading to wide-spread cognitive dysfunction ( ). \n\nThe association of brain disorders with abnormal static or dynamic functional connectivity highlights the need to develop models that can identify disorder-specific connectivity aberrations. This observation guides development of various approaches to brain connectivity analysis ( ;  ;  ;  ;  ;  ;  ;  ). However in most existing approaches, the functional connectivity matrices are not informed by the prediction task but instead estimated prior to training; thus, they depend entirely on the chosen input window of data samples. The independence from the downstream task results in inflexible estimation of connectivity matrices as the estimate is unchanged regardless of whether the task is to predict a brain disorder, age, or other quantity.   proposed a method where the functional connectivity structure is computed based on the learned representations of the data, but even this method lacks a learnable connectivity estimation method. We argue that task-dependent connectivity matrices can be estimated by a deep learning (DL) model using learnable weights. DL models are flexible in their ability to learn a variety of representations from the same data based on the architecture and ground-truth signal used in training. \n\nHowever, using a DL method to estimate a connectivity matrix can be challenging without the presence of the ground-truth graph during training. Another problem of many DL models is lack of consistency and interpretability in the learned representations. Saliency maps commonly used to address interpretability of these models ( ;  ;  ;  ) may be difficult to interpret ( ). Arguably, the difficulty of interpreting representations is the reason why studies using DL models incorporate inflexible but interpretable feature selection steps for connectivity estimation, for example Pearson correlation coefficients (PCC) ( ). \n\nIn most of the current studies, functional connectivity estimates are either static or dynamically computed using a sliding window approach dependent on the window size and stride ( ;  ;  ;  ;  ;  ). Unable to capture non-stationarity, static matrices miss essential information about dynamics. For example, dynamic functional connectivity estimates show re-occurring patterns which cannot be captured by their static counterparts ( ;  ;  ). Using a static graph learning method to capture a dynamical system may reduce classification performance ( ).   show improved results by just dynamically re-evaluating the learned static graph during testing. The improved performance for the relevant task is understandable as the dynamic connectivity provides essential information about the system, for instance, capturing re-occurring patterns. The brain\u2019s functional activity is also perceived to be highly dynamic and hence cannot be faithfully captured with a static or even window-based approach ( ). \n\nFurthermore, studies using functional connectivity to measure connectivity between brain regions or networks do not capture the direction of interaction and only measure undirected statistical dependence such as correlations, coherence, or transfer entropy. Correlation can arise for many reasons; for example, due to a common cause when an unobserved network affects two networks that are observed ( ;  ). Arguably, dynamics of interaction among brain networks is beyond simple correlations and correlation may only partially describe it. Whereas, effective connectivity is a more general way to represent dynamic and directed relationships among brain\u2019s intrinsic networks. As introduced by   effective connectivity falls into a model-based class of methods while multiple other methods, including those in the model-free class have been since developed ( ;  ; Chickering, 2002a; 2002b;  ;  ;  ;  ;  ;  ;  ;  ;  ). \n\nLike these approaches, to estimate brain networks\u2019 connectivity that is 1) directed, 2) interpretable, 3) flexible, and 4) dynamic, we have developed an approach called the Directed Instantaneous Connectivity Estimator (DICE): a predictive model to estimate dynamic directed connectivity between brain networks, represented as a dynamically varying directed graph by predicting the downstream binary label. Our model may be placed into the category of model-free connectivity methods as it does not model the data generation process. We defer to using \u201cdirected (network) connectivity\u201d (D(N)C) for the graphs that DICE estimates. \n\nUnlike existing supervised DL models that typically produce difficult-to-interpret representations, we designed our model primarily with interpretability in mind. Our model reveals what it learned about the dynamics of brain network connectivity without using post hoc interpretability methods. Effectively, we have built a \u201cglass-box\u201d layer within a traditionally \u201cblack-box\u201d DL model. In contrast to commonly used hidden layers, the \u201cglass-box\u201d layer propagates a weighted adjacency matrix of a directed graph, ensuring that it is interpretable in the context of the classification task. Hence, by estimating DC based on the task and using only the estimated connectivity structure for classification, our model learns to capture task-relevant networks and their connectivity, leading to a flexible estimation of an interpretable DC. By estimating DC instantaneously (window-size = 1), DICE removes the need for the window-size parameter used in many dynamic connectivity studies. \n\nTo thoroughly validate DICE\u2019s performance, we conduct a series of experiments on four neuroimaging datasets that span three disorders (schizophrenia, autism, and dementia) and cover a wide age range. We train the model on classification tasks for each of these brain disorders, age prediction, and gender classification, and analyze the resulting DC of the \u201cglass-box\u201d layer. Surprisingly, our deliberate focus on stable interpretable results has an enhancing side effect on DICE\u2019s predictive performance. As we show, the model\u2019s predictions are better or on par with state-of-the-art methods that were developed with a focus on classification performance rather than interpretability. We show that when learning to classify subjects based on a specific criterion, DICE estimates interpretable DCs specific to that criterion. For gender and mental disorder classification, subgraphs emphasized by the learned DCs are discriminative of gender and mental disorders, respectively. We also demonstrate that DICE learns interpretable DCs distinct to dementia, gender, and age prediction for the same subjects by enhancing connectivity for networks that pertain to the training signal. Our flexible estimation of DC structures advances the results of  , which show that functional parcel boundaries change for an individual based on the cognitive state. We show an increased utility of the inferred directionality for increasing the precision of explainable group differences. As a result, DICE can resolve more states in fMRI dynamics than is resolvable in typical dynamic functional network connectivity analyses. Additionally, DICE incorporates a temporal attention module that highlights crucial time steps relevant to the task, further improving the interpretation of predictions for the dynamics. The learned DC structures and temporal attention weights are stable and consistent across randomly-seeded trials. \n\n\n## Materials and methods \n  \n### Materials \n  \nWe use resting state functional magnetic resonance imaging (rsfMRI) data as input to our model. fMRI measures blood oxygenation level-dependent (BOLD) signal, which captures the functional activity of the brain over time. We test our model by classifying three different brain disorders, predict gender and age of subjects. For each brain disorder we perform binary classification of healthy controls (HC) and patients. Four datasets used in this study are collected from FBIRN (Function Biomedical Informatics Research Network )   project, from release 1.0 of ABIDE (Autism Brain Imaging Data Exchange )   and from release 3.0 of OASIS (Open Access Series of Imaging Studies )  . Healthy controls from the HCP (Human Connectome Project) ( ) are used for gender prediction. Refer to   for details of the datasets. \n\n#### Preprocessing \n  \nWe use two typical brain parcellation techniques; independent component analysis (ICA) and regions of interest (ROIs) based on a predefined atlas. The preprocessing pipeline used depends on the parcellation technique and the pipeline used in state-of-the-art studies for the dataset. All the preprocessing was done before training the model. \n\n##### ICA parcellation: \n  \nFor all experiments conducted using ICA as brain parcellation technique the fMRI data was preprocessed using statistical parametric mapping (SPM12,  ) under the MATLAB 2021 environment. A rigid body motion correction was performed to correct subject head motion, followed by the slice-timing correction to account for timing difference in slice acquisition. The fMRI data were subsequently warped into the standard Montreal Neurological Institute (MNI) space using an echo planar imaging (EPI) template and were slightly resampled to 3 \u00d7 3 \u00d7 3 mm  isotropic voxels. The resampled fMRI images were then smoothed using a Gaussian kernel with a full width at half maximum (FWHM) = 6 mm. \n\nWe selected subjects for further analysis ( ) if the subjects have head motion \u2264 3\u00b0 and \u2264 3 mm, and with functional data providing near full brain successful normalization ( ). 100 ICA components are estimated using a novel fully automated Neuromark pipeline \u201cneuromark_fmri_1.0\u201d  described in  . This method is capable of capturing robust imaging features that are comparable across subjects, datasets, and studies, which is beneficial for those studies need replication. The Neuromark framework leverages an adaptive-ICA technique that automates the estimation of comparable brain markers across subjects, datasets, and studies. A set of component templates were used as references to guide the estimation of single-scan components for the data. These component templates were created via a unified ICA pipeline. They were constructed using an independent resting-state fMRI data with large samples of healthy subjects from the genomics superstruct project (GSP). The GSP data include 1005 subjects\u2019 scans that passed the data QC. High model order (order = 100) group ICA was performed on the GSP data, and then the independent components (ICs) from the GSP data were used as the references to extract components for each dataset used for experiment in this study. The Neuromark framework extracts the components for each subject respectively, which means that the estimation of features of each subject is not influenced by the others. However, the choice of components (and number of components) can influence accuracy, but our study is not focusing on determining the best number of ICs rather use the available components and let the model decide the task-dependant components. \n\n\n##### Region parcellation: \n  \nState-of-the-art methods use different preprocessing pipelines for different datasets. For comparison with these methods on HCP, ABIDE, and FBIRN datasets, we select the same preprocessing pipelines as in the relevant comparing method. We use the HCP ( ) data which was first minimally pre-processed following the pipeline described in  . The preprocessing includes gradient distortion correction, motion correction, and field map preprocessing, followed by registration to T1 weighted image. The registered EPI image was then normalized to the standard MNI152 space. To reduce noise from the data, FIX-ICA based denoising was applied ( ;  ). To minimize the effects of head motion subject scans with framewise displacement (FD) over 0.3mm at any time of the scan were discarded. The FD was computed with fsl motion outliers function of the FSL ( ). There were 152 discarded scans from filtering out with the FD, and 942 scans were left. For all experiments, the scans from the first run of HCP subjects released under S1200 were used. ABIDE ( ) was pre-processed using C-PAC ( ). The preprocessing includes; slice time correction, motion correction, skull striping, global mean intensity normalization, nuisance signal regression, band pass filtering, and finally functional images were registered to anatomical space (MNI12). After preprocessing using C-PAC, 871 out of 1112 subjects were chosen based on the visual quality, inspected by three human experts which looked for brain coverage, high movement peaks and other artifacts resulted by scanner ( ;  ;  ). To pre-process FBIRN data, SPM12 pipeline was used as explained in previous section with few extra steps. After the smoothing using a Gaussian kernel, the functional images were temporally filtered by a finite impulse response (FIR) bandpass filter (0.01 Hz-0.15 Hz). Then for each voxel, six rigid body head motion parameters, white matter (WM) signals, and cerebrospinal fluid (CSF) signals were regressed out using linear regression. \n\nWe used two atlases for brain parcellation;  , and Harvard Oxford (HO) ( ) with 200, and 111 regions respectively. For each region, average value is computed for all the voxels falling inside a region, thus resulting into a single time-series for each region. After dividing data into regions, each time-series was standardized by their zscore having zero mean and unit variance. \n\n\n\n\n### Method \n  \nOur DICE model recieves the time-courses of the ICA components or ROIs represented as a matrix of size   N   *   T   (Number of components/ROIs * Number of time-points) and learns a set of   T   directed graphs representing the dynamic DC or DNC between spatial components (e.g., ICA-based spatial components, regions from an atlas), which we designate as nodes of a graph by predicting the binary labels. Let   G   represent the set of graphs where   G   = {  g  ,   g  , \u2026   , g  } where   T   is the total time-points and   g   = (  V  ,   E  ), where,   V   and   E   represent the nodes and edges present at time-point   t  . To create the graph   g   we first use a bidirectional long short-term memory (biLSTM) ( ) module to create the embedding   of node   i   at time   t  . We then use a self-attention module ( ) which takes all such embeddings at each time   t   and create a weight matrix among nodes thus providing the DC (graph) between nodes at each time-point. To create a final graph   G   for downstream classification, we use a temporal attention model that assign a weight to each   g   and compute the weighted sum of the set   G  . We explain the working and purpose of each module in detail in the following sections.   shows the complete architecture. \n\n#### biLSTM \n  \nThe time-point value   for node   i   at time   t   can be effected by many different factors and relations. Capturing these relations can increase model interpretability and improve downstream classification performance. In a time-series (fMRI data), one of these factors is the values/data at previous time-points  . In fMRI data, this relationship is unknown and is hard to capture and hence cannot be computed using a fixed method/formula (hand-crafted features). The difficulty is further increased by a) low temporal resolution of fMRI data and b) the fact that it is unknown how farther in time the effects of a time-point remains in a time-series. These effects are different for each subject and can even vary among nodes of the same subject. LSTMs have proved to be extremely effective for time-series/sequence data where the model takes an input from a sequence at time-point   t   and create representation for current and also predict representation for future time-courses based on the representation of previous time-points. LSTMs learn the temporal relationships between data through the cell\u2019s memory and forget gate. These gates are optimized on the data and downstream task (ground-truth signal) and the relationships between data are learned instead of computed. The working of the LSTMs can be explained by the following set of equations.   \u03c3   represents sigmoid activation, and \u2299 is the Hadamard product ( ). \n\nIn the above equations,   i  ,   f  , and   o   represent the input, forget and output gates at time   t   respectively.   c   represents the cell state (memory),   g   represents candidate for the cell state, and   h   represents the representation/embedding for the input at   t  .   W   and   W   represent the weights for the input and hidden vectors for the respective gate   x   \u2208 {  i  -input  , f  -forget  , o  -output}. Similarly   b  ,   b   are the biases for the respective gate   x   \u2208 {  i, f, o  }. We use a biLSTM to create representation   h   for each node   i  . Thus  ,   and  . Here   and   are representation for forward and backward pass. We use LSTM for each node (component/region) individually, sharing weights of LSTM among the nodes. As shown in  , LSTM\u2019s usually take a vector   x   as input at each step, however, we give   (scalar value) as input to the LSTM along with hidden vector and receive   for the node   i   at time-point   t  , which solves the window size problem occurring in dynamic-FNC studies. To make it easier to understand, one can assume that in our model the window size is 1. This allows us to later instantaneously compute connectivity matrix (links/edges) between the nodes at each time-point. The biLSTM receives temporal values of each component/region separately but share the weight matrices across regions. This allows the biLSTM to learn the temporal connections by looking at multiple nodes but does not learn spatial dependencies among nodes. For this exact reason we use self-attention across nodes. \n\n\n#### Self-Attention \n  \nA node in a graph can be linked with other nodes represented as the edge connectivity between them. The connectivity between nodes influence the value of a node   at a certain time-point. Thus it is important to measure the connectivity between nodes for the construction and interpretation of the graph. In our fMRI data where each   x   is a brain region/component, capturing the DC or DNC between nodes shows how brain networks are linked with each other and the direction of flow of information between brain networks. The estimated matrices can then be used to explain brain working and brain disorders. Connectivity between brain regions is independent of the structural connectivity and thus is unknown. To capture the directed connectivity between brain regions, we use a self-attention module. \n\nSelf-attention module captures the weights between   n   inputs of a sequence. Since in a dynamic system (brain network), the connectivity between nodes can change at any instance, therefore, at each time-point   t   we pass a sequence of   n   vectors  ,   n   = total nodes, as input to the self-attention module and create the weight matrix   W  , where each   is the connectivity weight matrix of input nodes at time-point   t  . \n\nThe self-attention module creates three embeddings, namely, key (  k  ), value (  v  ), and query (  q  ) and creates new embeddings for each input using these embeddings. The following set of equations can sum up the whole process. For simplicity, we omit the   t   from these equations.   represents transpose and \u2295 represents concatenation. \n\nHere   is the connectivity matrix between   n   nodes in the graph. As brain disorder are associated with disruptions in the connectivity of brain\u2019s intrinsic network, we only use our learned directed connectivity matrices   W   for downstream classification and not the features, thus forcing the model to estimate the differences in connectivity between the two classification groups (e.g., HC and patients). As DICE is tuned to estimate the DC or DNC for the groups of subjects and output the it, DICE captures and shows the basis of downstream classification. The DC or DNC estimated by the model can be easily represented as a graph which are extremely easy to interpret. The self-attention glass-box layer shows task-dependant nodes (brain regions) and their connectivity. \n\nThe features that represent time-courses are used to learn/estimate the DC or DNC structure. As the true connectivity/graph structure is never available in many applications to directly compare with, we propose that a connectivity matrix leading to state-of-the-art classification performance makes it more reliable than using the representations/embeddings for classification. \n\n\n#### Temporal attention \n  \nAs we use only the connectivity matrices learned by the model for downstream classification. For this purpose, we need to create a single weight matrix   W   based on the   W   matrices. For the downstream classification task, not all the time-points are equally important, hence it is crucial to incorporate a temporal attention module which assigns weight to each   W   and calculate a weighted average of all the weight matrices. We introduce a novel temporal attention module which we call global temporal attention (GTA). \n\n##### GTA: \n  \nTo give the attention module a global view of the graph, we present GTA. The global view allows the model to learn how each DC contributes to the global graph or structure of the data in the downstream task. We create an average of all the   T   DC and call it   W   representing the global view. We then compare the similarity of each local   W   with the global view and use them to create the temporal attention vector    \u03b1   .   shows the architecture details. \n\nHere \u2299 is the Hadamard product ( ) between matrices.   W   is computed as:\n \n\n\n\n\n### Training \n  \nWe used GTX 2080 with PyTorch as ML framework for our experiments. The hidden dimensions for the biLSTM was set to 100, whereas, self-attention including key, query, and value modules, were all set to 48. The dimensions of multi-layer perceptron (MLP) layers for calculating temporal attention vector were   \u03b7   *   len  (  flat  (  W  )),   \u03b7   *   len  (  flat  (  W  )), and 1 with   \u03b7   =   \u03b7   = 0.05. We noticed in our experiments that multiple heads of self-attention increases stability of the estimated DC. We used batch normalization after the first MLP layer. ReLU activation was used in our model between the MLP layers. A final two-layer MLP was used to get logits for binary classification problem with   W   as input with dimensions 64 and 2. We used cross-entropy loss with Adam optimizer. Let   \u03b8   represent the parameters of the entire architecture,   being the predictions and   y   the true labels, the loss is calculated as:\n \n\nWe also experimented with additional loss terms to encourage the model to estimate connectivity matrices where the values of the main diagonal are closer to 1. Please refer to   for details. We used L1-regularization to get a sparser solution.   \u03bb   (regularization weight) was set as 1  e   and learning rate was 2  e  . Based on the experiment, we reduced the learning rate either when validation loss reached plateau by a factor of 0.5 or exponentially with   \u03b3   = 0.99. Early stopping was used to stop training the model based on validation loss and patience of 25. For each dataset (ICA components or ROIs), to have a fair result, we perform n-fold testing where the value of n depended on the dataset and methods we compared against. For each test fold we performed experiments with 10 randomly-seeded trials. We report the mean AUC-ROC (Area Under Curve - Receiver Operating Characteristic) across the n test folds and the 10 randomly-seeded trials as it is a more reliable metric than simple accuracy for binary classification tasks. For example, for FBIRN data we had 18 test folds and for each fold we performed 10 trials, which gives us a list of 180 AUC-ROC values and we report the average of these values. In some cases we also report other metrics as well, such as accuracy. Due to the size of the data, we made some hyper-parameter changes for HCP region-based (ROIs) experiments. The hidden dimension size for bilstm and self-attention module was set to 64 and 32.   \u03b7   was set to 0.005. Furthermore, because of memory constraints encountered during HCP region experiments, during both training and testing we divide the total time-points (1200) into a set of three, each having 400 time-points. We create logits for all and compute the mean to get final logits. Batch size was set to 32. \n\n#### Hyper-parameters selection and fine-tuning \n  \nAll the parameters (hidden dimensions, number of layers,   \u03b7  ,   \u03b7  ,   \u03bb  , learning rate,   \u03b3  , patience, batch size) mentioned in   were set as hyper-parameters. We fine-tuned these hyper-parameters based on the average performance of the model on validation dataset across all the folds. We did not perform hyper-parameters tuning based on the test folds and we report only test-set results. We also want to note here that we permuted the order of subjects for each dataset and performed the experiments using the permuted order. This was done to avoid imbalance of subjects in the folds. On the same lines, when dividing the data into n-folds (test folds) we tried to balance the number of subjects of both classes in each fold. For example, in case of FBIRN data with 311 subjects and 151 and 160 subjects in class 0 and 1 respectively. When performing 18 fold testing, each test fold consisted of   subjects from class 0 and   subjects from class 1 and the rest of the data was used for training and validation, where we kept the validation set size same as the test set size. The validation set was used for hyper-parameters tuning, early stopping during training and selecting the model to apply on the test data. We made sure that no subject (or sessions of a subject) repeated across training, validation and test sets. The exact size of training, validation and test set can be calculated using the criteria mentioned above and the total number of subjects and number of folds mentioned in  . In some of the experiments keeping the same number of subjects in each fold created a small data leakage at the end. For the results reported, the maximum leakage was for FBIRN dataset with 18 test folds. For this purpose, we performed another experiment on FBIRN dataset where the last fold had all the left out subjects to prevent any data leakage. This had no effect on the performance of the model. Refer to Table A.11 for results. \n\n\n\n\n## Experiments \n  \nTo test if DICE accomplishes all the goals, we perform detailed experiments by classifying three brain disorders, classify male and female groups for HCP and OASIS subjects, and predict age for OASIS subjects. We perform experiments for all datasets using ICA time-courses and perform experiments on FBIRN, ABIDE and HCP data using regions-based (ROIs) data as well. In this paper we refer to matrices capturing functional connectivity between networks at a whole-brain level as functional network connectivity (FNC) (Allen et al., 2011b;  ) and when operating on ROIs \u2013 as FC. We report the average results for all the trials. Depending on the experiment, we compare our classification results with state-of-the-art DL methods ( ;  ;  ;  ;  ;  ;  ; Zhang et al., 2018a) and ML methods (Support Vector Machine (SVM), Logistic Regression (LR)). To avoid any discrepancy we report the results of the DL methods directly from the published studies, even though some studies use test data instead of validation data for selecting the best performing model/parameters. For ML methods we used the python package Polyssifier  which selects the best model/parameters based on the performance on validation data. \n\nTo show the efficacy of our model, we divide our results into three broad categories. In the following sections we show a) classification performance of our model, b) learned DC and DNC and c) the effects of temporal attention module. \n\n### Classification \n  \n shows the classification performance of our model using ICA data,   shows the performance using region-based (ROIs) data of FBIRN and HCP, and   shows results on ABIDE region-based (ROIs) data. \n\nOur model beats every state-of-the-art method used for comparison in this study in almost every metric for both ICA and region-based (ROIs) fMRI data across all datasets when using similar input data (fMRI). As our model does not use phenotypic information about subjects, it lacks behind ( ;  ) on ABIDE.   reports a decrease of ~ 2.5 AUC by using a different phenotypic information which clearly shows the dependence on phenotypic data. Whereas,   reports much lower AUC score by using only fMRI data. ML methods fail completely even on ICA data, We attribute this failure to two reasons. 1) The number of dimensions (  m  ) being much higher than the number of subjects (  n  ), thus creating the curse of dimensionality (  m   >>   n  ) and 2) The ML methods do not compute a graph structure for estimating the connectivity between the networks/components and instead mostly work with independent networks/components. According to our knowledge, no other model gives such high classification score across four neuroimaging datasets. The high classification score of the model computed using only the learned DC structure increases the confidence in the correctness of the learned DC structures. \n\n\n### Directed connectivity \n  \nThe learned interpretable, task-dependent (flexible) directed connectivity structures by our model is the most important contribution of our work. As this is a novel work, we show in detail, different aspects of the learned connectivity structures. We a) compare our learned DNC with FNC computed via PCC, b) compare the differences in DC and DNC between multiple classification groups, c) show how direction matters in connectivity, something which is not captured by FC and FNC, d) dive into the fact mentioned in introduction that unlike computed FNC (using PCC) our learned DNC is task dependent and changes based on the downstream task (ground-truth signal) and e) show the dynamic connectivity states for FBIRN data for HC and schizophrenia (SZ) subjects. All the aspects (a-e) discussed in detail in following sections show the correctness and interpretability of the learned DC and DNC. The interpretability of the connectivity matrices estimated by our model give insight into how brain networks are linked with each other and with the downstream classification task. This is very crucial to understand brain disorders and relevant brain networks. Unlike typical FC and FNC which ranges from \u22121 to 1, our learned matrices are based on attention and hence ranges from 0 to 1. More information on this in  . \n\n#### DNC vs FNC \n  \nAs the true connectivity between brain networks is not known, we compare our learned DNC with FNC.   shows the DNC learned by our model and the FNC computed using PCC using ICA components for FBIRN dataset. The DNC is   W   explained in  . Both DNC and FNC is the mean matrix for highest performing fold of FBIRN dataset with 16 subjects. The 100 ICA components are divided into informative (53) and noise (47). We show the connectvity between 53 non-noise components. These components are further divided into 7 domains/networks following (Allen et al., 2011a). Both matrices clearly show high intra-domain connectivity. The learned DNC shows similar pattern of FNC which increases the confidence in the DNC learned by our model but there are very important differences between the two.   Inter-network connectivity:   We see that our estimated DNC finds much more inter-network connectivities than the FNC which is mostly intra-network and has very low scores between networks.   Directionality:   Regarding the direct influence, DNC estimated by our model is directed and shows components in visual affecting components through out the domains, such information is not present in the FNC which is un-directed (symmetric across main diagonal) and does not show the direction of connectivity. Refer to   for more detail on this. \n\nTo compare the connectivity matrices in terms of classification results, we use an LR model and perform classification by first training and testing the model using PCC-based FNC and then by our estimated DNC as input. Refer to   for comparison. \n\n\n#### Directed connectome \n  \nCapturing directed connectivity is one of the methods to understand the direction and flow of information in the brain. Learning the direction of connectivity is one of the main advantages of our model as it might explain the direct influence of brain networks upon each other. To show the direction between components, we divide the DNC of FBIRN subjects into two connectomes showing the direction.   left shows the edges from   a   to   b   where   a   >   b  . For example the edge between (8,23) shows the edge from 23 to 8, whereas,   right shows the opposite. It is clear from the figure that direction matters and the connectivity between brain regions is beyond simple statistical dependence. For example,   shows that the components in visual network (VIN) affect components in other networks and the edges in the opposite direction are relatively much fewer. We also see direction of connectivity from cognitive control (CC) to sensorimotor (SM). Existing studies ( ;  ;  ) show that cognitive control is responsible for activities like attention, remembering and execution, things which are required when doing a motor task controlled by sensorimotor. Such directionality is important to study brain\u2019s working in more detail and is not present in FNC used by existing methods. The results are further discussed in  \n\n\n#### Connectivity differences among groups \n  \nAs hypothesized that brain disorders are linked with the connectivity of brain\u2019s intrinsic networks, we show how the learned DC and DNC changes for subjects belonging to different groups.   shows the DNC estimated by our model of HC and SZ subjects for FBIRN data whereas   shows DNC of male and female groups for OASIS dataset. Both results are computed using ICA pre-processed data. For ICA based DNC, there are similarity between the two matrices as they come from the same joint ICA. However, there are visible difference between the two for multiple networks like visual (VI), cognitive control (CC), default-mode (DM) and cerebellum (CB). The biggest difference between HC and SZ groups seems to be in the connectivity strength for VIN. For OASIS results   we see that females show high connectivity scores in default-mode network (DMN) compare to males and low sensori-motor network (SMN) connectivity compare to males, this has been verified by existing studies ( ;  ;  ;  ). To verify this by numbers, we use statistical testing to compare the two groups (male, female) and compare average connectivity for male and female in DMN and SMN.   shows the statistical results. \n\n performs the same experiment for region-based (ROIs) data. Here the regions for both sides of the brain (left and right) are divided into 7 domains following shaefer ( ). Again, in   for HC we see high connectivity score between regions of the same network. We also see connectivity between regions of same network across left and right side of the brain. The diagonals on top and bottom of the main diagonal shows this. Whereas the DC of SZ subjects is weakly connected compared to HC and is mostly shows intra-network connectivity. The sparsity explains and support the existing literature explaining SZ as functional dysconnectivity between brain networks ( ;  ;  ; van den Heuvel et al., 2010;  ;  ;  ). \n\n compares male and female groups based on region-based (ROIs) HCP data. We see similar patterns of hyper-connectivity of DMN and hypo-connectivity of SMN in females as compared to males. As the region-based (ROIs) parcellation divides the brain into left and right, we also see that females have high intra-network connectivity between left and right side of the brain as compared to males. \n\nTo verify the visual results, we use statistical testing to compare the DMN and SMN between males and females. The stats confirm the visual results with 1) female DMN showing higher connectivity than female SMN and male DMN, and 2) male SMN showing higher connectivity than male DMN and female SMN. We also see that the networks are highly statistically different. Refer to  . \n\n\n#### Task dependent DNC \n  \nHuman brain can be divided into multiple parts/regions where each region is linked with a set of tasks. For example, the hippocampus is associated with memory. Thus it is important to know which region/network(s) are linked with the downstream task (e.g. disorder classification). Finding the linked regions/networks would help us understand the disorder and allow to study the association of these regions/network(s) with the disorder in more detail. In this section, we see how the DNC structure learned by our model changes and identifies different networks for the same subjects based on the downstream task. For this purpose, we perform an experiment, where we compare the estimated DNC for OASIS data when predicting dementia, age and gender of the same subjects. The number of subjects were balanced with both HC and patients equalling 50% of the total subjects but had ~ 62% female subjects.   shows that our model produces task dependent DNC and the networks/domains showing high connectivity for each task adheres to the existing literature. The   shows the DNC learned when classifying subjects for dementia. We see high connectivity for components in the SM, DM, and CB networks. These networks are linked with dementia in existing literature, which support the results of our method. Whereas when classifying gender of same subjects, the estimated DNC is different and show high connectivity for components in DM and reduced connectivity for SMN.   shows the FNC computed via PCC for the same subjects. As FNC computed using PCC is only data dependent, the FNC would remain same for all the tasks and shows the inflexibility of the method.   therefore shows a) our model learns task dependent DNC and b) our model accurately finds networks linked with the downstream classification task. We see this as a significant advantage over studies which compute a fixed/static FNC using PCC and hence is independent of the downstream task. We see that   which is the learned connectivity structure when predicting age does not show high connectivity between networks and the connectivity values for SMN and DMN are almost same. This could be a reason of small age variance in the dataset. \n\nWe use statistical scores to verify the visual results.   shows the statistical difference between the three DCs as a whole and between DMN and SMN. We also compare the estimated DCs with FC  . \n\nWe see that all three DNCs are extremely statistically different. It is also proven that DMN is given higher connectivity scores for gender prediction whereas, SMN connectivity is much higher when predicting dementia comparing to gender and age prediction tasks. To clear how the connectivity values change for DMN and SMN we point out the average connectivity scores of the networks for dementia and gender classification and compare it with the values of DMN and SMN computed via PCC. The connectivity values in FC for SMN and DMN are 0.580 and 0.487 respectively (and would remain same irrespective of the classification task). Whereas, when classifying dementia our model show much higher SMN average value of 0.64 and a little decreased value of 0.478 for DMN showing a focus on SMN despite having more female subjects in the test set. When predicting gender for the same subjects the DNC estimated by our model has a decreased SMN value of 0.555 and increased value of 0.527 for DMN hence focusing less on SMN and more on DMN when compared to the dementia classifying task thus verifying that our estimated DCs are task-dependent and not only data dependent. We discuss the meaning and significance of this result in  . \n\nTo see the matrices as graph of nodes (regions) and edges (connectivity), we plot   and   on the brain and show the results in  . The figure shows high number of nodes and edges among components of VIN and SMN and among the two networks for dementia classification  , and high number of nodes and edges among components in DMN for gender classification  . \n\n\n#### Dynamic connectivity states \n  \nStudies like ( ;  ;  ;  ) show that human\u2019s brain FC is dynamic and can be used to find patterns which are not visible in static FC studies. These studies show that dynamic FC show re-occuring patterns. To study these patterns, dynamic connectivity of the human brain is divided into distinct   k   states ( ;  ;  ). There are multiple methods proposed to find the   k   states with k-means being one of the most used methods. These studies show that the transition and time spent in each state is different for patients (SZ, dementia, autism) and HC. To validate our results and to find such patterns we use k-means to find   k   (5) such states using the DCs estimated by DICE for FBIRN dataset. We calculate and compare the time spent by both groups (SZ and HC) per state. \n\n shows that SZ subjects spend more time in weakly connected states (1,3) than HC which stay in states which show high connectivity score for visual (VI) and sensorimotor (SM). We also see that HC tend to change state more often than SZ which spend ~ 66% time in one state (number 3). Existing studies (Miller and Calhoun, 2020a; 2020b;  ) show that window-less approach can find dynamic patterns that are not captured by the vastly used window-based approach. As DICE is an instantaneous model, we investigate if DICE can capture more dynamic states than the window-based dynamic-FNC studies. For this purpose, using elbow method ( ), we found that the best   k   for the estimated DCs is not 5, and set   k   = 10 and show the resultant states in  . We see the model captures additional states that were not visible with   k   = 5. The additional states found show the pattern of directionality, specially in the states where HC spend more time than SZ. For example, in  , state 2 show dense connectivity for components in VIN and the direction is from VI to other states, and state 5 show similar direction but with sparse connectivity.   captures the additional state (9) which shows the opposite direction, that is, VIN has mostly incoming edges. We believe this state represents the brain activity when different networks (e.g. SMN) are giving input to VIN to control the vision. We discuss this result in  . \n\n\n\n### Temporal attention \n  \nOur temporal attention module finds the important time-points that are relevant for the downstream task (e.g. gender prediction). As not all time-points are equally important for the downstream task, and fMRI data has low temporal resolution, the temporal attention is an effective way of finding important bio-markers for neuroimaging dataset. Finding the relevant time-points can help reduce the data and allow to focus on activities at specific points.   shows the weights assigned to the subjects of FBIRN. \n\nWe show weights for 16 subjects (8 per class) with 10 randomly-seeded trials. The results show that the temporal attention module is very stable and assign similar weights to the time-points for every trial. \n\nTo further check the correctness of the time-points selected by our model and how these time-points are useful in terms of classification performance, we perform an experiment where after training the model, we use   W   of the top 5% values to train an LR model and then use the top 5% time-points of the test data to test the model. Similarly we perform experiments for bottom 5% values as well.   shows the comparison for the three brain disorder dataset. The results show that the LR model provides high AUC score by just using 5% of the important time-points. Thus, it proves that a) not all time-points are important for classification of the downstream task and b) our model accurately finds the important time-points. We use an LR model for this experiment to show that the learned top and bottom 5% values are not limited to our DICE model but is generalized such that an independent LR module gives high classification performance using the top 5% data identified by our model and does not learn on the low 5% data. Finally, our experiments also show that not using the temporal attention reduces the model classification performance by upto 10% A.12. \n\n\n\n## Discussion \n  \nOur experiments revealed a number of interesting properties of DICE and uncovered some interpretable directed connectivity graphs that we feel are of high utility for the neuroimaging field. As supported by results, models with glass-box layer like DICE have a high potential for studying resting-state dynamics of the brain. In the following, we discuss the most pertinent results. \n\n### Inter-network and directed connectivity \n  \nResults in   and   show that DICE infers DNC that agrees with the essential findings of the FC studies ( ;  ;  ;  ;  ;  ;  ;  ;  ) and provides two additional aspects: inter-network connectivity and direction of connectivity. The inter-network connectivity is of great significance as the brain is not made up of isolated networks and many tasks require information passing and neurons firing through multiple networks. Thus making it crucial to find how these networks are connected to each other if connected at all for patients and controls. Capturing the dysconnectivity between networks for patients can lead to knowledge discovery about the functionality of the human brain and the effects of brain disorders on it. Furthermore, finding directionality between networks is also of great significance. We showed in experiments that our model captures the direction of connectivity between networks. The direction of connectivity from VI to other networks, and from CC to SM networks is justifiable. Existing studies ( ;  ;  ) show that cognitive control is responsible for functions like attention, remembering, and execution. These functions are often required when doing a motor task controlled by sensorimotor, which hints at the direct effect of the CC network on the SM network, captured by DICE. Regarding VI and other networks, we know that VI is mostly a means of input (visuals) to our brain, which is then processed by different parts of the brain. Thus, most of the flow of information is from VI to other networks and few in the opposite direction, which is required to control VI for accomplishing different motor tasks controlled by SM. Therefore, our experiments also show that most incoming connections to VI are through the SM network, thus accurately capturing the flow of information between networks. This flow of information is not captured in simple correlations. We believe these two aspects are crucial to understanding brain working and are currently missed in connectivity estimation methods such as FNC. \n\nDirected connectivity directed influence of an intrinsic brain network on other networks. Estimating the direction of connectivity may simplify targeted interventions that are instrumental in establishing causal relations. Capturing causality between networks further helps to understand complex systems and answer counter-factual questions ( ), and is left to future work. Our model finds non-negative relations between components/nodes, which we consider dependencies or relevance rather than correlations. However, we understand that the negative correlations in FC and FNC are also helpful and provide descriptive information. We think it might be an easy fix to incorporate negative relations in connectivity matrices estimated by DICE. We discuss this in  . \n\n\n### Interpretability \n  \n shows how the DC and DNC estimated by DICE are interpretable in how accurately they capture the difference in connectivity between 1) schizophrenia patients and controls and, 2) male and female groups. In classifying schizophrenia patients from controls, our model learned the most significant differences were in the VI, SM, and DM networks. Controls show robust connectivity of VI and SM with each other and with other networks, which is missing for SZ patients. The finding of dysconnectivity and/or lower connectivity scores for VI and SM networks for SZ patients is not surprising as there exists ample evidence in prior studies of schizophrenia leading to multiple abnormalities related to visual and motor functions such as perception of contrast and motion, detection of visual contours, and control of eye movements to name a few ( ;  ;  ;  ). These abnormalities certainly affect motor skills which we feel is a reason for the low connectivity for SM and VI networks captured by our model for SZ patients. DICE also captures hyper-connectivity in DMN for SZ patients which is reported by existing studies ( ). \n\nWhereas in classifying gender in the same dataset, DICE emphasized hyper-connectivity in the DM network and hypo-connectivity for the SM network for females compared to males. The differences captured in the DC and DNC for both tasks are supported by existing studies ( ;  ;  ;  ;  ;  ;  ; van den Heuvel et al., 2010;  ;  ;  ) that show the role of the DMN in gender classification and VI dysconnectivity for schizophrenic patients. Similarly to existing studies ( ; Zhang et al., 2018b), DICE shows that female subjects have higher connectivity between the contralateral homologue brain networks relative to males. \n\nDL models are commonly viewed as black-box models because of the difficulty of interpretation and not easily explained performance on the tasks they are trained on. These models can show excellent performance on tasks such as classification based on the reasons that are not substantially revealing about the input data nor their dynamics. One reason is shortcut learning ( ): a DL model can classify images with or without airplanes with high accuracy by paying attention exclusively to the background (blue sky). Although predictive, such models cannot help in knowledge discovery. To control for shortcut learning we would like to be able to see why predictions are made. One approach is making DL model interpretable. For that a posthoc method is often used, e.g., saliency maps ( ;  ;  ;  ). Such methods explain the input data by finding which part(s) of the input the model is most sensitive to. Saliency maps have shown some good results in computer vision tasks in 2d images. The use of saliency maps in neuroimaging and temporal data has different challenges ( ) as the output maps are noisy, difficult to interpret and does not provide good boundaries nor the connection between different salient regions. Selection of the method for obtaining saliency maps is also something to consider as some of the methods are architecture based. Hence, using saliency maps to get task-specific brain\u2019s connectivity graph is not feasible using current methods. To overcome the black-box nature of DL models and avoid using a posthoc method, we focused on the interpretability of the model\u2019s results. For this purpose, as brain disorders are commonly associated with disruptions in the connectivity pattern of brain networks, we use only the learned connectivity matrices by our model for the downstream classification or prediction tasks, thus making the model extract the abnormality in connectivity relevant to the ground-truth signal. One way to conceptualize about our approach is to think of the generated DC and DNC as a \u201cglass-box layer\u201d (clear and interpretable) layer as noted in  . This approach combines flexibility (the layer is trainable) with interpretability and enables the model to capture differences in the connectivity of the groups in classification task. Regression is also possible with our approach, although we leave it for the future work. Our \u201cglass-box layer\u201d approach enables learning the essential networks and their connection to other networks relevant to the training signal and directly output that without using a posthoc method. As the DC and DNCs estimated by our model are based on learnable functions, the output matrices can have slightly different values when the model is retrained, which is an attribute of DL models. Therefore, all the connectivity matrices shown in the paper are averaged over several randomly-seeded trials. \n\n\n### Task-dependent flexible DNC \n  \nWe fully utilize the flexibility of our DL model to learn task-dependent (ground-truth signal) directed connectivity structures. We show in   that our model estimates DNC structures for the same subjects that are distinct to the ground-truth task of dementia, age, or gender. Hence our model can show the networks and their connectivity crucial for specific downstream tasks. The networks identified by the model through the learned DNC for dementia classification (SM, CB, VI) match the results of prior studies ( ;  ;  ;  ;  ). Whereas, for gender prediction, the most prominent network identified by the network was DM, which again matches existing literature ( ;  ;  ;  ). We feel this is a strong validation of the ability of DICE to find disorder-dependent networks and connectivity patterns. We showed in   that our model focused more on SMN than DMN despite having almost two-thirds of female subjects in the test set. This result is significant because the model learned that the SMN connectivity, is more important than DMN for the downstream task of dementia classification and hence enhances the signals for SMN. This eliminates the need to acquire strictly matched subjects with only the difference(s) for which you want to find the relevant networks and connectivity. For example, when trying to find the networks related to schizophrenia using PCC, one needs to find two groups (schizophrenia patients and controls) that do not have any other differences. Extraneous differences would create ambiguity regarding whether the networks identified are related to the disorder (schizophrenia) or some other difference, e.g., gender. Instead of explicitly confronting the confounding factors by regressing them out or taking equivalent measures, DICE performs the \u201cde-confounding\u201d implicitly based on the training labels. \n\nAnother notable property of our model is that it finds the relevant networks and the connectivity structures (sub-graphs) without receiving them during training, making DICE a self-supervised graph learning model. \n\n\n### Dynamic DNC and temporal-attention \n  \nAs hypothesized, and shown in previous studies ( ;  ;  ;  ) results in   show that connectivity between brain\u2019s intrinsic network is dynamic, and dynamic connectivity can capture patterns which are missed by static models. Notably, controls and SZ patients spend different amounts of time in each state 10. Controls spend more time than SZ patients in strongly connected states, especially for visual and sensorimotor networks. On the other hand, SZ patients spend time in weakly connected states and do not often spend time in other states. Similar patterns were observed in FNC studies ( ;  ;  ;  ;  ). \n\nMoreover, using all subjects in the FBIRN ( ), our model finds additional states doubling the state resolution. We explain this temporal resolution increase by instantaneity of directed connectivity estimation in DICE in contrast to using a sliding window. Therefore, estimating connectivity instantaneously makes the model robust and finds patterns that are missed when using a window-based approach. Another explanation and an additional factor is the increased richness of representation via a directed graph - the connectivity matrices of DICE have twice the number of parameters compared to FC and FNC. Our experiment with   k  =  10   states show similar patterns of strongly and weakly connected states but they now vary in the direction of the connectivity. This result shows that both the connectivity strength and direction of connectivity are dynamic (changes over time). As this state is rare (based on time spent), it would be harder for window-based approaches to capture it. It would be interesting to see when and how the direction of connectivity changes and how external factors like performing a task can trigger these changes. This, however, is a topic of the future work. \n\nFinally, we show that not all time-points of the fMRI data are equally important to the downstream prediction task and discriminative connectivity matrices exhibit temporal dynamics. Using temporal attention, our model finds important time-points relevant to the ground-truth signal used in training. This further helps in interpretability as our model finds the time-points where the brain activity shows signals relevant to the task. Potentially, this would also be important in task data where the subject is asked to perform different tasks, and the DICE model can be used to find out which task revealed the symptoms of the underlying disorder. Our experiments show that temporal attention assigns stable and consistent weights to time-points across different randomly-seeded tasks. We also notice that a) just 5% of time-points are sufficient for achieving high classification performance and b) exclusion of temporal attention (assigning the same weight to every time-point) negatively affects classification performance. Consistent temporal attention values across randomly-seeded trials further strengthens the evidence of temporally dynamic discriminative DCs and the value of attention mechanism. As our experiments show, our attention module is indeed reliable per the definitions and potential issues discussed by   and  . As a learnable method, DICE and other \u201cglass-box layer\u201d models need to be able to consistently across training runs assign temporal attention values and estimate connectivity between nodes, whereas inflexible methods computing correlations such as PCC do not have this property. In a way, flexibility of the learnable model comes with an additional requirement of stability of learned interpretations. Even though our DICE model works well by showing high classification performance and assigning consistent self and temporal attention values on relatively small datasets, as we show, having more subjects for training leads to an even more consistent assignment of temporal weights in our experiments. \n\n\n\n## Conclusions \n  \nOur work demonstrates importance of learnable interpretable estimators of dynamic, directed, and task-dependent connectivity graphs from fMRI data. DICE learns to estimate interpretable dynamic and directed graphs that represent the directed connectivity among brain networks. The end-to-end training process removes the need for existing external methods such as PCC and K-means, which are interpretable but inflexible and strictly depend on the input data. Implementing DICE with glass-box layer allowed us to bypass the need for a posthoc method for interpreting learned model representations. \n\nConnectivity matrices estimated by DICE show how brain connectivity changes across disorders, genders, and age. The learned connectivity matrices help understand the human brain and its disorders as the actual ground-truth connectivity matrix is not available. Furthermore, we moved from FC and FNC to DC and DNC to learn the direction of connectivity and simultaneously removed the issue of window sizing of input data by making the model instantaneous. The learned connectivity matrices provide knowledge that adheres to existing studies. Utilizing flexibility of DL models in learning data representations, we show that using the same data, distinct connectivity structures can be learned based on the downstream task and the ground-truth signal. This flexibility allows acquiring more information from the data by using different training labels, which would require a much more involved process of data selection and manual filtering out of confounding factors for methods that are fully determined by the data, like PCC. Our model highlights different networks linked with the downstream classification task, e.g., the default mode network for gender prediction. Unlike other interpretable models that may pay for it with a decrease in classification performance ( ;  ;  ;  ), DICE beats state of the art methods in multiple classification problems on four neuroimaging datasets. \n\nFor classification DICE uses the learned connectivity structures. Together with the temporal weights these structures are reasonably consistent across varying seeds. Notably, DICE\u2019s performance drops without the use of temporal attention. The temporal attention module of the model finds interpretable bio-markers crucial to performing the classification task and shows that only a small fraction of time-points is enough for attaining maximum performance. Notably, not all time points are discriminative, as evident from the sparse distribution of temporal attention weights in   and high predictive power of just the top 5% of the attention weights of  . \n\nAs the ground truth for the dynamic graph structure in resting state fMRI is unavailable, we believe there is a need for models with \u201cglass-box layer\u201c like DICE that can estimate this structure based only on the data and classification labels. \n\nIn future work, we would like to omit pre-processing with a dimensionality reduction method\u2014like the used here ICA or region-based parcellation\u2014and train a model end-to-end on the voxel-level data. This, however, may require substantially larger datasets and may not be as useful as the current model for an average sized research dataset. As DICE estimates the direction of connectivity, for future work, we would like to examine how the direction of connectivity changes through time and during tasks for HC and patients. \n\n \n", "metadata": {"pmcid": 9844250, "text_md5": "c232e112b410f6880b5fba1e0fa897ea", "field_positions": {"authors": [0, 85], "journal": [86, 96], "publication_year": [98, 102], "title": [113, 204], "keywords": [218, 311], "abstract": [324, 2981], "body": [2990, 65404]}, "batch": 2, "pmid": 36356823, "doi": "10.1016/j.neuroimage.2022.119737", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9844250", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9844250"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9844250\">9844250</a>", "list_title": "PMC9844250  Through the looking glass: Deep interpretable dynamic directed connectivity in resting fMRI"}
