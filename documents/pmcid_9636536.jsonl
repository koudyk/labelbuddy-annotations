{"text": "Chen, Gang and Pine, Daniel S. and Brotman, Melissa A. and Smith, Ashley R. and Cox, Robert W. and Taylor, Paul A. and Haller, Simone P.\nNeuroimage, 2021\n\n# Title\n\nHyperbolic trade-off: The importance of balancing trial and subject sample sizes in neuroimaging\n\n# Keywords\n\n\n\n# Abstract\n \nHere we investigate the crucial role of trials in task-based neuroimaging from the perspectives of statistical efficiency and condition-level generalizability. Big data initiatives have gained popularity for leveraging a large sample of subjects to study a wide range of effect magnitudes in the brain. On the other hand, most task-based FMRI designs feature a relatively small number of subjects, so that resulting parameter estimates may be associated with compromised precision. Nevertheless, little attention has been given to another important dimension of experimental design, which can equally boost a study\u2019s statistical efficiency: the trial sample size. The common practice of condition-level modeling implicitly assumes no cross-trial variability. Here, we systematically explore the different factors that impact effect uncertainty, drawing on evidence from hierarchical modeling, simulations and an FMRI dataset of 42 subjects who completed a large number of trials of cognitive control task. We find that, due to an approximately symmetric hyperbola-relationship between trial and subject sample sizes in the presence of relatively large cross-trial variability, 1) trial sample size has nearly the same impact as subject sample size on statistical efficiency; 2) increasing both the number of trials and subjects improves statistical efficiency more effectively than focusing on subjects alone; 3) trial sample size can be leveraged alongside subject sample size to improve the cost-effectiveness of an experimental design; 4) for small trial sample sizes, trial-level modeling, rather than condition-level modeling through summary statistics, may be necessary to accurately assess the standard error of an effect estimate. We close by making practical suggestions for improving experimental designs across neuroimaging and behavioral studies. \n \n\n# Body\n \n## Introduction \n  \nSound experimental design is key for empirical science. While reasonable statistical models may effectively extract the information of interest from the data, one first has to ensure that there is enough information present to begin with. Since there are significant constraints to acquiring data, such as cost and finite acquisition time, the experimenter should aim to optimize the experimental design to maximize relevant information within those practical limitations. A poorly designed experiment will bury signal within noise and result in unreliable findings. Of critical importance for the detection of an effect of interest in both neuroimaging and behavioral studies is to determine an appropriate sampling of a population (i.e., subjects) and a psychological process/behavior (i.e., stimuli/trials of a task/condition). Here we explore how sampling these two dimensions (i.e., subjects and trials) impacts parameter estimates and their precision. We then discuss how researchers can arrive at an efficient design given resource constraints. \n\n### Statistical efficiency \n  \n Statistical efficiency   is a general metric of quality or optimization (e.g., keeping the standard error of an estimate small, while also conserving resources). One can optimize parameter estimation, a modeling framework, or an experimental design based on this quantity. A more efficient estimation process, model, or experimental design requires fewer samples than a less efficient one to achieve a common performance benchmark. \n\nMathematically, statistical efficiency is defined as the ratio of a sample\u2019s inverse Fisher information to an estimator\u2019s variance; it is dimensionless and has values between 0 and 1. However, since the model\u2019s Fisher information is often neither known nor easily calculated, here we will refer more informally to a quantity we call \u201cstatistical efficiency\u201d or \u201cprecision\u201d of an effect estimate as just the inverse of the standard error. This quantity is not dimensionless and is not scaled to model information, but it conveys the relevant aspects of both the mathematical and non-technical meanings of \u201cefficiency\u201d for the estimation of an effect. Alternatively, we also refer to the standard error, which shares the same dimension as the underlying parameter, as a metric for the   uncertainty   about the effect estimation. \n\nSample size is directly associated with efficiency. As per the central limit theorem, a more efficient experimental design requires a reasonably large sample size to achieve a desired precision of effect estimation and to reduce estimation uncertainty. For example, with   n   samples   x  ,   x  , \u2026,   x   from a hypothetical population, the sample mean   asymptotically approaches the population mean. As a study\u2019s sample size   n   increases, the efficiency typically improves with an asymptotic \u201cspeed\u201d of   (an inverse parabola). A related concept is statistical power, which, under the conventional framework of null hypothesis significance testing, refers to the probability that the null hypothesis is correctly rejected, given that there is a \u201ctrue\u201d effect, with a certain sample size. Here, we focus on efficiency or uncertainty instead of power to broaden our discussion to a wider spectrum of modeling frameworks. \n\n\n### Subject sample size in neuroimaging \n  \nStatistical inferences are contingent on the magnitude of an effect relative to its uncertainty. For example, if the average BOLD response in a brain region is 0.8% signal change with a standard error of 0.3%, the statistical evidence is considered strong for the effect of interest. On the other hand, if the standard error is 0.6%, we would conclude that the statistical evidence for this effect is lacking because the data cannot be effectively differentiated from noise. Now if the standard error of 0.6% is based on data from only 10 participants, we may consider collecting more data before reaching the conclusion of a lack of strong evidence for the effect. \n\nIt is surprisingly difficult to predetermine an appropriate sample size in neuroimaging. In the early days a small sample size might have efficiently addressed many questions on how cognitive operations are implemented in the brain (e.g., mean brain activation in specific regions with large effect magnitudes alongside relatively low uncertainty). For example, based on data from somatosensory tasks, one study indicated that as little as 12 subjects were sufficient to detect the desired group activation patterns (without considering multiple testing adjustments) ( ) and 24 subjects would be needed to compensate for the multiplicity issue. A few power analysis methodologies have been developed over the years that are intended to assist investigators in choosing an appropriate number of subjects (e.g., fMRIPower ( ), Neurodesign ( ),  ). Yet, even with these tools, power analyses are rarely performed in neuroimaging studies according to a recent survey ( ): the median subject number was 12 among the 1000 most cited papers during 1990\u20132012, and 23 among the 300 most cited papers during 2017\u20132018; only 3\u20134% of these reported pre-study power analyses. In fact, unless required for a grant application, most experiments are simply designed with sample sizes chosen to match previous studies. \n\nDetermining requisite sample sizes for neuroimaging studies is challenging. First, there is substantial heterogeneity in effect sizes across brain regions; thus, a sample size might be reasonable for some brain regions, but not for others. Second, the conventional modeling approach (massively univariate analysis followed by multiple testing adjustment) is another complicating factor. Because of the complex relationship between the strength of statistical evidence and spatial extent, it is not easy to perform power analysis while considering the multiplicity issue (e.g. permutation-based adjustment). Third, imaging analyses inherently involve multiple nested levels of data and confounds, which presents a daunting task for modeling. For instance, a typical experiment may involve several of these levels: trials, conditions (or tasks), runs, sessions, subjects, groups and population. Finally, there are also practical, non-statistical considerations involved, such as feasibility, costs, scanner availability, etc. Even though recent work has led to better characterizations of FMRI data hierarchy ( ;  ;  ), challenges of sample size determination remain from both modeling and computational perspectives. \n\nTheoretically, a large subject sample size should certainly help probe effects with a small magnitude and account for a multitude of demographic, phenotypic and genetic covariates. As such, several big data initiatives have been conducted or are currently underway, including the Human Connectome Project (HCP), Adolescent Brain Cognitive Development (ABCD), Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI), Enhancing NeuroImaging Genetics through Meta Analysis (ENIGMA), UK Biobank Brain Imaging, etc. Undoubtedly, such initiatives are valuable to the research community and will continue to provide unique opportunities to explore various aspects of cognition, emotion and mental health. On the other hand, these initiatives come with high expenditure, infrastructure requirements and analytical hurdles (different sites/scanners/software). Is \u2018big data\u2019 really the best or only solution to achieving high precision for small-to-medium effects? For research questions where resources are limited (e.g., rare diseases, non-human primates), recruiting a large number of potential participants may be out of the question. In these cases, one may wonder what alternative strategies are available to achieve similar or even higher statistical efficiency with the limited number of participants or resources available. \n\nIn setting up an experiment, the choice of subject sample size is a trade-off between statistical and practical considerations. On the one hand, estimation efficiency is assumed to increase with the sample size; thus, the larger the subject sample size, the more certain the final effect estimation. On the other hand, costs (of money, time, labor etc.) increase with each added \u201csample\u201d (i.e., subject); funding grants are finite, as is scanner time and even the research analyst\u2019s time. Even though a cost-effectiveness analysis is rarely performed in practice, this trade-off does play a pivotal role for most investigations as resources are usually limited. \n\n\n### A neglected player: Trial sample size \n  \nThe number of trials (or data points, in resting-state or naturalistic scanning) is another important sampling dimension, yet to date it has been understudied and neglected in discussions of overall sample size. Just as the number of subjects makes up the sample size at the population level, so does the number of trials serve as the sample size for each condition or psychological process/behavior. As per probability theory\u2019s law of large numbers, the average effect estimate for a specific condition should asymptotically approach the expected effect with increased certainty as the number of trials grows. Trial sample size often seems to be chosen for convention, practical considerations and convenience (i.e., previous studies, subject tolerance). As a result, the typical trial sample size in the field is largely in the range of [10, 40] per condition ( ). \n\nIt seems to be a common perception that the number of trials is irrelevant to statistical efficiency at the population level, other than the need to meet a necessary minimum sample size, as evidenced by the phrase \u201csample size\u201d in neuroimaging, by default, tacitly referring to the number of subjects. We hypothesize that the lack of focus on trial sample size likely results from the following two points:\n   \n Trial-level effects are usually of no research interest.   Often investigators are interested in condition-level effects and their comparisons. Therefore, trial-level effects generally attract little attention. \n  \n The conventional modeling strategy relies on condition-level summary statistics.   The conventional whole-brain, voxel-wise analysis is usually implemented in a two-step procedure: first at the subject level where trial-level effects are all bundled into one regressor (or into one set of bases) per condition; and second at the population level where cross-trial variability is invisible. Such a two-step approach avoids the computational burden of solving one \u201cgiant\u201d, integrative model. However, as a result the cross-trial variability, as part of the hierarchical integrity of the data structure, is lost at the population level. As the ultimate attention is usually paid to population-level inferences, it is this focus on condition-level effects that leads to the unawareness of the importance of both trial-level variability and trial sample size. \n  \n\nWe note that the main goal of most FMRI studies is to generalize the results, to both the condition and population levels. In order to achieve these dual goals, a study must include a sufficient number of samples, in terms of   both   trials and subjects, respectively. In practice, studies tend to focus mainly on population-level generalizability, and therefore most efforts have gone into increasing subject sample sizes (e.g., the increasing number of \u201cbig data\u201d initiatives), while the trial sample size is typically kept at some minimal level (e.g., 20\u201340). As a result, we would expect the generalizability at the condition level to be challenging, in comparison to that of the population level. Condition-level generalizability is further reduced by the common modeling practice of ignoring cross-trial variability ( ;  ). \n\nA small number of studies have chosen a different strategy for experimental designs with focus on scanning a few subjects for an extended period of time, such as dozens of runs (e.g.,  ;  ), in order to obtain a large number of trials. These are variously depicted as \u201cdense\u201d, \u201cdeep\u201d or \u201cintense\u201d sampling in the literature. Some argued that such a sampling strategy would be more advantageous due to its avoidance of potentially large cross-subject variability ( ). Such studies should have the advantage of having high generalizability at the condition level. However, in practice, these studies tend to include only one or a few subjects, so that generalizability to the population-level would be limited. \n\n\n### The current study \n  \nThe main goal of our current investigation is to examine the impact of trial sample size (i.e., stimulus presentations) per condition alongside the number of subjects on statistical efficiency. On the one hand, the investigator does typically consider the number of trials or stimuli as a parameter during experimental design, but it is largely treated as a convenient or conventional number which the subject is able to tolerate within a scanning session. On the other hand, from the modeling perspective, the trials are usually shrouded within each condition-level regressor in the subject-level model under the assumption that all trials share exactly the same BOLD response. Furthermore, only the condition-level effect estimates are carried over to the population-level model; therefore, trial sample size does not   appear   to have much impact at the population level. However, statistically speaking the trial sample size   should   matter, because increasing the number of trials in a study increases the amount of relevant information embedded in the data. Addressing this paradox is the focus of this paper, along with the issue of study generalizability. \n\nA related question is:   can the information associated with trial sample size be leveraged statistically to improve estimation efficiency  ,   in the same way that increasing the number of subjects would?   It is certainly the case that increasing the number of trials in a study increases the amount of relevant information to be studied. Thus, do trial sample size and cross-trial variability play a role in statistical efficiency? And if so, how big of a role compared to the subject sample size? \n\nIn the current study, we adopt a hierarchical modeling framework, and utilize both simulations and an experimental dataset to show that trial sample size is an important dimension when one optimizes an experimental design. Importantly, we demonstrate that the \u201ctrial number\u201d dimension has nearly the same weight and influence as its \u201csubject number\u201d counterpart, a fact which appears to have been underappreciated and underused in the field to date. As a result, we strongly suggest that the number of trials be leveraged alongside the number of subjects in studies, in order to more effectively achieve high statistical efficiency. In our modeling efforts, we compare the summary statistics approach of condition-level modeling (CLM) directly to a hierarchical framework of trial-level modeling (TLM) that explicitly takes cross-trial variability into consideration at the population level to examine the impact of cross-trial variability. We aim to provide a fresh perspective for experimental designs, and make a contribution to the discussion of \u2018big data\u2019 versus \u2018deep scanning\u2019 ( ;  ). \n\n\n\n## Trial-level modeling \n  \nFirst, we describe the formalization of our modeling framework (for convenient reference, several of the model parameters are summarized in  ). To frame the data generative mechanism, we adopt a simple effect structure with a group of   S   subjects who complete two task conditions (  C   and   C  ) while undergoing FMRI scanning. Each condition is exemplified with   T   trials ( ). We accommodate trial-level effects with a focus on the contrast between the two conditions, as is common in task FMRI. As opposed to the common practice of acquiring the condition-level effect estimates at the subject level, we obtain the trial-level effect estimates   y   of the   c  th condition ( ) and assume the following effect formulation with   c  ,   s   and   t   indexing conditions, subjects and trials, respectively:\n \nwhere   \u03bc   codes the population-level effect of the   c  th condition,   \u03c0   indicates the deviation of   s  th subject from the population effect   \u03bc   under the   c  th condition,   and   are the cross-subject and within-subject cross-trial variances, respectively, and   \u03c1   captures the subject-level correlation between the two conditions. \n\nOne advantage of a trial-level formulation is that it allows the explicit assessment of the relative magnitude of cross-trial variability. For the convenience of discussion, we assume homoscedasticity between the two conditions:  .  Specifically, the ratio of cross-trial to cross-subject variability can be defined as,\n \nLarge trial-to-trial variability has been extensively explored ( ;  ;  ). Strong evidence based on electroencephalography indicates that the substantial cross-trial variability is mainly caused by the impacts of ongoing dynamics spilling over from the prestimulus period that dwarf the influence of the trial itself ( ). Furthermore, recent investigations show that the variability ratio   R   appears often to be greater than 1 and up to 100. For example,   R   ranged from 10 to 70 for the contrast between congruent and incongruent conditions among 12 regions in a classic Flanker FMRI experiment ( ). In a reward-distraction FMRI experiment, the   R   value ranged from 5 to 80 among 11 regions ( ). Even for behavioral data, which are likely significantly less noisy than neuroimaging data, the cross-trial variability is large, with   R   between 3 and 11 for reaction time data in a reward-distraction experiment ( ), cognitive inhibition tasks such as the Stroop, Simon and Flanker task, digit-distance and grating orientation tasks ( ;  ). \n\nWhat role, if any, does trial sample size ultimately play in terms of statistical efficiency? Study descriptions typically do not discuss the reasons behind choosing their number of trials, likely a number selected by custom or convenience rather than for statistical considerations. Under the conventional analytical pipeline, each condition-level effect is estimated at the subject level through a regressor per condition. To examine differences between the conventional summary statistics pipeline through CLM and TLM as formulated in ( ), we lay out the two different routes of obtaining condition-level effect estimates from subject-level analysis through time series regression: (A) obtain the   c  th condition-level effect   through a regressor for all the trials under the   c  th condition; (B) estimate the trial-level effects   y   using one regressor per trial and then obtain the condition-level effect through averaging,\n \nPipeline (A) includes the following two-step process: first average trial-level regressors and then perform CLM through time series regression. In contrast, pipeline (B) can be considered as swapping the two steps of averaging and regression in pipeline (A): regression occurs first (i.e., TLM), followed by averaging the trial-level effect estimates. As the two processes of averaging and regression are not operationally commutative,   and   are generally not the same. However, with the assumption of an identical and independent distribution of subject-level cross-trial effects,  the latter can be a proxy when we illustrate the variability of condition-level effect estimates (and later when we perform simulations of CLM in contrast to TLM):\n \nThe variance expression ( ) indicates that even though trial-level effects are assumed to be the same under the conventional CLM pipeline, cross-trial variability   is implicitly and almost surreptitiously carried over to the population level. The important implication is that while the trial sample size   T   does not explicitly appear in the conventional CLM at the population level, it does not mean that its impact would disappear; rather, because of the way that the regressors are created, two implicit but strong assumptions are made: 1) all trials elicit exactly the same response under each condition, and 2) the condition-level effect   is direct measurement without any sampling error. \n\nWe now derive the expression for the standard error for the estimation of the contrast between the two conditions at the population level. Directly solving the hierarchical model ( ) would involve numerical iterations through, for example, restricted maximum likelihood. Fortunately, with a relatively simple data structure with two conditions, we can derive an analytic formulation that contains several illuminating features. With the notions\n \nand the variance expression ( ), we have\n \n\nThus, the contrast between the two conditions at the population level can be expressed as\n \nwhere the variance   \u03c3   can be derived as\n \nImportantly, the explicit expression for   \u03c3   above allows us to explore the contributions of various quantities in determining the statistical efficiency for the contrast   \u03bc  . We note that, in deriving the variance   \u03c3  , the average effects at the condition level,   and  , are assumed to have their respective conditional distributions; thus, trial sample size   T   and cross-trial variability   \u03c3   directly appear in the formulation ( ). In contrast, their counterparts in the conventional CLM pipeline,   and  , would be treated as direct measurements at the population level, leading to a one-sample (or paired) Student\u2019s   t  -test. Below, in simulations we will use the one-sample   t  -test as an approximation for the conventional CLM pipeline and further explore this relationship. We note that it is because of this simplification in the CLM pipeline, that the impact of trial sample size   T   and cross-trial variability   \u03c3   has been historically hidden from close examination. \n\nThe variance formula ( ) has important implications for the efficiency of an experimental design or power analysis. One appealing aspect is that, when parameters   \u03c1  ,   \u03c3   and   \u03c3   are known, we might be able to find the required sample sizes   S   and   T   to achieve a designated uncertainty level   \u03c3  . However, we face two challenges at present: the parameters   \u03c1  ,   \u03c3   and   \u03c3   are usually not empirically available; even if they were known, one cannot uniquely determine the specific sample sizes. Nevertheless, as we elaborate below, we can still gain valuable insight regarding the relationship between the subject and trial sample sizes in an experimental design, as well as their impact on statistical efficiency along with the parameters   \u03c1  ,   \u03c3   and   \u03c3  . \n\nThe variance expression ( ) immediately reveals two important aspects of the two sample sizes. First, statistical efficiency, as defined as the reciprocal of the standard error   \u03c3  , is an inverse parabolic function in terms of either the subject sample size   or the trial sample size  . This implies that the efficiency of an experimental design improves as either sample size increases. However, this inverse parabolic relationship also means that the marginal gain of efficiency diminishes when   S   (or   T  ) increases. In addition, subject sample size makes a unique contribution in the first term  , which represents the cross-subject variance. The two sample sizes,   S   and   T  , combine symmetrically in the second term  , which is the cross-trial variance. In the general case that the first term is not negligible compared to the second, we might say that the subject sample size influences   \u03c3   more than the trial sample size. \n\nWe can rearrange the variance formula ( ) and express   T   as a function of   S  , with the other quantities treated as parameters:\n \nThis expression shows more about the interplay between the two sample sizes within the   \u03c3   estimation: namely that they have a hyperbolic relationship.  This means that one can \u201ctrade-off\u201d between   S   and   T   values for a given uncertainty   \u03c3  , while all other parameters remain constant. If   \u03c3  ,   \u03c1   and   R   were known, one could use the above expression to find possible combinations of   S   and   T   that are associated with a desired standard error   \u03c3  . \n\nAnother important feature of the hyperbolic relation ( ) is the presence of two asymptotes: one at   T   =   T  * = 0, and one where the denominator is zero at\n \nEach asymptote sets a boundary for the minimum number of respective samples required to have a given statistical efficiency (given the other parameters). For the number of trials, the requirement that   T   >   T  * merely means there must be   some   trials acquired. For the number of subjects,   S  * is typically nonzero, so the requirement   S   >   S  * can be a meaningful constraint. \n\nThese features and other relations within the expressions ( )-( ) can be appreciated with a series of example curves in  . Each column has a fixed   \u03c1  , and each row has a fixed   R  . Within each panel, each curve is only defined where   S   >   S  * and   T   >   T  *, with the vertical asymptote for each curve shown as a dotted line (and the horizontal asymptote is the   S  -axis). Each solid curve displays the set of possible (  S  ,   T  ) combinations that would result in designs having the same   \u03c3  , defining an isocontour of statistical efficiency. Thus, the possible trade-offs between   S   and   T   for a given   \u03c3   are demonstrated along a curve. In terms of the \u201cbalance\u201d of trade-offs between   S   and   T  , there are a few items to note:\n   \nAs noted above,   S  * sets the minimum number of subjects required to be able to reach an uncertainty level   \u03c3  . \n  \nWhen one is near the horizontal   T   =   T  * = 0 asymptote, there is very little marginal gain in   \u03c3   by increasing the subject sample size   S  ; this scenario corresponds to current \u201cbig data\u201d initiatives collecting a large pool of subjects. Inversely, when approaching the vertical asymptote, we emulate the other extreme, the scenario of \u201cdeep scanning\u201d with a lot of trials in only a few subjects, statistical efficiency barely increases when increasing the trial number. However, as indicated by the previous point, one would have to recruit a minimum number of subjects,   S  *, to reach a designated statistical efficiency for population-level analysis ( ). In practice, the subject sample size in most deep scanning studies is likely far below the threshold   S  *. \n  \nWithin the asymptotic region, the isocontour is symmetric around the line   T   \u2212   T  * =   S   \u2212   S  *, which simplifies here to   T   =   S   \u2212   S  *; that is, if (  S  ,   T  ) is a point on an isocontour, then so is (  T   +   S  *,   S   \u2212   S  *). \n  \nBecause   T  * = 0 and   S  * > 0, the subject sample size   S   tends to have slightly more impact on reaching a statistical efficiency than the trial sample size   T  ; however, as   S  * \u2192 0, that difference decreases. For a given   S  *, the amount of subject \u201coffset\u201d also matters less as   R   increases: the isocontour moves further from the asymptote, so the values being traded off become relatively larger, diminishing the relative impact of   S  *. That is, in both cases, the   T   =   S   \u2212   S  * relation from the previous point becomes well approximated by   T   \u2248   S  , and (  S  ,   T  ) is essentially exchangeable with (  T  ,   S  ). \n  \nCombining the previous two points, once paying the \u201cfixed cost\u201d of adding the minimal number of subjects   S  *, one can equivalently trade-off the remaining number of samples between   S   and   T  , while maintaining a constant uncertainty   \u03c3  . Or, viewed another way, in gauging the relative importance of each sample size to reach a specific uncertainty   \u03c3  , the number of subjects has an \u201cextra influence\u201d of magnitude   S  * over the trial sample size   T  . \n  \nAs trial number increases and   T   \u2192 \u221e, the lowest uncertainty   \u03c3   that could be achieved would be given by the first term in the variance expression ( ):  . \n  \nThe gray dashed line in   shows the trajectory of optimized (  S  ,   T  ) pairs, each defined for the constraint of having a fixed total number of samples ( ). As   R   increases, the optimal trajectory approaches   S   \u2248   T  . This is in line with the exchangeability or symmetry between the two sample sizes elaborated above in 4). \n  \n\nOne can also observe from   the role that the correlation   \u03c1   plays in the estimation of   \u03c3   and the hyperbolic relation between   S   and   T  . Namely,   \u03c1   does not affect the shape or slope of the hyperbola, but instead it just assists in determining the location of the   S  * asymptote, which can be appreciated from the expressions ( )-( ). In other words, the correlation   \u03c1   only changes the impact of the cross-subject variance component (the first term   in ( )) but not that of the cross-trial variance component (the second term   in ( )). All other parameters being equal, as   \u03c1   increases, the minimal number of subjects   S  * for a study design decreases. This makes intuitive sense: the smaller the correlation (including anticorrelation) between the two conditions, the more the hyperbola is shifted rightward (and thus the more difficult to detect the contrast between the two conditions). Additional views on the role of   \u03c1   are provided in  , in an idealized optimization case. \n\nFinally, we note that we could express   S  * explicitly as a function of   S   and   T   by taking the expression ( ) and substituting the value of   \u03c3   formulation ( ). This results in the following relationship of   S  * with the two sample sizes:\n \nWith   S  * expressed as a function of (  S  ,   T  ), one could rearrange this expression for   T   on the left-hand side, and calculate isocontours with   S  * held constant; these would have exactly the same shape and properties as those for uncertainty in  . This is not surprising because there is a one-to-one correspondence between   S  * and   \u03c3  , as shown in the   S  * definition ( ). \n\n### Limit cases of trial-level modeling \n  \nTo further understand the roles of various parameters, we consider different scenarios based on the variability ratio   R   and the number of trials   T  . First, we start with the last expression in the variance formulation ( ), in particular the additive terms in square brackets. The second term is small when the variance ratio   R   is relatively low compared to the trial sample size   T  , producing this limiting behavior:\n \nThus, in the case of low   R   (and/or large   T  ), the second component in the full variance expression could be practically ignored, and the standard error   \u03c3   essentially depends only on the number of subjects,   \u03c3   \u221d (  S  ) ; it is independent of the trial sample size   T   as well as cross-trial variance  . For example, with 20 \u2264   T   \u2264 200 and \u22120.5 \u2264   \u03c1   \u2264 0.5, this would require that   R   be around 1 or less. In such a case, the isocontours would be approximately vertical lines, essentially matching the full contours of the first two rows in  ; and   S   is approximately the asymptotic value   S  *. This relation also includes parts of the plots in the last three rows, as the isocontours become approximately vertical in the asymptotic limit of the trial number   T   reaching the hundreds or above. \n\nNext, we consider the opposite limiting case. If the variability ratio   R   is relatively high compared to the trial sample size   T  , then the variance expression ( ) becomes:\n \nThe expression for   \u03c3   shows that standard error can be expressed independent of the cross-subject variability   \u03c3   and is dependent only on the cross-trial variability   \u03c3   R  . Additionally, we note that the standard error   \u03c3   depends on both sample sizes equally, with an asymptotic speed of   \u03c3   \u221d (  ST  ) . As a corollary of the relationship ( ), we could say that the relative impact of   S  * has become negligible, and so that the trade-off relationship   T   =   S   \u2212   S  * is well approximated by the exchange   T   \u2248   S  . Thus, the two sample sizes have equal impact on reaching an isocontour and can be equivalently traded off for each other. This is illustrated in all the isocontours except for   \u03c3   = 0.125, 0.25 with   \u03c1   = 0.5 and   R   = 5 or all the isocontours except for   \u03c3   = 0.125 (blue) with   \u03c1   = 0.5 and   R   = 10, 50 in  . In practice, for typical study designs that have 20 \u2264   T   \u2264 200 and   \u03c1   = 0.5, this limiting case would apply if   R   were approximately greater than, for example, 20 or 100 for the respective limits. \n\nWe comment briefly on the intermediate scenario, where   has a moderate value compared to   T  . In this case, both sample sizes play some extent of role in the uncertainty   \u03c3  . However, as noted above, the number of subjects plays a slightly larger role than the number of trials. This is observable by the presence of a non-negligible   S  * which offsets the (  S  ,   T  ) trade-off. In  , relevant contours for this intermediate case are:   \u03c3   = 0.125 (blue) with   R   = 5, 10, 50 and those of   \u03c3   = 0.25 (blue). \n\nWe also highlight one feature of the variability ratio   R  . From the above two limit cases for   \u03c3  , we see that   R   has an important scale, based on the number of trials. That is, it is the size of   R   relative to   that determines much of the expected behavior of the standard error, and even whether it has any meaningful dependence on the number of trials\u2014in Case 1,   \u03c3   was essentially independent of   T  . The correlation   \u03c1   plays a role in this as well, but typically   T   is something that the experimenter controls more directly. \n\nTo summarize, we note that subject sample size   S   always plays a crucial role in achieving an adequate level of statistical efficiency. In contrast, the impact of trial sample size   T   can be much more subtle. At one extreme, its role may be negligible if   R   is around 1 or less for most trial sample sizes currently used in practice (Case 1); in fact, this limit case is what is implicitly assumed when researchers select a small number of trials and accounnt for all trials via a single regressor under condition-level modeling. However, we emphasize that empirical data indicates that this low cross-trial variability scenario rarely occurs. At the other extreme, the trial sample size is almost as important as its subject counterpart if   R   is large relative to   T   (Case 2). In between these two limits lies the intermediate scenario where trial sample size is less important than subjects, but its influence remains sizeable. Based on the empirical values of   R  , we expect that most\u2014if not all\u2014experimental data will likely fit into the two latter cases, with   T   being an important consideration. This has the beneficial consequence for study designs that the trial sample size can be utilized to meaningfully trade-off with the subject sample size per the variance formulation ( ). \n\n\n\n## Simulations \n  \nTo further explore the impact of subject and trial sizes, we use numerical simulations to test and validate the theoretical reasoning laid out above.  Suppose that trial-level effects   y   are generated through the model formulation ( ) with population-level effects of   \u03bc   = 0.5,   \u03bc   = 1.0 and a cross-subject standard deviation   \u03c3   = 1, all in the typical BOLD units of percent signal change. Simulations for five sets of parameters were conducted:\n   \nfive subject sample sizes:   S   = 20, 40, 60, 80, 180 \n  \nfive trial sample sizes:   T   = 20, 40, 60, 80, 180 \n  \nfive cross-trial standard deviations:   \u03c3   = 1, 10, 20, 50, 100 \n  \nfive subject-level correlations:   \u03c1   = 0.1, 0.3, 0.5, 0.7, 0.9 \n  \ntwo modeling approaches: trial-level (TLM) and condition-level (CLM). \n  \n\nWith the cross-subject standard deviation set to   \u03c3   = 1, the five trial samples sizes correspond to variability ratios of   R   = 1, 10, 20, 50, 100. With the population-level contrast   \u03bc   =   \u03bc   \u2212   \u03bc   = 0.5, the theoretical standard error is generally described by the expression ( ), approaching the asymptotic expressions in ( ) and ( ) in some cases of   R   and  . The various combinations of parameters lead to 5 \u00d7 5 \u00d7 5 \u00d7 5 \u00d7 2 = 1, 250 different cases, each of which was repeated in 1000 iterations (with different random seeds). \n\nTo evaluate the simulated models, we define the following quantities for investigation. For example, for model parameters such as the contrast and its standard error, we calculate the mean (or median) and standard error of each of these two estimated parameter values across 1000 iterations. Firstly, the point estimate of a parameter is considered   unbiased   when the expected mean of the sampling distribution is equal to the population value; for the present simulations, this would be the case if the mean of the estimated contrast is approximately   \u03bc   =   \u03bc   \u2212   \u03bc   = 0.5 across the iterations. Secondly, the uncertainty information for each parameter is measured in the form of 95% quantile interval. We note that the uncertainty for the effect (i.e., contrast) is numerically obtained from the repeated sampling process while the point estimate for the standard error of the effect is assessed per the respective hierarchical model. Thirdly, we validate the standard error (or efficiency) of the effect estimate per the formulation ( ). Finally, we investigate the presence of a hyperbolic relationship between the two sample sizes of subjects and trials. \n\nSimulation findings are displayed in  ,  ,   and  . Each plot shows a different way of \u201cslicing\u201d the large number of simulations, with the goal of highlighting interesting patterns and outcomes. Each plot shows results with correlations between the two conditions of   \u03c1   = 0.5, but the patterns and trends are quite similar for other values of   \u03c1  , so there is no loss of generality by simply assuming   \u03c1   = 0.5. As noted above, the formula ( ) and   show that changing   \u03c1   typically affects the value of   S  * and hence the location of the vertical asymptote, much more than the shape of the isocontours themselves. \n\nWe summarize some of the main findings from the simulations, which can be observed across the figures:\n   \n Effect estimation is unbiased, but its uncertainty varies  . As shown in   and  , unbiased estimation was uniformly obtained at the simulated contrast of   \u03bc   =   \u03bc   \u2212   \u03bc   = 0.5 from both TLM and CLM. However, the estimation uncertainty, as indicated by each 95% highest density interval (vertical bar) among 1000 iterations, is noticeably different across simulations. In particular, uncertainty decreases (larger bars) as   R   increases and improves (smaller bars) when the trial or subject sample size or both increase. TLM and CLM rendered virtually the same effect estimates. \n  \n The standard error of effect estimation depends strongly on three factors: variability ratio, trial and subject sample sizes  .   and   show the standard error   \u03c3   estimates from the simulations. The standard error increases with   R  , and it decreases as either   T   or   S   (or   ST  ) increases. Specifically, the median   \u03c3   estimates from the simulations match largely well with the theoretical expectations, with TLM producing a median closer to the predictions than CLM, as well as a smaller percentile spread. The uncertainty of the effect estimation for   \u03bc  , as indicated by the 95% quantile interval (error bar) in   and  , was obtained through samples across 1000 iterations. On the other hand, the standard error estimates for   \u03c3  , another indicator of uncertainty for the effect estimation of   \u03bc   as shown in   and  , were analytically assessed from the associated model. Nevertheless, these two pieces of uncertainty information for effect estimation are comparable from each other: the 95% quantile intervals in   and   are roughly two times the standard error estimates for   \u03c3   in   and  . \n  \n The hyperbolic relationship is empirically confirmed in simulations.   The confirmation can be seen in the close overlap of the estimated uncertainty (blue for TLM and red for CLM in   and  ) versus the theoretical prediction (green). The hyperbolic relation between the number of trials and the number of subjects should allow one to trade-off   S   and   T   while keeping other parameters (e.g., statistical efficiency) constant. In addition, when the variability ratio is relatively large (e.g.,   R   \u2265 10), this relationship also appeared to be validated in the symmetric pattern in these simulations\u2014see the cyan and magenta boxes in   and  , which highlight sets of cells that have roughly equal efficiency. In other words, with relatively large   R  , this trade-off is directly one-for-one with an approximate symmetry (Case 2 in  ); for smaller variance ratios, the hyperbolic relationship becomes more asymmetric and needs to trade-off a greater number of trials than subjects to keep an equal efficiency. \n  \n Optimizing both trial and subject sample sizes is critical to maximize statistical efficiency.   What is an optimal way to reduce effect estimate uncertainty?   and   suggest that increasing   S   and   T   together is typically a faster way to do so than increasing either separately. For example, start in the lower left cell of  , where   S   =   T   = 20. Note that moving to another cell vertically (increasing   T  ) or horizontally (increasing   S  ) leads to deceased uncertainty (smaller percentile bars). Moving either two cells up (increasing   T   by 40) or two cells right (increasing   S   by 40) leads to similar uncertainty patterns. However, moving diagonally (increasing each of   T   and   S   by 20) leads to slightly more reduced uncertainty, and this property holds generally (and also for the standard error in  ). This is expected from the theoretical behavior of the hyperbolic relationship ( ). In other words, these simulations reflect the fact that to maximize statistical efficiency of experimental design both sample sizes   T   and   S   should typically be increased. \n  \n The differences between trial-level and condition-level modeling are subtle.   TLM and CLM rendered virtually the same effect estimates (  and  ). However,   and   show that CLM may result in some extent of underestimation of the standard error   \u03c3  , as well as an increased uncertainty of   \u03c3   (i.e., larger bars in red), in certain scenarios. The extent to which an underestimation may occur depends on three factors:   R   and the two sample sizes. Specifically, when cross-trial variability is very small (i.e.,   R   \u2272 1), the underestimation of condition-level modeling is essentially negligible unless the trial sample size   T   is less than 40. On the other hand, when cross-trial variability is relatively large (i.e.,   R   \u2273 20), the underestimation may become substantial especially with a small or moderate sample size (e.g.,   R   = 50 with \u2272 50 subjects or trials). In addition, the underestimation is more influenced by subject sample size   S   rather than trial sample size   T  . This observation of substantial underestimation when trial sample size is not large illustrates the importance of TLM and is consistent with the recent investigations ( ;  ). \n\nWe reiterate that the problems with CLM are not limited to the attenuated estimation of standard error. They are also associated with increased uncertainty across the board (larger error bars in red,   and  ). On the other hand, some extent of overestimation of standard error occurred for TLM under the same range of parameter values (blue lines in   and  ). In addition, the uncertainty of the TLM standard error estimation is right skewed (longer upper arm of each error bar in blue,   and  ). This was caused by a small proportion of numerical degenerative cases that were excluded from the final tallies because of algorithmic failures under the LME framework when the numerical solver got trapped at the boundary of zero standard error. Although no simple solutions to the problem are available for simulations and whole-brain voxel-level analysis, such a numerical degenerative scenario can be resolved at the region level under the Bayesian framework ( ). \n  \n\n\n## Assessing the impact of trial sample size in a neuroimaging dataset \n  \n### Data description \n  \nThe dataset included 42 subjects (healthy youth and adults) and was adopted from two previous studies ( ;  ). During FMRI scanning, subjects performed a modified Eriksen Flanker task with two trial types, congruent and incongruent: the central arrow of a vertical display pointed in either the same or opposite direction of flanking arrows, respectively ( ). The task had a total of 432 trials for each of the two conditions, administered across 8 runs in two separate sessions. Only trials with correct responses were considered in the analysis. Thus, there were approximately 380 trials per condition per subject (350 \u00b1 36 incongruent and 412 \u00b1 19 congruent trials) after removing error trials. \n\nData processing was performed using AFNI ( ). Details regarding image acquisition, pre-processing and subject-level analysis can be found in  . Effect estimates at the trial-level for correct responses in each condition were obtained with one regressor per trial for each subject using an autoregressive-moving-average model ARMA(1, 1) for the temporal structure of the residuals through the AFNI program 3dREMLfit ( ). For comparison, effects at the condition level were also estimated through the conventional CLM approach using one regressor per condition via 3dREMLfit. The main contrast of interest was the comparison between the two conditions (i.e., incongruent versus congruent correct responses). \n\n\n### Assessing cross-trial variability across the brain \n  \nThe top row of   displays axial slices of the effect estimate of interest, the contrast \u201cincongruent versus congruent\u201d. The lower row displays the variability ratio   R   associated with the contrast, which was estimated at the whole-brain voxel level through the model ( ) using the AFNI program 3dLMEr ( ). Translucent thresholding was applied to the overlays: results with   p   < 0. 05 are opaque and outlined, and those with decreasing strength of statistical evidence are shown with increasing transparency. Substantial heterogeneity exists across the brain in terms of the relative magnitude of cross-trial variability   R   (with most high   R   \u2273 50 in low-effect regions).   shows the distribution of the voxelwise variability ratio   R   for the FMRI dataset, which has a mode of 20 and a 95% highest density interval [6, 86]. These   R   values are consistent with previous investigations of variability ratios in neuroimaging ( ;  ) and psychometric data ( ). Interestingly, many of the locations with high effect estimates (dark range and red) had relatively low variability ratios,   R   \u2272 20 (dark blue and purple colors). The regions of high contrast and strong statistical evidence (and low-medium   R  ) include  the intraparietal sulcus area, several visual areas, premotor eye fields and inferior frontal junction, which are likely involved in the task. Most of the rest of the gray matter, as well as white matter and cerebrospinal fluid, had notably higher   R   \u2265 50. \n\n\n### Impact of trial sample size \n  \nNext, we investigated the impact of increasing trial sample size using the same Flanker FMRI dataset. Four different trial sample sizes were examined, by taking subsets of the total number available \u201cas if\u201d the amount of scanning had been that short: 12.5% (\u2248 48 trials from the first run during the first session); 25% (\u2248 95 trials from the first run of both sessions); 50% (\u2248 190 trials from the first session); and 100% (\u2248 380 trials). Two modeling approaches were adopted for each of the four sub-datasets with different trial sample sizes: TLM through the framework ( ) using the AFNI program 3dLMEr, and CLM through a paired   t  -test using the AFNI program 3dttest++. \n\n shows the values of effect estimates and the associated statistics in a representative axial slice as the number of trials increases, along with the comparisons of TLM vs CLM. Regions showing a large, positive effect (hot color locations in  ) are fairly constant in both voxelwise value and spatial extent across trial sample sizes. Additionally, they are quite similar between the two modeling approaches, as the differences for these regions (third column) are small, particularly as the trial sample size increases. In general, regions with negative effects exhibit the most change as trial sample size increases, moving from negative values of fairly large magnitude to values of smaller magnitude; most of these regions also show corresponding weak statistical evidence (cf.  ). The statistical evidence for regions with positive effects incrementally increases with the number of trials ( ). The difference in statistical evidence between TLM and CLM (third column) are expressed as a ratio, centered on zero: in regions with strong statistical evidence, differences are typically small in the center of the region, with some differences at the edges; in the latter case, CLM tends to render larger values, which is consistent with having an underestimated standard error   \u03c3   for a similar effect estimate (which was observed in the simulations in   and  ). \n\n displays a direct comparison of changes in statistical evidence with increasing number of trials. For both modeling approaches, TLM and CLM, most of the regions with positive effects show notable increase in statistical evidence with trial sample size. In scenarios where the cross-trial variability is relatively large (Case 2; cf.  ), one would theoretically expect statistical efficiency to increase with the square root of the trial sample size. Here, the number of trials doubles between two neighboring rows, which results in roughly a   increase in the statistical value (given fairly constant effect estimates). Several parts of the plot show approximately similar rates of increase for CLM and TLM. \n\n\n\n## Discussion \n  \nCareful experimental design is a vital component of a successful scientific investigation. An efficient design would effectively \u201cguide\u201d and \u201cdivert\u201d the information to the collected data with minimal noise; an inefficient design might lead to a loss of efficiency, generalizability or applicability, or worse, a false confidence in the strength of the obtained results. Adequate sampling of both subjects and trials is crucial to detect a desired effect with proper statistical evidence in task-related neuroimaging but also behavioral tasks. Historically, efforts to improve statistical efficiency have mostly focused on increasing subject sample size. In contrast, increasing trial number has received substantially less attention and is largely neglected in current tools to assess statistical power. In fact, there is little guidance in FMRI research on how to optimize experimental designs considering both within-subject and between-subject data acquisition. The present investigation has demonstrated how vital it is to consider trial sample size for any FMRI study. \n\n### Importance of trial sample size for statistical efficiency \n  \nIn this investigation we show that, due to the hyperbolic relationship between the two sample sizes, the trial number plays an active role in determining statistical efficiency in a typical task-based FMRI study. Currently, many investigators tend to assume that only the number of subjects affects the efficiency of an experimental design through an inverse-parabolic relationship. Thus, most discussions of efficiency improvement focus on increasing the subject sample size, and the number of trials is largely treated as irrelevant. Assuming a negligible role for trial sample size would be valid if the cross-trial variability was small relative to cross-subject variability (first two rows in  ). However, converging evidence from empirical data indicates that the cross-trial variability is usually an order of magnitude larger than its cross-subject counterpart. As a result, trial sample size is nearly as important as subject sample size. In other words, in order to improve the efficiency of an experimental design, a large number of both trials and subjects would be optimal (see last three rows in  ). Alternatively, trial sample size can be utilized in a trade-off with the number of subjects to maintain similar design efficiency. \n\nIn practice, additional considerations such as cost, scanner time, habituation, subject fatigue, etc. will play an important role in a study design. These might affect the overall balance between the sample sizes of subjects and trials. For example, in a study of 10,000 subjects (such as the UK Biobank), it seems unfeasible to recommend having 10,000 trials per subject. Even if scan costs were covered for such a large number of trials, subject fatigue and habituation would mitigate the benefits of optimizing theoretical statistical efficiency. However, even for smaller scales in terms of number of trials, one could see efficiency benefits by having 100 versus 50 trials, for example. Hence, a positive note from the current investigation is that adding one more trial often would be more cost effective than adding one more subject; adding subjects is typically much more expensive than adding a bit more scan time. When a study includes a larger trial sample size, one might opt to tweak the design, such that multiple runs and/or multiple sessions are used to reduce fatigue. In summary, while additional practical considerations may make having roughly equal sample sizes unfeasible in some cases, most FMRI studies would benefit greatly from increasing the trial number. \n\nThe current work also sheds some light on the state of power analyses in neuroimaging. In theory, one could stick with the conventional practice in the larger field of psychology/neuroscience of estimating a study\u2019s power (i.e., largely estimating the required sample size given a certain effect size). It is more difficult to provide an optimization tool that could assist the investigator to achieve the most efficient experimental design.   offered a generic power analysis interface that could aid researchers in planning studies with subject and trial sample sizes for psychological experiments. While our analyses, simulations and example experiment have shown the importance of having an adequate trial sample size, optimizing these study parameters is practically impossible given the number of unknown parameters involved in psychological and neuroimaging studies. Nevertheless, similar to trade-offs that can be made in power analyses, our discussion here emphasizes the importance of being aware of the trade-off between the two sample sizes: one can achieve the same or at least similar efficiency through manipulating the two sample sizes to a total amount of scan time or cost, or increase the efficiency by optimizing the two sample sizes with least resource cost. \n\n\n### Trial-level versus condition-level modeling: Accounting for cross-trial variability \n  \nAt present, most neuroimaging data analysis does not consider triallevel modeling of BOLD responses. Even in scenarios where trial-level effects are a research focus (e.g., machine learning), within-subject cross-trial variability has not been systematically investigated. Recent attempts to model FMRI task data at the trial level ( ) have revealed just how large the variance across trials within the same condition can be\u2014namely, many times the magnitude of between-subjects variance. Traditional analysis pipelines aggregate trials into a single regressor (e.g., condition mean) per subject via condition-level modeling. This pipeline relies on the assumption that responses across all trials are exactly the same, and therefore the cross-trial variability is largely ignored. Interestingly, cross-trial variability is not even necessarily smaller for experiments that have sparse, repetitive visual displays (e.g., such as the Flanker task with simple arrow displays) compared to experiments which feature stimuli with more pronounced visual differences (e.g., smiling human faces with various \u201cactors\u201d who differ in their gender, age, race, and shape of facial features). Another potentially important aspect is the integration of multiple data modalities such as FMRI and EEG through hierarchical modeling ( ). Similar challenges and proposals have also been discussed in psychometrics ( ;  ;  ) as well as in other neuroimaging modalities such as PET ( ), EEG ( ;  ) and MEG ( ). \n\nCross-trial variability appears largely as random, substantial fluctuations across trials, and it is present across brain regions with no clear pattern except bilateral synchronization ( ). In other words, a large proportion of cross-trial fluctuations cannot be simply explained by processes such as habituation or fatigue. However, there is some association between trial-level estimates and behavioral measures such as reaction time and stimulus ratings when modeled through trial-level modeling at the subject level. The mechanisms underlying cross-trial fluctuations remain under investigation (e.g.,  ). \n\nTo more accurately characterize the data hierarchy, we advocate for explicitly accounting for cross-trial variability through triallevel modeling. Another reason for this recommendation is conceptual because researchers expect to be able to generalize from specific trials to a category or stimulus type. Simply because trial-level effects are of no interest to the investigator does not mean that they should be ignored as commonly practiced. As demonstrated in our simulations, to support valid generalizability, we suggest using trial-level modeling, especially when the trial sample size is small (i.e., 50\u2013100 or less,  ) to avoid sizeable inflation of statistical evidence (or the underestimation of the standard error). It is worth noting that triallevel modeling presents some challenges at both the individual and population level. The computational cost is much higher, with a substantially larger model matrix at both the subject and population level. In addition, larger effect estimate uncertainty, outliers, and skewed distributions may occur due to high collinearity among neighboring trials or head motion; experimental design choices, such as the inter-trial interval, can be made to help reduce these issues. Recent investigations ( ;  ;  ) provide some solutions to handle such complex situations under the conventional and Bayesian frameworks. \n\n\n### Beyond efficiency: Trial sample size for generalizability, replicability, power and reliability \n  \nProperly handling uncertainty, replicability and generalizability lies at the heart of statistical inferences. The importance of considering the number of trials in a study extends beyond statistical efficiency to other prominent topics in neuroimaging. In particular,   statistical efficiency   relates to the interpretation and perception of results within a single study, but trial sample size will also have important effects on the properties that a group of studies would have\u2014for example, if comparing results within the field or performing a meta analysis. \n\nFirst, replicability within FMRI has been a recent topic of much discussion. This focuses on the consistency of results from studies that address the same research question separately, using independent data (and possibly different analysis techniques). Concerns over low rates of agreement across FMRI studies have primarily focused on the subject sample size (e.g.,  ). We note that replicability is essentially the same concept as uncertainty, which was discussed in  : characterizing the spread of expected results across many iterations of Monte Carlo simulations mirrors the analysis similar datasets across studies. As shown in that section and in  \u2013 , increasing the number of trials plays an important role in decreasing uncertainty across iterations\u2014and by extension, would improve replicability across studies. While some investigations of FMRI replicability have called for more data per subject (e.g.,  ), the present investigation provides a direct connection between the number of trials and uncertainty/replicability. \n\nGeneralizability is a related but distinct concept that refers to the validity of extending the specific research findings and conclusions from a study conducted on a particular set of samples to the population at large. Most discussions of generalizability in neuroimaging have focused on the sample size of subjects: having some minimum number of subjects to generalize to a population. However, FMRI researchers are often also interested in (tacitly, if not explicitly) generalizing across the chosen condition samples: that is, generalizing to a \u201cpopulation of trials\u201d is also important. From the modeling perspective, generalizability can be characterized by the proper representation of an underlying variability through a distributional assumption. For example, under the hierarchical framework ( ) for congruent and incongruent conditions, the cross-subject variability is captured by the subject-level effects   and   through a bivariate Gaussian distribution, while the cross-trial variability is represented by the trial-level effects through a Gaussian distribution with a variance   In contrast, the common practice of condition-level modeling in neuroimaging can be problematic in terms of generalizability at the condition level due to the implicit assumption of constant response across all trials ( ;  ), which contradicts the reality of large cross-trial variability. \n\nFor generalizability considerations, there should also be a floor for both subject and trial sample sizes as a rule of thumb. Various recommendations have been proposed for the minimum number of subjects, ranging (at present) from 20 ( ) to 100 ( ); these numbers are likely to depend strongly on experimental design, tasks, region(s) of interest (since effect magnitudes will likely vary across the brain), and other specific considerations. Similarly, no single minimum number of trials can be recommended across all studies, for the same reasons. Here, in a simple Flanker task we observed that the effect estimate fluctuated to some extent when the trial sample size changed from approximately 50 to 100 (see  ), although we note that the fluctuations were quite small in regions of strong statistical evidence. On the other hand, the same figure, along with  , shows that the statistical evidence largely kept increasing with the number of trials, even in regions that showed strong evidence at 50 trials. \n\nTest-retest reliability can be conceptualized as another type of generalizability: namely, the consistency of individual differences when examined as trait-like measures including behavior (e.g., RT) or BOLD response. Unlike population-level effects that are assumed to be \u201cfixed\u201d in a statistical model, test-retest reliability is characterized as the correlation of subject-level effects, which are termed as \u201crandom\u201d effects under the linear mixed-effects framework. The generalizability of reliability lies in the reference of subject-level effects relative to their associated population-level effects. For example, subject-specific effects characterize the relative variations around the population effects. A high reliability of individual differences in a Flanker task experiment means that subjects with a larger cognitive conflict effect relative to the population average are expected to show a similar RT pattern when the experiment is repeated. Due to their smaller effect size compared to population effects, subject-level effects and reliability are much more subtle and may require hundreds of trials to achieve a reasonable precision for reliability estimation ( ). \n\n\n### The extremes: \u201cbig data\u201d and \u201cdeep data\u201d \n  \nPartly to address the topics of replicability and generalizability, people have proposed both   big data   studies with a large subject sample size (thousands or more) and   deep (or dense) sampling   studies with a large trial sample size (hours of scanning, several hundreds or thousands of trials). In the former, the number of trials is rarely discussed, and similarly for the latter, the number of subjects is rarely discussed. In the current context of assessing the interrelations between the two sample sizes, this means that these two options represent the extreme tails of the hyperbolic asymptotes (e.g., see  ). Additional simulation results (not shown here), which simulated these two extreme scenarios of deep scanning (3 subjects and 5000 trials) and big data (10000 subjects and 20 trials), indicated that effect estimate and uncertainty follow the general patterns summarized in  \u2013 : a large sample size of either trials or subjects leads to reduced standard error; a minimum number of subjects is required to achieve a designated standard error at the population level. Unsurprisingly, effect estimation for deep scanning has large uncertainty while it is relatively precise for big data. \n\nBetween these two competing opinions, big data seems to be more popular. The goals of these initiatives are to detect effects that are potentially quite small in magnitude, as well as to examine demographic variables and subgroups. However, if the number of trials is not considered as a manipulable factor in these cases, an important avenue to increased statistical efficiency is missed. In other words, an extremely large number of subjects is not necessarily the most effective way to achieve high efficiency when considering the resources and costs to recruit and collect data. Even though such large numbers of subjects would lead to the statistical efficiency gain at an asymptotic speed of inverse parabolic relationship with the number of subjects, our investigation suggests that high efficiency could be achieved with substantially fewer subjects   if   the experiment was designed to leverage the two sample sizes. Additionally, as noted above, \u201cgeneralizability\u201d comes in multiple forms, and these studies, albeit many subjects, still run the risk of not being able to properly generalize to a population of trials or stimulus category. Given the enormous cost of scanning so many subjects, this could be a lost opportunity and be inefficient, both statistically and financially. These studies might be able to save resources by scanning fewer subjects while increasing the number of trials, namely by utilizing the   S   \u2212   T   trade-offs noted in this work. Hence, slightly smaller big data\u2014with a larger number of trials\u2014might be more cost-effective, similarly efficient, and generalize across more dimensions. \n\nThe other extreme of extensive sampling with a few subjects has gained attraction. For example,   collected 500 trials per condition during 100 runs among only three subjects and revealed strong evidence to support that most brain regions are likely engaged in simple visual and attention tasks.   argued that a large amount of within-subject data improves precision, reliability and specificity.   advocated the extensive sampling of a limited number of subjects for its higher productivity of revealing general principles. We agree that a large trial sample size with a few subjects does provide an unique opportunity to explore subject-level effects. However, we emphasize that, without an enough number of subjects to properly account for cross-subject variability, one intrinsic limitation is the lack of generalizability at the population level, as evidenced by the minimum number of subjects required for each particular uncertainty level (see   S  * in  ). Therefore, these kinds of studies will surely be useful for certain kinds of investigations, but the associated conclusions are usually limited and confined to those few subjects, and will not be able to generalize at the population level. \n\n\n### Limitations \n  \nWe have framed study design choices and statistical efficiency under a hierarchical framework for population-level analysis. It would be useful for researchers to be able to apply this framework directly for planning the necessary numbers of subjects and trials to include in a specific study. However, in addition to effect magnitude as required in traditional power analysis, in general we do not   a priori   know the parameter values in the hierarchical model (e.g.,   \u03c1  ,   \u03c3  ,   \u03c3   in the model ( )) that would make this possible. Moreover, as seen in the Flanker dataset here, these parameter values are likely heterogeneous across the brain. In general, study designs can be much more complicated: having more conditions can lead to a more complex variance-covariance structure, for example. Some evidence indicates that the role of sample sizes may be different for classifications (such as multivoxel pattern analysis). As the spatial distribution of the effect within a region is crucial in these analytical techniques, the accuracy of statistical learning is more sensitive to cross-voxel variability than to its cross-subject and cross-trial counterparts ( ). \n\nThere are limitations associated with a large number of trials. Even though increasing the number of trials can boost statistical efficiency, it must be acknowledged that this does increase scanner time and study cost. Additionally, adding trials must be done in a way that does not appreciably increase fatigue and/or habituation of the subject (particularly for youth or patient populations), otherwise the theoretical benefits to efficiency will be undermined. These practical considerations are important. Although, as noted above, in most cases adding one trial will be a noticeably lower cost than adding one subject, most studies fit scenarios where adding trials should effectively boost statistical efficiency. Splitting trials across multiple scans or sessions is one way in which this problem has been successfully approached in some studies with a larage trial samle size (e.g.  ;  ). \n\nThe incorporation of measurement error remains a challenge within the conventional statistical framework. Specifically, FMRI acquisitions do not measure BOLD response directly, even though that is typically the effect of interest. Instead the effect is estimated from a time series regression model at the subject level. Thus, the effect estimates at the subject level, as part of the two-stage summary-statistics pipeline, contain estimation uncertainty. Ignoring this uncertainty information essentially assumes that the measurement error is either negligible or roughly the same across subjects, an assumption that simply is not met in FMRI due to noise in the acquired data and imperfect modeling. Therefore, incorporating subject-level measurement error at the population-level can calibrate and improve the model fit. Another modeling benefit is that the uncertainty information could be used to better handle outliers: instead of censoring at an artificial threshold, outliers can be down-weighted by their uncertainty ( ). This modeling strategy has been widely adopted to achieve higher efficiency and robustness in traditional meta-analysis ( ). Even though the methodology has been explored in straightforward population-level analyses through condition-level modeling ( ;  ;  ), its adoption is algorithmically difficult for hierarchical modeling under the conventional statistical framework. However, it would be relatively straightforward to include subject-level measurement errors under the Bayesian framework ( ); yet, computational scalability presently limits its application to region-level, not whole-brain voxel-wise, analysis. \n\nFinally, here we only examined task-based FMRI with an event-related design. It is possible that cross-trial variability and other parameters might differ for block designs. As each block with a duration of a few seconds or longer can be conceptualized as a \u201cbundle\u201d of many individual instantaneous trials, a block design could be viewed as more efficient than its event-related counterpart ( ). In addition, the saturation due to the cumulative exposure, as expressed in the convolution of a block with the assumed hemodynamic response function, may also lead to much less cross-block variability compared to an event-related experiment. Resting-state and naturalistic scanning are other types of popular FMRI acquisitions. Although we do not know of specific investigations examining the joint impact of number of \u201ctrials\u201d (i.e., within-subject data) and subjects on statistical efficiency in resting-state or naturalistic scanning, we suspect that our rationale is likely applicable: the number of data points may play just as important of a role as the number of subjects in those cases ( ;  ). Questions have been raised about the minimal number of time points needed in resting state, but not from the point of view of statistical efficiency. These are large topics requiring a separate study. \n\n\n### Suggestions and guidance \n  \nBased on our hierarchical framework, simulations and data examination, we would make the following recommendations for researchers designing and carrying out task-based FMRI studies:\n   \nWhen reporting \u201csample size\u201d, researchers should be more careful and refer distinctly about \u201csubject sample size\u201d and \u201ctrial sample size\u201d. Each is distinct and important in its own right. \n  \nWhen reporting the number of trials in a study, researchers should clearly note the number of trials   per condition   and   per subject  . Too often, trial counts are stated in summation, and it is not clear how many occurred per condition. This makes it difficult to parse an individual study design or for meta analyses to accurately combine multiple studies. For example, one might have to track through total trials, find the total number of participants in the final analysis, and make assumptions about relative distributions of conditions. These steps had to be performed for many entries in a recent meta analysis of highly cited FMRI studies ( ), where a large number of papers did not even include   any   reporting of trial counts. The former situation is inexact and involves unwelcome assumptions, while the latter makes evaluating a study impossible. It should be easy for a researcher to report their trial counts per condition and per subject, to the benefit of anyone reading and interpreting the study. It would be important to provide descriptive statistics in scenarios where the final number of trials is different per subject, due to, for instance, the exclusion of trials based on subject response. \n  \nWhile we have studied the relation of statistical efficiency to subject and trial sample sizes, it is difficult to make an exact rule for choosing these sample sizes. Nevertheless, from the point of view of optimizing statistical efficiency, one could aim for a roughly equal number of trials and subjects. In practice, there are typically many more factors to consider, making a general rule difficult. However, adding more trials is   always   beneficial to statistical efficiency, and will typically improve generalizability. For example, if the resources for subjects are limited, an experiment of 50 subjects with 200 trials per condition is nearly efficient as a design of 100 subjects with 100 trials (or 500 subjects with 20 trials) per condition. \n  \nIn addition to the statistical efficiency perspective, one should also consider generalizability, which would put a floor on both trial and subject sample sizes. It is difficult to create a single rule for either quantity, given the large variability of study designs and aims; indeed, suggestions for a minimum number of subjects have ranged from 20 to 100 (and will likely continue to fluctuate). As for trial sample size, we consider 50 as a minimum necessary number for a simple condition (e.g., the Flanker task). With a more varied task, such as displaying faces that can have a wider range of subtle but potentially important variations, using a larger number of trials would likely improve generalizability. \n  \nWhen choosing a framework between trial-level and condition-level modeling, the former is typically preferable. But the latter could be adopted when the trial sample size is reasonably large (i.e., > 50 \u2013 100), since one might expect similar results, and condition-level modeling has the advantage of less computational burden. For smaller trial sample sizes, trial-level modeling shows clear benefits in terms of generalizability and accuracy of effect uncertainty; it is also quite computationally feasible in this range. \n  \n\n\n\n## Conclusion \n  \nFor typical neuroimaging and behavioral experiments, the trial sample size has been mostly neglected as an unimportant player in optimizing experimental designs. Large multi-site \u201cbig data\u201d projects have proliferated in order to study small to moderate effects and individual differences. Through a hierarchical modeling framework, simulations, and an experimental dataset with a large number of trials, we hope that our investigation of the intricate relationship between the subject and trial sample sizes has illustrated the pivotal role of trials in designing a statistically efficient study. With the recent discovery that cross-trial variability is an order of magnitude higher than between-subject variability, a statistically efficient design would employ the balance of both trials and subjects. Additional practical factors such as subject tolerance and cost/resources would also need to be considered, but the importance of trial sample size is demonstrated herein. \n\n \n", "metadata": {"pmcid": 9636536, "text_md5": "411486782eb1e71e337fa811649efbc4", "field_positions": {"authors": [0, 136], "journal": [137, 147], "publication_year": [149, 153], "title": [164, 260], "keywords": [274, 274], "abstract": [287, 2150], "body": [2159, 79851]}, "batch": 2, "pmid": 34906711, "doi": "10.1016/j.neuroimage.2021.118786", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9636536", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9636536"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9636536\">9636536</a>", "list_title": "PMC9636536  Hyperbolic trade-off: The importance of balancing trial and subject sample sizes in neuroimaging"}
