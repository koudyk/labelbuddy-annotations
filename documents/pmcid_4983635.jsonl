{"text": "Ramasubbu, Rajamannar and Brown, Matthew R.G. and Cortese, Filmeno and Gaxiola, Ismael and Goodyear, Bradley and Greenshaw, Andrew J. and Dursun, Serdar M. and Greiner, Russell\nNeuroimage Clin, 2016\n\n# Title\n\nAccuracy of automated classification of major depressive disorder as a function of symptom severity\n\n# Keywords\n\nMajor depression\nSeverity of symptoms\nDiagnosis\nFunctional magnetic resonance imaging\nMachine learning\nClassification\nSupport vector machine\n\n\n# Abstract\n \n## Background \n  \nGrowing evidence documents the potential of machine learning for developing brain based diagnostic methods for major depressive disorder (MDD). As symptom severity may influence brain activity, we investigated whether the severity of MDD affected the accuracies of machine learned MDD-vs-Control diagnostic classifiers. \n\n\n## Methods \n  \nForty-five medication-free patients with DSM-IV defined MDD and 19 healthy controls participated in the study. Based on depression severity as determined by the Hamilton Rating Scale for Depression (HRSD), MDD patients were sorted into three groups: mild to moderate depression (HRSD 14\u201319), severe depression (HRSD 20\u201323), and very severe depression (HRSD \u2265\u00a024). We collected functional magnetic resonance imaging (fMRI) data during both resting-state and an emotional-face matching task. Patients in each of the three severity groups were compared against controls in separate analyses, using either the resting-state or task-based fMRI data. We use each of these six datasets with linear support vector machine (SVM) binary classifiers for identifying individuals as patients or controls. \n\n\n## Results \n  \nThe resting-state fMRI data showed statistically significant classification accuracy only for the   very severe depression   group (accuracy 66%, p\u00a0=\u00a00.012 corrected), while   mild to moderate   (accuracy 58%, p\u00a0=\u00a01.0 corrected) and   severe depression   (accuracy 52%, p\u00a0=\u00a01.0 corrected) were only at chance. With task-based fMRI data, the automated classifier performed at chance in all three severity groups. \n\n\n## Conclusions \n  \nBinary linear SVM classifiers achieved significant classification of very severe depression with resting-state fMRI, but the contribution of brain measurements may have limited potential in differentiating patients with less severe depression from healthy controls. \n\n   Highlights  \n  \nSVM binary classifiers achieved significant classification of very severe depression with resting state fMRI data. \n  \nPrefrontal, anterior cingulate and insula were the most discriminative brain regions. \n  \nNo significant classification could be achieved for less severe MDD with resting state data. \n  \nWith emotional task data, SVM classifier performed at chance for all MDD severity groups. \n  \n \n\n# Body\n \n## Introduction \n  \nMajor depressive disorder (MDD) is a complex brain disorder associated with dysregulation of distributed neuronal networks involving several cortical and limbic regions. This position is based on evidence from the neuroimaging literature that has documented distinct structural and functional alterations in patients with MDD compared to healthy controls ( ,  ,  ). However, these group-level inferences have had minimal impact on clinical translation at the individual patient level \u2013 that is, they do not directly lead to a way to determine whether a specific subject has MDD or not. Recently, machine learning techniques have been applied to neuroimaging data to draw inferences for individual subjects, with the potential for improving patient-specific clinical diagnostic and treatment decisions ( ,  ). Current diagnosis of mental disorders is based on diagnostic criteria drawn from self-reported clinical symptoms without any objective biomarkers. This has led to the search, in recent years, for a diagnostic system that can use objective measurements from a subject's brain to validate and improve the accuracy of psychiatric diagnosis. \n\nIn the last decade, several neuroimaging studies have examined the classification accuracy of machine learned classifiers in differentiating patients with MDD from healthy controls. One major focus has been the application of machine learning techniques to magnetic resonance imaging (MRI) data, including both structural and function MRI (fMRI) data. Machine learning is a sub-area of artificial intelligence that applies statistical methods to training data, such as high dimensional neuroimaging data, to find patterns that can distinguish patients from healthy controls. Authors reported classification accuracy for MDD ranging from 67 to 90% using structural MRI data ( ,  ,  ), 94% using resting-state fMRI data ( ,  ), 67\u201386% using task-related fMRI data ( ,  ,  ) and 76.3% using combined structural and functional MRI data ( ). High accuracy prediction is clinically important, as MDD is heterogeneous in symptom profile and prone to clinician bias with poor inter-rater reliability ( ). The identification of MDD subtypes based on neural abnormalities or brain imaging methods might improve classification accuracy, facilitate new drug discovery and move toward stratified medicine. \n\nDepression subtypes defined by symptom severity have several clinical implications for the treatment and prognosis. For example, baseline symptom severity is associated with drug-placebo differences in randomized control trials ( ) and antidepressants are recommended as the choice of treatment for severe depression whereas psychosocial interventions as the choice of treatment for mild-moderate subthreshold depression (NICE guidelines CG90, 2009). Additionally, epidemiological studies have shown the association of symptom severity with functional impairment, co-morbidity and increased risk of mortality ( ,  ,  ). In machine learning approaches, severity-related brain abnormalities have been shown to offer good discriminating potential in the classification of MDD and healthy controls. In emotional task fMRI data,   found significant correlations between the distance of participants' feature vectors from the separating hyperplane of a trained support vector machine, and those participants' severity scores from the Hamilton Rating Scale for Depression (HRSD) ( ), which suggests a relationship between depression severity and test predictions ( ). Similarly, another study using structural MRI data reported a strong relationship between the fitted SVM weights and ratings of illness severity ( ). These findings suggest that fitted machine learned classifiers may capture patterns of brain abnormality in functional and structural neuroimaging data related to MDD severity. A model derived from a machine learned classifier may constitute an objective biomarker for depression severity. To date, no previous study has examined how the performance of machine learning algorithms in differentiating MDD vs. health may differ as a function of MDD symptom severity. This research question has important clinical implications in the context of whether machine learning approaches using fMRI data can yield comparable accuracy in the classification of MDD at various levels of severity. \n\nWe examined the accuracy of two-class machine learning classification of three distinct groups of MDD patients, with different levels of symptom severity based on the HRSD Scores, versus healthy controls. The three groups of MDD with severity gradation were: mild to moderate depression (HRSD score 14\u201319), severe depression (HRSD 20\u201323), and very severe depression (HRSD \u2265\u00a024). (While there is no consensus on cutoff scores on the HRSD for identifying MDD severity subtypes, these severity ranges are consistent with several published recommendations ( ,  ,  )). We expected that the classifiers would achieve higher accuracy for the patient groups with very severe depression compared to those with severe depression or mild-moderate depression. For each range of severity, we also considered two types of fMRI data \u2013 from either resting-state or from an emotional-face matching task \u2013 hence, we examined classifier performance for 3\u00a0\u00d7\u00a02 different situations. \n\n\n## Materials & methods \n  \n### Participants \n  \nEthics approval was obtained from the local review board. All participants were fluent in English and gave informed, written consent to participate in the study. Forty-five patients meeting DSM-IV criteria for MDD ( ) according to the Structured Clinical Interview for DSM-IV Axis 1 Disorders ( ), were recruited through advertisements. (See   for participant demographics). Patients included 29 females and 16 males, all right-handed, in the age range of 19\u201358\u00a0years (mean 37\u00a0\u00b1\u00a011 SD). The Edinburgh Handedness Inventory was used to assess handedness ( ). The severity of depressive and anxiety symptoms was assessed using the clinician-administered, 17-item Hamilton Rating Scale for Depression ( ), the Montgomery Asberg Depression Rating Scale (MADRS) ( ), and the Hamilton Anxiety Rating Scale (HAM-A) ( ). Patients were also rated for disease severity using the Clinical Global Impression (CGI) scale ( ), which allows clinicians to provide a severity rating based on their clinical experience. Patients were included in the study if they met the following inclusion criteria: (1) acute episode of MDD of unipolar subtype and a score of 14 or higher on the HRSD, and (2) free of psychotropic medication for a minimum of three weeks at time of recruitment. Exclusion criteria were: (1) Axis I disorders such as bipolar disorder, anxiety disorder, or psychosis, (2) history of substance abuse within six months of study participation, (3) borderline personality disorder, (4) medical or neurological disorders, (5) severe suicidal symptoms, (6) failure to respond to three trials of antidepressant medication, or (7) contraindications for MRI (metal implants, pregnancy, etc.). Patients were divided into three MDD severity groups based their HRSD scores. The mild-moderate group (HRSD 14\u201319) included 12 patients. The severe group (HRSD 20\u201323) included 18 patients. The very severe group (HRSD 24\u00a0+) included 15 patients. \n\nNineteen healthy controls, matched for gender (11 females, 8 males) and age (20\u201352\u00a0years, mean 33 +/ 10 SD), were also recruited for the study through advertisements. These participants were screened using the Structured Clinical Interview for DSM\u2013IV Axis I Disorders, non-patient version, to ensure they did not have previous or current Axis I psychiatric disorders ( ) nor any family history of Axis I disorders, as determined by self-report. The control's HRSD scores ranged from 0 to 7. The demographics of the MDD patients and healthy controls are summarized in  . \n\n\n### MRI data acquisition \n  \nMR images were collected using a 3 Tesla General Electric MR scanner (Signa VHi; General Electric Healthcare, Waukesha, WI, USA) equipped with an eight-channel, phased-array head coil. For each participant, two resting-state fMRI scans of 220\u00a0s in duration were acquired using a single-shot gradient-recalled echo, echo planar imaging sequence (110 volumes, repeat time (TR) 2000\u00a0ms, echo time (TE) 30\u00a0ms, flip angle 65\u00b0, field of view (FOV) 240\u00a0\u00d7\u00a0240\u00a0mm squared, matrix size 64\u00a0\u00d7\u00a064, in-plane resolution 3.75\u00a0mm, 30 axial slices, 4\u00a0mm slice thickness). For the resting-state collection, participants were required to remain in the MRI scanner with their eyes open and fixated on a black crosshair at the center of a projection screen. The participants were instructed to relax, not think about anything in particular, and not to fall asleep. In addition, four emotional face task fMRI scans were collected per scanning session (for each subject), lasting 300\u00a0s each (150 volumes, TR 2000\u00a0ms, TE 30\u00a0ms, flip angle 65\u00b0, FOV 240\u00a0\u00d7\u00a0240 mm squared, matrix size 64\u00a0\u00d7\u00a064, in-plane resolution 3.75\u00a0mm, 30 axial slices, slice thickness 4\u00a0mm). A T1-weighted structural MRI (TR 9.2\u00a0ms, TE minimum, flip angle 20\u00b0, FOV 256\u00a0\u00d7\u00a0256 mm squared, matrix size 512\u00a0\u00d7\u00a0512, in-plane resolution 0.5\u00a0mm, 176 sagittal slices, slice thickness 1\u00a0mm) was also acquired for anatomical registration of the fMRI data. \n\n\n### fMRI emotional-face matching task paradigm \n  \nWhile undergoing fMRI brain imaging, participants viewed triads either of faces or of control geometrical designs during a series of trials ( ). Each face had one of four emotional expressions: angry, fearful, happy, or sad. For each face triad, participants used a button box to indicate which of two target faces depicted the same emotion as the source face. Similarly, for control condition, participants responded with button press to indicate which of two geometrical designs matched with source geometrical design. Each fMRI run included 60 trials (12 for each of the four assessed emotions and the control condition). The order of presentation was randomized and each individual trial lasted 5\u00a0s (images: 3\u00a0s; inter-trial interval: 2\u00a0s). Stimulus onset asynchrony between successive trials was jittered (5\u00a0s or more in random increments of 0.5\u00a0s) to preserve fMRI signal variance ( ). Previous work has shown that this emotional face matching task compared to control condition engages affective processing mechanisms, and reliably activates the amygdala and other relevant prefrontal and cingulate regions ( ). \n\n\n### Pre-processing \n  \nWe considered two fMRI datasets, each involving all of the subjects, both control and MDD: one for resting-state, and another for the emotional face task. Each dataset was preprocessed using SPM8 (Wellcome Trust Centre for Neuroimaging, London, UK) and in-house code written in MATLAB (The MathWorks, Inc., Natick, MA, USA). The preprocessing steps for fMRI data included: (1) 6 parameter rigid body motion correction of fMRI volumes in SPM8, (2) non-linear spatial warping to MNI EPI template at 4\u00a0\u00d7\u00a04\u00a0\u00d7\u00a04\u00a0mm cubed resolution (43\u00a0\u00d7\u00a051\u00a0\u00d7\u00a037 voxels grid) in SPM8, and (3) 8\u00a0mm full width at half maximum (FWHM) Gaussian spatial smoothing of fMRI volumes in SPM8. The pre-processed registered fMRI data were masked to exclude voxels outside the brain using a hand-built mask. This mask retained 26,904 voxels (1,513,406\u00a0mm ) out of the 81,141 voxels in the interpolated fMRI volume space. \n\n\n### Dataset notation \n  \nTo facilitate description of our analysis, we define a simple notation. We performed six analyses, using resting-state fMRI data or emotional face task fMRI data from one of the three patient severity groups (mild-moderate MDD, severe MDD, and very-severe MDD) as well as controls. We will use the phrase \u201cdataset S\u201d to refer to the dataset used in a given analysis. Therefore, dataset S consisted of either resting-state or emotional face task fMRI data from the patients in a given severity group as well as controls. \n\n\n### Overview of machine learning approach \n  \nWe ran six analyses, each testing the ability of machine learning to produce classifiers that could effectively differentiate between healthy controls and MDD patients from one of the three MDD severity groups, using either resting-state fMRI or emotional face task fMRI data. For each, we ran the LearnFMRI process, which selected one out of five different feature extraction algorithms as well as the regularization parameter value for the linear SVM learning algorithm (all described below). LearnFMRI then ran this particular choice of algorithm and regularization parameter value on all of the training data to produce a classifier, which could then be used to diagnose a future subject; see  . \n\nWe now provide a detailed explanation of the LearnFMRI procedure. To reduce the dimensionality of the fMRI data, our LearnFMRI system selects one of five different feature extraction algorithms for each of the 6 datasets S (each hand-coded in MATLAB): (1) independent components analysis (ICA) whole brain map feature extraction, ICA-Whole-FE; (2) ICA significant cluster feature extraction, ICA-Clust-FE; (3) pair-wise correlation feature extraction, PairCor-FE; (4) general linear model (GLM) analysis whole brain map feature extraction, GLM-Whole-FE; and (5) GLM significant cluster feature extraction, GLM-Clust-FE. (LearnFMRI considered only ICA-Whole-FE, ICA-Clust-FE and PairCor-FE for resting-state fMRI data, and all five for task-based datasets.) Feature extraction algorithms ICA-Clust-FE and GLM-Clust-FE used statistical testing between patients and controls to extract features (voxel clusters) that were significantly different between the groups. To reduce the potential for overfitting, it performed statistical comparisons only between patients and controls in training sets (see   section below). Therefore, different sets of participants (i.e. only the training set participants) contributed to these statistical tests in different folds of the nested cross-validation described below. Statistical maps differed between folds. These differences are illustrated in  . The ICA-Whole-FE, PairCor-FE and GLM-Whole-FE algorithms did not use statistical testing between patients and controls to generate features. Details of feature extraction algorithms are provided below. \n\nFor each task, LearnFMRI also tested the linear support vector machine (SVM) learning algorithm with regularization parameter values 0.1, 0.3, 1.0, 3.0, or 10.0 and selected the best-performing parameter value. \n\nTesting multiple combinations of feature extraction and classifier algorithms on the test data and then presenting only the algorithms that perform best on that data may create a substantial danger of   overfitting  , where an algorithm works well because it is matching the specific pattern of noise that happens to be present in the dataset tested. This good performance does not generalize to new data with different noise patterns. (Note that this overfitting is in terms of the   choice   of algorithm; this is still a problem even when cross-validation is used to protect against overfitting in terms of the algorithms' learned weight values.) LearnFMRI therefore used internal cross-validation to protect against overfitting with respect to (1) the choice of feature extraction algorithm, (2) the extracted features (feature extraction used the patient/control labels), (3) the choice of regularization parameter value for the linear SVM learning algorithm and (4) the weights chosen by the linear SVM learning algorithm. \n\nOur LearnFMRI system is summarized in  . Given the set of labeled training data for each dataset S, LearnFMRI considers each combination of feature extractor and regularization parameter and returns the best-performing choice of feature extraction algorithm as well as a linear SVM classifier trained using the best regularization parameter value. To estimate the generalization performance of the chosen feature extraction algorithm and trained linear SVM classifier, we used a five-fold cross-validation process, repeated ten times, with different random partitioning of participants into the five folds. Note that different cross-validation folds found different best combinations of feature extraction/regularization parameter. This cross-validation process estimates the accuracy not of a single machine learned classifier but of the entire process of selecting the feature extraction algorithm and regularization parameter value and training the linear SVM classifier. \n\n\n### Feature extraction preliminaries \u2013 cluster growing algorithm \n  \nThe ICA-Clust-FE and GLM-Clust-FE feature extraction algorithms each identify significant clusters in statistical parametric maps (details provided below). Each uses the following automated region-growing algorithm to identify clusters: Given a 3D statistical parametric map, (t-map generated by comparing patients vs. controls in terms of ICA maps values or GLM beta weight values), the cluster-growing algorithm grows a cluster around each positive or negative statistical peak (local extremum) in the map. Specifically, it uses each peak voxel as a seed of the cluster, and then adds neighboring, statistically significant voxels to the growing cluster one-at-a-time, until the algorithm encounters either non-significant neighbor voxels or significant voxels that have already been added to another growing cluster. If two statistical peaks are too close together (within 10\u00a0mm of each other), the less-significant peak is not used as a cluster seed. This prevents large \u201chills\u201d of significant voxels that happen to have two or more peaks that are close together from being divided into multiple smaller clusters. \n\n\n### Independent components analysis (ICA) feature extraction \u2013 ICA-Whole-FE, ICA-Clust-FE \n  \nOur ICA feature extraction algorithms, ICA-Whole-FE and ICA-Clust-FE, are both based on the ICA procedure of  . Briefly, 15 ICA \u201cconnectivity maps\u201d are computed. ICA-Whole-FE simply combines a given participant's ICA map values into one long feature vector for that participant. ICA-Clust-FE extracts significant voxel clusters by comparing patients vs. controls for each of the 15 ICA maps. Note that ICA-Whole-FE does not use the participant labels (patient or control), whereas ICA-Clust-FE does use the labels. We applied ICA-Whole-FE or ICA-Clust-FE to resting-state or task-based fMRI data separately. Details of the algorithms are provided in Appendix A. \n\n\n### Pair-wise correlation feature extraction \u2013 PairCor-FE \n  \nThe Harvard-Oxford atlas includes 26 prefrontal regions involved in executive control and/or emotion regulation (see  ); Previous literature suggest that changes in these regions have been associated with major depressive disorder ( ). PairCor-FE defined each participant's feature vector as the 325 pair-wise correlations among those regions' time courses, and al computed these correlation features separately for either the resting-state or task-based fMRI data, as follows. All fMRI data runs for the participant were concatenated along the time axis (two runs for resting-state data, four runs for task-based data). The mean fMRI activation time course was computed for each region (mean across voxels in the region). The Pearson correlation coefficient was computed for each pair of time courses among all 325 pairs of different regions; this 325-tuple of correlation values was the feature vector for the participant, which were used by the classifier; see the   section below. \n\n\n### GLM analysis feature extraction for task-based data \u2013 GLM-Whole-FE, GLM-Clust-FE \n  \nThe GLM-Whole-FE and GLM-Clust-FE feature extraction algorithms were used with emotional-face task fMRI data only. Both involve the standard General Linear Model (GLM) analysis, based on the following statistical contrasts:   \nlocalizer contrast (sum of all five trial types), \n  \nemotional faces - geometric stimuli, \n  \npositive - negative emotional faces (happy - other emotional faces), and \n  \nnegative faces - geometric stimuli (where negative faces included angry, fearful, and sad faces). \n  \n\nGLM-Whole-FE combines the four (first-level) contrast maps for a given participant into one long feature vector for that participant. GLM-Clust-FE compares contrast maps in terms of patients vs. controls and extracts significant clusters for each map. See Appendix B for details. \n\n\n### Base-learner: linear SVM \n  \nOur LearnFMRI learning algorithm uses the linear support vector machine (SVM) learning algorithm to create trained linear SVM classifiers. We used the LIBSVM implementation of the linear SVM learning algorithm and classifier, along with in-house MATLAB code for all data manipulation, cross-validation book-keeping, and accuracy computations. \n\n\n### Machine learning algorithm \u2013 LearnFMRI \n  \nGiven a labeled training dataset, LearnFMRI will produce a classifier that can accurately classify novel participants. As shown in  , LearnFMRI first selects one feature extraction algorithm (which is one of ICA-Whole-FE, ICA-Clust-FE, PairCor-FE, GLM-Whole-FE or GLM-Clust-FE) as well as the linear SVM regularization parameter   C  \u00a0\u2208\u00a0{0.1,\u00a00.3,\u00a01.0,\u00a03.0,\u00a010.0}. LearnFMRI uses an internal cross-validation to find the appropriate feature selection and regularization parameter; see  . This involves trying each specific feature selector and regularization parameter on a portion of the training data and evaluating the performance on the remaining subset. (This is repeated four times; see \u201cIllustration\u201d section below.) After finding the best choice of feature selector and regularization parameter, LearnFMRI then uses these \u201csettings\u201d to train the classifier, using all of the training data. It then returns that resulting trained classifier. \n\n\n### Classifier performance \n  \nFor each dataset S, our goal is a single classifier (SVM*) that can accurately diagnose novel participants \u2013 that is, participants who were not in the training set. To estimate the expected out-of-sample (generalization) accuracy of this classifier SVM* \u2013 the result of running LearnFMRI on all of the training data from a given dataset S \u2013 we used five-fold cross-validation; see  . For each of the five folds, approximately one fifth of the participants was held out as a test set, with the remaining four-fifths comprising the training set. Test and training sets were balanced as closely as possible for proportions of patients versus controls. Five-fold cross-validation was repeated ten times with different random assignments of participants to the five folds. Note that this cross-validation ran the entire LearnFMRI learning algorithm for each fold, which in turn used internal cross-validation steps inside it \u2013 i.e. nested cross-validation inside the outer five-fold cross validation. The use of nested cross-validation was important for protecting against overfitting in the selection of the feature extraction algorithm, cluster selection from statistical testing (patients vs. controls), and choice of regularization parameter for the linear SVM base-learner. \n\nWe quantified the performance of these classifiers using multiple measures: accuracy, sensitivity, specificity, balanced accuracy, positive predictive value, and negative predictive value (all measures were based on cross-validation). As described above, there were six analyses: mild-moderate MDD (respectively, severe MDD or very severe MDD) patients vs. controls, using either resting-state or task-based fMRI data. For each of these analyses, each of the participants used in that analysis was present in the (outer) test set in precisely one iteration of outer cross-validation, on each of the ten repetitions (see above). Thus, each participant's data underwent ten classification attempts. For each participant, we computed the proportion of correct classification attempts. Accuracy was computed as the mean proportion of correct classification attempts across all participants. Sensitivity was computed as the mean proportion of correct classification attempts for patients (true positives), and specificity was computed as the mean proportion of correct classification attempts for controls (true negatives). Balanced accuracy was computed as the mean of sensitivity and specificity. Positive predictive value (and negative predictive value, respectively) was computed as the proportion correct among positive (respectively, negative) predictions. \n\nFor each of the six analyses, mean accuracy values were compared against chance accuracy using one-tailed bootstrap statistical tests on participants' proportion of correct classification attempts values. Chance accuracy was derived from randomly guessing the participant class (patient/control) weighted by the relative proportions of patients and controls in the given analysis. Specifically, let r\u00a0=\u00a0proportion of patients\u00a0=\u00a0#patients\u00a0/\u00a0(#patients\u00a0+\u00a0#controls), which is in the range [0,1]. Then random accuracy\u00a0=\u00a0r \u00a0+\u00a0(1\u00a0\u2212\u00a0r) , which is in the range [0,1]. Chance accuracy values ranged from 50 to 53% depending on the numbers of patients and controls used in each analysis. The alpha-value (false positive rate under the null hypothesis of chance accuracy) was set a 0.05. Multiple comparison correction was performed using the Bonferroni method (i.e. multiplying the individual p-values by the number of tests; 6 in this case). \n\n\n### Illustration of the overall learning\u00a0+\u00a0evaluation process \n  \nWe provide a detailed illustration, for a given run of five-fold cross-validation (i.e. set of all five iterations of five-fold cross-validation). See  . Here, we first divided the participants into five folds, approximately balanced for proportion of patients and controls. On the i-th iteration of outer cross-validation, we held out the i-th fold as the test set (i.e. outer test set). All participants not in fold i were used as the training set for that iteration (i.e. outer training set input to the learning algorithm LearnFMRI). LearnFMRI then computed accuracy scores for each combination of feature extraction algorithm and regularization parameter. To do so, the learning algorithm employed a four-fold cross validation (inner cross-validation) analysis for each possible combination. For a given combination, on the j-th iteration of inner cross-validation, we held out the j-th fold as the inner test set. All participants not in folds j or i were used as the inner training set for that inner iteration. Statistical comparisons between patients and controls during feature extraction were performed only on participants from the inner training set. The resulting statistical differences were used to extract features for the inner test set participants without using those participants' labels (patient versus control). The classifier was trained on the inner training participants (those not in either fold j or i) and tested on the inner test participants (in fold j). Accuracy results were averaged over the four inner cross-validation folds. In this way, (inner) cross-validated accuracy scores were computed for each combination of feature extraction and regularization parameter. LearnFMRI then chose the best combination, defined as that combination yielding the highest accuracy (proportion of correctly classified participants) over the four-fold inner cross-validation tests. That best combination specified the feature extraction algorithm and regularization parameter, which were then applied to all the participants in the outer training set (i.e. all participants not in fold i), resulting in a trained linear SVM classifier. The choice of feature extraction method and the trained classifier are the output of the learning algorithm. Their performance was then tested on participants in the outer test set (i.e. participants in fold i). \n\n\n### Visualization of machine learning analysis \n  \nTo gain insight into the automated diagnosis process, we analyzed the classifier weights for various fMRI-based features. The linear SVM learning algorithm produces a \u201cweight\u201d for each feature, which recall corresponds to a value extracted from one voxel or region or the correlations between two regions (see descriptions of feature extraction algorithms above). The weights for this classifier are presented in the   section of the Results. \n\nTo visualize which brain regions a classifier used, we created a weight map by weighting each relevant region by the absolute value of its appropriate weight value. We did this only for the analysis of patients with very-severe MDD vs. controls using resting-state fMRI data, as this was the only analysis that performed significantly above chance. \n\n\n\n## Results \n  \n### Demographics \n  \nThere were no significant differences between healthy controls and MDD patients in sex or age ( ). There were no significant differences among the three patient groups (mild-moderate MDD, severe MDD, and very-severe MDD) in terms of sex, age, age of MDD onset, illness duration, or duration of current MDD episode ( ). As expected, there were group differences in HRSD scores between patients and controls and among the three patient groups. There were also significant differences in MADRS scores, HAM-A scores, CGI scores among the three patient groups, which is consistent with severity categories defined by HRSD scores. \n\n\n### Classification results \n  \nBased on ten repetitions of five-fold cross-validation, classification using resting-state fMRI data comparing MDD patients with very severe depression vs. controls achieved a sensitivity of 59%, specificity of 72% and accuracy of 66% ( ). This accuracy value was significantly above chance (p\u00a0=\u00a00.012, Bonferroni corrected for the 6 tests). Classification analyses using resting-state fMRI data with patients in the mild-moderate and severe depression groups did not achieve accuracies significantly above chance ( ). Interestingly, accuracies were not significantly above chance for classification using the emotional face task fMRI data for any of the three patient groups ( ). \n\n\n### Discriminating brain regions \n  \nWe applied LearnFMRI to resting-state fMRI data from all patients with very-severe MDD and healthy controls to derive one model. In this case, the learning algorithm selected pair-wise correlation feature extraction and the SVM regularization parameter value of C\u00a0=\u00a00.1. The regions considered for pair-wise correlation features come from the Harvard-Oxford atlas and are listed in  .   shows these regions, colour-coded based on the learned classifier weights and superimposed on one participant's anatomical scan.   shows the learned weights from the trained linear SVM classifier for all 325 pair-wise correlation features from the analysis of very severe depression vs. healthy controls using resting-state fMRI data. (There are subtleties in interpreting weight values from trained classifiers. See   for discussion. \n\n\n\n## Discussion \n  \nIn this study, we evaluated the performance of two-class automated classification (healthy controls vs. patients) for three groups of patients with MDD: mild-moderate MDD, severe MDD, and very-severe MDD. The main finding is that using pattern analysis of resting-state fMRI activity, the accuracy of learned classifier was significantly better than chance at classifying very severe depression versus healthy controls. However, the performance of the classifiers for distinguishing healthy versus mild-moderate depression and healthy versus severe depression, were only at the chance level. Another important finding is that fMRI activation patterns evoked by the emotional face processing task failed to show significant classification performance, for any of the MDD severity groups. Given the small sample size, our results should be considered as preliminary. \n\nThe finding of higher classification accuracy for very severe depression is consistent with previous machine learning studies that showed significant correlations between prediction scores and symptom severity scores using structural and functional data. The classification accuracy of 66% for very severe depression is comparable to that of previous studies using working memory neural correlates and structural data ( ,  ). However, contrary to our results, those previous studies, using similar supervised SVM learners, could significantly distinguish controls from MDD with moderate severity (mean HRSD: 21\u201322) ( ,  ,  ). The inconsistencies in results could be partly explained by variations in methodology and MRI data, as we used resting-state fMRI data whereas those previous studies used structural MRI and emotional recognition task-dependent fMRI data. Given the lower accuracies for the classification of less severe depression groups, our results suggest that less severe forms of MDD may be heterogeneous and is likely to capture mild forms of depressive states such as dysthymia and anxiety or personality weighted conditions. As less severe forms of depression may be associated with mild brain abnormalities, it might be harder for the learning algorithm to find a meaningful boundary between these groups and controls in a small dataset. We may need larger sample to improve the power and enable the classifiers to distinguish these groups from healthy controls. Considering that the need for machine learning methods in the diagnosis of milder depression would be greater in clinical practice than that of more severe form of depression, the poor accuracy in the classification of milder depression by machine learning methods shown in this study may limit its use as a tool in the early detection of milder or subthreshold depression. However, results based on small sample size precludes any conclusions on clinical utility. In addition, although our current classifier yielded significant classification for very severe depression, the clinical utility of this current system may be limited by its modest specificity (72%). Again, this needs to be tested in larger and independent samples. \n\nThe brain regions that contributed to the classification of very severe depression included the various prefrontal and limbic regions listed in  . These regions have been reported to have abnormal structure and function in group-level analyses between patients with MDD and healthy control ( ,  ,  ). Moreover, the resting-state functional connectivity between prefrontal, insula and anterior cingulate regions was found to be positively correlated with severity of depression in univariate analysis ( ,  ), which is consistent with our findings, and suggests the greater contribution of these networks in the classification of very severe depression from healthy controls. \n\nAlthough previous fMRI studies using univariate analysis showed significant correlation between severity of depressive symptoms and alteration in regional brain activity due to emotional tasks or stimuli, our results failed to show significant accuracy in distinguishing healthy controls from depression patients, grouped at three levels of severity. Of course, this may be due to the different objectives, as univariate correlations (at the class level) are neither sufficient nor necessary for effective classification performance. In addition, this behavior could be due to low reliability of the task or poor variance of task-related activation between the three depression groups and the control group. Alternatively, this may be due to the small sample sizes here, coupled with the complexity of the emotional task. Although this is the first study to use an emotional-face matching task in fMRI machine learning analysis, several studies used this paradigm to elicit responses in neural regions and circuits implicated in emotional processing ( ,  ). Previously published fMRI machine learning studies ( ,  ) used an emotional face recognition task that is more cognitively/perceptually demanding than the emotional face matching task used here. In conclusion, our findings suggest that the pattern of resting-state fMRI BOLD signals produced better classification of severe MDD than the fMRI patterns evoked by the emotional face matching task. \n\nThe reasons for the better performance of the classifier using resting-state data than task related data remains speculative and could be related to the abnormalities of the default mode network (DMN) in MDD. DMN refers to spontaneously organized brain activities from a network of brain regions including anterior cingulate cortex, medial prefrontal cortex, posterior cingulate cortex, precuneus, and inferior parietal lobule ( ), which is activated during rest and deactivated during active tasks ( ). Previous studies of MDD showed increased resting-state functional connectivity of the DMN areas especially in anterior cingulate and medial prefrontal regions ( ) and decreased functional connectivity in bilateral prefrontal areas of DMN during emotional processing tasks ( ). Furthermore, higher levels of rumination about depressive symptoms was found to be correlated with higher DMN dominance ( ) and severe depressive symptoms ( ). It is therefore possible that the increased levels of rumination and associated increased DMN activity during the resting stage may have contributed for the greater performance of the classifier for very severe depression, whereas the lack of activation in DMN due to reduction in rumination during the engagement with the task may partly explain the poor performance of the classifier with task related data. \n\n### Methodological issues \n  \nAs mentioned above, a major limitation of the study is the small sample size, which might have influenced our results. Although previous machine learning studies in MDD achieved higher accuracies using small datasets ( ,  ,  ), yet larger studies in two independent samples are needed to develop and test predictive models that are sufficiently stable to use in clinical practice. Recent machine learning studies using structural MRI have recommended participant groups with 130 participants or more per group to learn an effective classification for schizophrenia versus healthy controls ( ). However, there are no clear guidelines on required sample sizes for machine learning studies using resting and task-related fMRI data in patients with MDD. Additionally, owing to our unbalanced sample between MDD (N\u00a0=\u00a045) and healthy controls (N\u00a0=\u00a019), we did not examine the accuracy of classification of MDD as a single group vs. healthy controls. Another major methodological issue is the categorization of MDD severity groups based on HDRS scores. As mentioned previously, there is no consensus on the validity of cutoffs on HDRS for defining the severity categories. The American Psychiatric Association (APA) Handbook of Psychiatric Measures recommended the following thresholds to define grades of severity on HRSD: mild to moderate \u2264\u00a018, severe 19\u201322, very severe \u2265\u00a023 ( ). In contrast, others have used 20 as the cutoff to distinguish severe depression from mild to moderate ( ) and 24 or 25 as the cutoff to distinguish severe from very-severe depression ( ,  ). As there is very limited empirical research in this area, we used other severity measures such as MADRS and CGI scores to corroborate the severity categories defined by HDRS (see  ). A third potential issue is that we used linear SVM classifiers. We focused on this algorithm because it offers the advantage that one can examine the learned weights and attempt to interpret how the classifier is using the input features to produce a classification prediction. It is possible that other machine learning classifiers such as the non-linear radial basis function (RBF) SVM will yield better accuracy in this context. Unfortunately, it is difficult to provide a simple, straightforward interpretation of how algorithms such as RBF SVM produce predictions for a given individual. This difficulty of interpretation presents a barrier to deployment in the clinic, as medical practitioners place a high degree of importance on being able to interpret and evaluate the predictions of any automated clinical decision-making system. \n\n\n\n## Conclusions \n  \nResting-state brain activity provides a statistically significant classification of healthy controls vs. patients with very severe MDD (HRSD scores \u2265\u00a024) but not for less severe depression. Moreover, even the classification accuracy that our approach achieved for very severe MDD is not sufficient from a clinical perspective. The negative results of our study help to focus the future efforts of our community, on considering larger sample sizes. We anticipate this may lead to better results that may provide clinically meaningful classification results for MDD based on severity. \n\nThe following are the supplementary data related to this article. \n\n \n", "metadata": {"pmcid": 4983635, "text_md5": "4c78e52c1b27765a70d2c80c6a57eab1", "field_positions": {"authors": [0, 176], "journal": [177, 192], "publication_year": [194, 198], "title": [209, 308], "keywords": [322, 463], "abstract": [476, 2766], "body": [2775, 43530]}, "part": 1, "chapter": 8, "page": 4, "pmid": 27551669, "doi": "10.1016/j.nicl.2016.07.012"}, "display_title": "pmcid: <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4983635>4983635</a> \u2014 Part 1 Chapter 8 Page 4", "list_title": "1.8.4  Accuracy of automated classification of major depressive disorder as a function of symptom severity"}
