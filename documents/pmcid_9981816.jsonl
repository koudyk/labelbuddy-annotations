{"text": "Walters, Jonathon and King, Maedbh and Bissett, Patrick G. and Ivry, Richard B. and Diedrichsen, J\u00f6rn and Poldrack, Russell A.\nNeuroimage, 2022\n\n# Title\n\nPredicting brain activation maps for arbitrary tasks with cognitive encoding models\n\n# Keywords\n\nEncoding models\nComputational modeling\nCognition\nOntologies\nfunctional MRI\n\n\n# Abstract\n \nA deep understanding of the neural architecture of mental function should enable the accurate prediction of a specific pattern of brain activity for any psychological task, based only on the cognitive functions known to be engaged by that task. Encoding models (EMs), which predict neural responses from known features (e.g., stimulus properties), have succeeded in circumscribed domains (e.g., visual neuroscience), but implementing domain-general EMs that predict brain-wide activity for arbitrary tasks has been limited mainly by availability of datasets that 1) sufficiently span a large space of psychological functions, and 2) are sufficiently annotated with such functions to allow robust EM specification. We examine the use of EMs based on a formal specification of psychological function, to predict cortical activation patterns across a broad range of tasks. We utilized the Multi-Domain Task Battery, a dataset in which 24 subjects completed 32 ten-minute fMRI scans, switching tasks every 35 s and engaging in 44 total conditions of diverse psychological manipulations. Conditions were annotated by a group of experts using the Cognitive Atlas ontology to identify putatively engaged functions, and region-wise cognitive EMs (CEMs) were fit, for individual subjects, on neocortical responses. We found that CEMs predicted cortical activation maps of held-out tasks with high accuracy, outperforming a permutation-based null model while approaching the noise ceiling of the data, without being driven solely by either cognitive or perceptual-motor features. Hierarchical clustering on the similarity structure of CEM generalization errors revealed relationships amongst psychological functions. Spatial distributions of feature importances systematically overlapped with large-scale resting-state functional networks (RSNs), supporting the hypothesis of functional specialization within RSNs while grounding their function in an interpretable data-driven manner. Our implementation and validation of CEMs provides a proof of principle for the utility of formal ontologies in cognitive neuroscience and motivates the use of CEMs in the further testing of cognitive theories. \n \n\n# Body\n \n## Introduction \n  \nA deep understanding of the neural architecture of mental function should enable the accurate prediction of a specific pattern of brain activity for any given psychological task, based only on the cognitive functions known to be engaged by that task. This type of prediction has been achieved in a number of specific domains using voxelwise encoding models (EMs), which aim to predict neural responses from a set of known features (such as stimulus features or computational model components) (  ;  ). These models have been particularly effective in visual neuroscience, where they have been used to characterize neuronal mechanisms of attention (e.g.,  ), motion perception (e.g.,  ), and natural image processing (e.g.,  ), among many others. They have also been applied in the context of language, where they have been used with models that embed linguistic stimuli in a low-dimensional space to predict patterns of brain activity for untrained words ( ) and to create a cortical atlas of semantic space ( ). \n\nIn the present work we examine the use of encoding models based on a formal specification of cognitive function, known as a cognitive ontology ( ), to predict cortical activation patterns for cognitive tasks. In particular, we utilize the Cognitive Atlas ( ), a knowledge base that defines a set of cognitive functions and the way that they are measured in various tasks. The resulting cognitive encoding models (CEMs) map from an annotation of the functions engaged by any particular task condition to activation at each location in the brain. The implementation and validation of such models provides a proof of principle for the utility of formal ontologies in cognitive neuroscience and motivates the use of this approach in the further testing of cognitive theories, which often vary in their predictions regarding the cognitive functions engaged by any given task. \n\nThe implementation of cognitive encoding models has been limited in large part by the availability of datasets that 1) are sufficiently broad to span a large space of cognitive functions, and 2) are sufficiently annotated with respect to these functions to allow robust specification of the encoding model. To address this challenge, we utilized the Multi-Domain Task Battery ( ), in which 24 subjects completed 32 ~10-minute fMRI scans, switching tasks every 35 s and engaging in 44 total task conditions spanning a wide range of cognitive functions. Each of the task conditions was annotated by a set of experts using the Cognitive Atlas to identify the putatively engaged cognitive functions. We then fit CEMs based on task responses estimated from a set of regions spanning the entire neocortex ( ), and assessed the degree to which these models could accurately predict brain responses on held-out task conditions. \n\nOur work extends several recent studies that have attempted to decode cognitive functions using cognitive ontologies or to predict activation for novel tasks using cognitive relations between tasks. One recent study ( ) demonstrated the utility of a cognitive ontology for decoding concepts from patterns of brain activation, using population-level data aggregated across 30 fMRI studies (196 total experimental conditions) ( ). Training on many diverse conditions and accurately decoding concepts for unseen conditions provided strong evidence that ontology-based approaches may be useful for establishing selective associations between brain regions/networks and particular cognitive functions. However, the features in this study were relatively task-focused (e.g. stimulus or response features), unlike the function-focused features used in the present paper. Another recent study ( ) showed the effectiveness of metadata-based features in an encoding model to predict brain activity and to decode tasks (instead of concepts), using subject-level data (103 total tasks) ( ). This study also demonstrated the ability to predict activation patterns for unseen tasks, using a latent feature space derived from the Neurosynth database ( ). The ability to predict activation patterns for novel tasks based on data from other tasks was also demonstrated by Pinho et al., ( ), using the extensive Individualized Brain Charting database ( ). \n\nIn the present study, we extend these previous studies by developing cognitive encoding models using expert cognitive annotations of a broad range of tasks. We first assess the ability of individualized CEMs to predict brain activation patterns for unseen tasks, comparing generalization performance to a permutation-based null model and other benchmarks. Next, we perform hierarchical clustering on the similarity structure of CEM generalization errors to examine relations amongst ontological entities. To quantify CEM generalization across subjects, we train CEMs using the data from a single subject and then evaluate how well this model can predict the activation patterns for each of the other individuals, repeating this process for each subject. Finally, we characterize the cognitive relevance of canonical large-scale resting-state functional networks by examining the strength of learned regression weights within each network. \n\n\n## Methods \n  \n### Data and code availability \n  \nThe data used in this study are openly available via OpenNeuro ( , doi:  ). All analysis code is available at  . \n\n\n### Dataset \n  \nWe used the openly available Multi-Domain Task Battery fMRI dataset ( ), designed to measure a broad range of cognitive processes. All experimental protocols were approved in the original study by the Ethics committee at Western University (Protocol #107,293), including informed consent provided to all participants. In brief, 24 healthy subjects (16 F; age:   M   = 23.8, SD = 2.6) each engaged in 47 diverse task conditions (e.g., finger tapping, movie watching, n-back, rest) across 32 ~10-minute scans. Each scan consisted of a continuous task paradigm, in which subjects performed a sequence of 17 tasks (5-  sec   instructions, 30-  sec   task execution) presented in a random order fixed across subjects (see   for tasks and their annotations). Subjects were scanned on two task sets (A and B), and each task set had two scanning sessions (with 8 scans per session). 16 subjects were scanned on set A in year 1 and set B in year 2, while the other 8 subjects were scanned on set A in year 2 and set B 2\u20133 weeks later. The two task sets partially overlapped: 29 task conditions in A and 32 task conditions in B, with 14 common to both. While the original paper considered target and non-target trials of the Verbal 2-back and Object 2-back tasks as different conditions (resulting in 4 total conditions), the current study viewed working memory load as constant across the trial types and thus considered each version of the n-back task as a single condition (resulting in 2 total conditions). Moreover, since contrasts in the present study were computed relative to the Rest condition, all analyses described herein are based on 44 unique task conditions. Finally, one subject (  sub-29)   was excluded from analyses due to failure of fMRI preprocessing. \n\n\n### fMRI data \n  \nResults included in this manuscript come from preprocessing performed using   fMRIPrep   1.5.1rc1 (Esteban, Markiewicz, et al., 2018, 2018, RRID:SCR_016216), which is based on   Nipype   1.3.0rc1 ( ,  , RRID:SCR_002502). Many internal operations of   fMRIPrep   use   Nilearn   0.5.2 ( , RRID:SCR_001362), mostly within the functional processing workflow. The anatomical and functional data preprocessing descriptions below are adapted from automated output of   fMRIPrep  . \n\n#### Anatomical data preprocessing \n  \nFor each subject, a T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with \u201cN4BiasFieldCorrection\u201d ( ), distributed with ANTs 2.2.0 ( , RRID:SCR_004757). The T1w-reference was then skull-stripped with a   Nipype   implementation of the \u201cantsBrainExtraction.sh\u201d workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using FAST (FSL 5.0.9, RRID:SCR_002823,  ). A T1w-reference map was computed after registration of the T1w image (after INU-correction) using \u201cmri_robust_template\u201d (FreeSurfer 6.0.1,  ). Brain surfaces were reconstructed using \u201crecon-all\u201d (FreeSurfer 6.0.1, RRID:SCR_001847, Dale, Fischl, and Sereno, 1999), and the brain mask was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mind-boggle ( , RRID:SCR_002438). Volume-based spatial normalization to the   ICBM 152 Nonlinear Asymmetrical template version 2009c   ( , RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym] was performed through nonlinear registration with \u201cantsRegistration\u201d (ANTs 2.2.0), using brain-extracted versions of both T1w reference and the T1w template. \n\n\n#### Functional data preprocessing \n  \nFor each of the 32 BOLD runs per subject, the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of   fMRIPrep  . A deformation field to correct for susceptibility distortions was estimated based on a field map that was co-registered to the BOLD reference, using a custom workflow of   fMRIPrep   derived from D. Greve\u2019s \u201cepidewarp.fsl\u201d script ( ) and further improvements of HCP Pipelines ( ). Based on the estimated susceptibility distortion, an unwarped BOLD reference was calculated for a more accurate co-registration with the anatomical reference. \n\nThe BOLD reference was then co-registered to the T1w reference using \u201cbbregister\u201d (FreeSurfer) which implements boundary-based registration ( ). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) were estimated before any spatiotemporal filtering using \u201cmcflirt\u201d (FSL 5.0.9,  ). BOLD runs were slice-time corrected using \u201c3dTshift\u201d from AFNI 20,160,207 ( , RRID:SCR_005927). The BOLD time-series were resampled to surfaces on the following spaces:   fsaverage5  . The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying a single, composite transform to correct for head motion and susceptibility distortions. These resampled BOLD time-series will be referred to as   preprocessed BOLD in original space  , or just   preprocessed BOLD  . The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in   MNI152NLin2009cAsym   space. First, a reference volume and its skull-stripped version were generated using a custom methodology of   fMRIPrep  . Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD and DVARS were calculated for each functional run, both using their implementations in   Nipype   (following the definitions by  ). Three global signals were extracted from the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (  CompCor  ,  ). \n\nPrincipal components were estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128 s cut-off) for anatomical   CompCor   (aCompCor). A subcortical mask was obtained by heavily eroding the brain mask to exclude cortical GM regions. Components for aCompCor were calculated within the intersection of the aforementioned mask and the union of CSF and WM masks calculated in T1w space. Components were also calculated separately within the WM and CSF masks, and the   k   components with the largest singular values were retained, such that the retained components\u2019 time series were sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, or combined). The remaining components were dropped from consideration. \n\nThe confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each ( ). All resamplings were performed with   a single interpolation step   by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using \u201cantsApplyTransforms\u201d (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels ( ). \n\n\n#### Statistical maps and parcellation \n  \nIn the MDTB dataset, subjects were scanned on two task sets, separated by 2\u20133 weeks (  n   = 8) or a year (  n   = 16), with two scanning sessions per task set and 8 scans per session. Since the two task sets shared a subset of tasks, data for each task was available in either two or four scan sessions. Thus, for each subject, two or four statistical activation maps were estimated for each of the 44 task conditions. Maps were generated by first fitting a general linear model (GLM) to the time series of each voxel, separately for each of the 32 imaging runs. A fixed effects analysis then combined runs within sessions, and session-wise contrasts of   condition > rest   were computed for each task condition, with the resulting t-statistics converted to z-scores. \n\nTask conditions, including the 5-second instruction screen preceding each 30-second task block, were modeled in the GLM as boxcar or event-related regressors convolved with the canonical hemodynamic response function (SPM). The duration of each regressor varied by condition (30, 16, 15, 14, 10, 5, 2, or 1 s; see   for the complete description). The other regressors included in the model were confounds estimated from fMRI preprocessing: 6 motion parameter estimates, framewise displacement, 7 cosine bases for high-pass filtering, and the first 6 components from anatomical CompCor. A first-order autoregressive model was used to model the temporal structure of the noise. \n\nFor computational speed during model training and testing, we parcellated the resulting z-maps into 1000 cortical regions, using an atlas derived from resting-state data that was optimized for both local and global spatial signal variability ( ). In doing so, the z-score of a given region was calculated as the average across its voxels. \n\n\n\n### Labeling tasks with ontological feature vectors \n  \nA group of five cognitive scientists [JW, MK, PB, RI, RP] collaboratively and iteratively labeled the 44 tasks with 36 cognitive, perceptual, and motor features ( ) based on a conceptual analysis of each task. Annotations, along with entities and relations in the Cognitive Atlas, were updated throughout several iterations of the labeling process. Each feature corresponded to an entity in the Atlas, such as \u201cworking memory maintenance\u201d ( ). The resulting feature vectors captured the tasks\u2019 partially overlapping functional requirements. Most of the features were binary (e.g., \u2018Visual word recognition\u2019, \u2018Updating\u2019, \u2018Motor planning\u2019), indicating the presence or absence of a particular operation while executing the task. Three features were parametrically coded to reflect the magnitude of their involvement: 1) \u2018Response alternatives\u2019 (0, 1, 2, 4, 6, or 7 response options), 2) \u2018Left hand response execution\u2019 (0, 1, 2, 5, 7, 8, 12, or 15 responses in a task block, and 3) \u2018Right hand response execution\u2019 (0, 1, 2, 3, 4, 5, 7, 8, 12, or 15 responses in a task block). These three features were recoded as the index of their respective rank-orderings, with 0 being the smallest value, and were then scaled to a range of 0 to 1. Additionally, all features were manually classified as being either primarily cognitive or perceptual/motor in nature ( ) in order to assess the relative importance of those two classes of features in the model. \n\n\n### Cognitive encoding models (CEMs) \n  \n#### Training and testing \n  \nFor each subject, region-wise cognitive encoding models (CEMs) were trained to predict responses across tasks from features that captured the tasks\u2019 psychological requirements, as formalized by an ontology. This region-wise linear mapping from ontological space to brain activation space was learned using ridge regression. In principle, once region-wise CEMs are learned, a cortical activation pattern can be predicted for any arbitrary task from its ontological feature vector. \n\nTo estimate generalization performance to unseen tasks, a leave-two-out cross-validation (CV) scheme was employed ( ). In each CV split, region-wise CEMs were trained on 42 tasks, and the activation patterns of the two held-out tasks were predicted. Three performance metrics were then measured: two-way classification accuracy (see below), map-wise Pearson\u2019s   r,   and   R  . \n\nA nested-CV scheme was necessary to select the regularization strength parameter   alpha  . The outer loop (leave-two-out CV) used a correlation-based minimum-distance classifier to classify each of the two predicted maps as one of the two true held-out maps, and an inner loop (10-fold CV) found the optimal   alpha   using Pearson\u2019s   r   between true and predicted maps as the scoring function. The following   alpha   values were used in the parameter search: [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. \n\nMap-wise Pearson\u2019s   r   (and   R   was computed for each of the two held-out tasks in each CV split (44 tasks choose 2 = 946 total splits). Since the predicted map of one task was associated with either two or four observed maps (derived from two or four fMRI imaging sessions), its map-wise correlation was taken as the average across sessions. The resulting 1892 averaged correlations across all CV splits were then averaged to obtain a single map-wise correlation for a given subject, providing a summary of estimated generalization to unseen tasks. For the noise ceiling analysis (described below) computed at the level of tasks, these 1892 values were also averaged by task. \n\n\n#### Two-way classification accuracy \n  \nClassification accuracy was computed using a leave-two-out CV scheme based on  . In each fold, the trained region-wise CEMs predicted the statistical activation maps of two held-out tasks. A correlation-based minimum-distance classifier, classified each of the two predicted maps as belonging to one of the two tasks. Since one predicted map was associated with either two or four observed maps (corresponding to two or four fMRI imaging sessions), the average Pearson correlation across the two or four sessions served as input to the classifier. Two-way classification accuracy was considered correct if both predicted images were correctly classified ( ), leading to a theoretical chance level of 25% accuracy. \n\n\n#### Noise ceiling \n  \nIn general when evaluating encoding models, it is important to interpret model performance relative to the noise ceiling of the data (i.e. its reliability). While our models are trained at the level of individual brain regions, our main interest and analyses revolve around predictions for individual subjects at the level of cortical activation maps evoked by tasks. Thus, to allow for direct comparison between model performance and an upper limit of explainable variance, we quantified the noise ceiling of each task, for each subject, as the square root of the between-session reliability (Pearson\u2019s   r  ) of the activation maps ( ). For tasks that were completed at both timepoints and thus had two between-session reliability estimates, these reliabilities were averaged in any downstream analyses. \n\n\n#### Task-shuffled null model \n  \nWe compared CEM performance to a task-shuffled (permutation-based) null model. In each permutation, tasks were randomly remapped to one another such that each task\u2019s feature vector was replaced by that of another task. This procedure thus targeted the relationship between tasks\u2019 ontological features and regions\u2019 activations. Notably, because each task had two or four corresponding rows in the original design matrix (depending on whether the task appeared in two or four imaging sessions), this method of randomizing the design matrix preserved blocks of tasks (i.e., the two or four rows of a given task all received identical feature vectors from another task); however, column densities were not preserved (i.e., feature vectors that originally appeared in two rows could appear in four rows, or vice-versa), which at least partially violated the assumption of exchangeability. Randomization and training/testing (using the same nested cross-validation procedure as the main models) was repeated for 20 iterations, and final performance metrics for each subject were calculated as the average across iterations. \n\n\n\n### Feature set comparison \n  \nTo assess the relative importance of the Cognitive and Perceptual-Motor features in model performance, all of the main analyses presented in this paper were performed on three different feature sets: all 36 features (  All  ), 24 higher-order cognitive features (  Cognitive  ), and 12 perceptual-motor features (  Perceptual-Motor  ). Assignment of features to these groups was based on expert judgment of authors JW and RP. \n\n\n### Hierarchical relations between ontological entities \n  \nThe hierarchical organization of cognitive functions was examined based on classifier generalization errors. First, a confusion matrix was generated by calculating, for each task, how often its predicted pattern was classified correctly or was misclassified as each of the other tasks. These results were aggregated across subjects by considering all CV splits from all subjects. This confusion matrix was projected into cognitive space by multiplying the confusion matrix by the feature matrix. Next, a representational dissimilarity matrix (RDM) was constructed using 1-correlation as the distance metric. Finally, hierarchical agglomerative clustering was applied to the RDM using the UPGMA algorithm. \n\n\n### Between-subject similarity of brain function \n  \nWe measured how well CEMs generalized between subjects by training CEMs on each subject individually and then testing each of these \u201csource\u201d subject\u2019s CEMs on every other \u201ctarget\u201d subject. CEMs were trained on all 44 tasks and these parameters were then used to predict the activation patterns for all other subjects. Two-way classification accuracy was then calculated for every pair of tasks (analogous to the main analyses) for every target subject. \n\n\n### Using CEMs to characterize the cognitive relevance of large-scale resting state networks \n  \nSeven canonical large-scale functional resting-state networks (RSNs) ( ) were probed for their cognitive relevance using the individualized CEMs. Feature importances were estimated within each RSN by aggregating regional model coefficients across subjects from their individualized CEMs, and then aggregating across regions within each RSN. Specifically, for a given region and feature, the coefficients across subjects were first averaged. Then, for a given network and feature, these subject-averaged values were averaged across all regions within the network. This procedure allowed us to address whether spatial distributions of brain functions systematically overlapped with known functional networks. \n\n\n\n## Results \n  \n### Cognitive encoding models generalize well to unseen tasks \n  \n#### General CEM performance \n  \nCEMs generalized well to unseen tasks ( ; example predictions in  ), with an average cross-validated two-way classification accuracy well above chance-level performance of 0.25 for all three feature sets: All (  M   = 0.65, SD = 0.07); Cognitive (  M   = 0.40, SD = 0.06); Perceptual-Motor (  M   = 0.60, SD = 0.07). The task-shuffled null model showed performance below the theoretical chance accuracy level (  M   = 0.09, SD = 0.02). \n\nAdditionally, relative to the null model, the predicted activation maps of CEMs showed stronger positive correlations with the true held-out maps: All (  M   = 0.69, SD = 0.08); Cognitive (  M   = 0.63, SD = 0.09); Perceptual-Motor (  M   = 0.67, SD = 0.08); task-shuffled (  M   = 0.54, SD = 0.09). The three feature sets also showed higher   R   than the null model: All (  M   = 0.38, SD = 0.13); Cognitive (  M   = 0.22, SD = 0.13); Perceptual-Motor (  M   = 0.35, SD = 0.13); task-shuffled (  M   = 0.09, SD = 0.12) (see   for region-wise   R   values). \n\nThe null model\u2019s classification accuracy was systematically below chance due to a bias towards predicting the mean of the training data (this also explains its relatively high   R   ; see   for additional details). \n\n\n#### Feature set comparison \n  \nNext, we asked how two-way classification accuracy differed between the three feature sets. Notably, though the number of features varied across models, our cross-validation procedure provided an implicit control for this difference. A one-way repeated measures ANOVA revealed a significant difference between feature sets, F(2, 44) = 750.66,   p   < .001. A post hoc analysis with three Bonferroni-corrected paired samples t-tests revealed that All significantly outperformed both Cognitive (  t  (22) = 32.69,   p   < .001) and Perceptual-Motor (  t  (22) = 10.50,   p   < .001), and Perceptual-Motor significantly outperformed Cognitive (  t  (22) = 26.26,   p   < .001). \n\nThis pattern of results indicates that the Cognitive and Perceptual-Motor feature spaces each provided valuable information in predicting patterns of cortical activation and that CEM performance was not driven exclusively by perceptual-motor features. Thus, the inclusion of features that broadly span the space of mental functions (from lower-level perceptual-motor functions to higher-level cognitive functions) in region-wise encoding models may lead to superior performance when predicting cortical activation maps of novel tasks. \n\n\n#### Noise ceiling \n  \nTo situate the encoding model results with respect to the noise ceiling of the data, for each subject and task we estimated the noise ceiling in the activation maps by computing the square root of the test-retest reliability (Pearson\u2019s   r  ) between imaging sessions. The out-of-sample correlations for the individualized CEMs was comparable to the test-retest reliabilities, with a Pearson\u2019s   r   of 0.70 ( ) (Model:   M   = 0.71, SD = 0.18; Noise Ceiling:   M   = 0.80, SD = 0.15). The average test-retest reliability for individual tasks ranged from 0.61 to 0.94 ( ) and for individual subjects ranged from 0.44 to 0.90 ( ). These results indicate that CEMs successfully learned a generalizable mapping from ontological space to brain activation space, overall approaching but generally not exceeding the noise ceiling. \n\nThe instances where the noise ceiling was exceeded (176 of 1012 task-subject pairs, or 17.4%) were driven by two related factors. First, the five subjects with the lowest average test-retest reliabilities comprised 42% of such cases, suggesting that reliability estimates may have been artifactually low in these cases. Second, there were five meta-tasks in the MDTB that each had three task conditions (i.e., Mental Rotation, Spatial Map, Visual Search, Response Alternatives, and Prediction), and these task conditions comprised 70% of such cases. Thus, for these tasks, correlations for out-of-sample predictions during cross-validation were likely boosted when one or two of the related conditions were present in the training data. However, as explained in the following section, model performance in general was not driven solely by such dependencies. \n\n\n#### Model performance is not driven solely by dependencies between train and test data induced by cross-validation splits \n  \nAs some of the tasks defined in the encoding models are experimental conditions derived from the same meta-task (e.g., Easy, Medium, and Hard trials in the Mental Rotation meta-task), we examined whether our main results were driven purely by the subset of cross-validation splits in which one or both of the held-out test tasks had counterparts in the training data (i.e., thereby increasing statistical dependence between training and testing data). We found that, for splits in which neither of the test tasks had counterparts in the training data, all three feature sets nevertheless performed above the theoretical chance accuracy level of 25% ( ), providing strong evidence that the generalizability of these encoding models was not an artifact of statistical dependencies induced by particular cross-validation splits. However, it was also clear that performance increased as the match of tasks between training and test splits increased, and that this was driven primarily by perceptual-motor features. \n\n\n\n### Classifier generalization errors reveal hierarchical relationships between cognitive functions \n  \nTo examine relationships between ontological features, classifier generalization errors from the individualized CEMs were aggregated by task across subjects.   shows a clustered representational dissimilarity matrix (RDM) for ontological entities based on classifier generalization errors. \n\n\n### Between-subject generalization \n  \nWe quantified how well CEMs generalized between subjects by training models on individual subjects and testing them on all others ( ). Subjects showed average generalization accuracies ( , top) (  M   = 0.65, SD = 0.07) that were well above the theoretical chance accuracy level of 0.25 and below the self-transfer accuracies (  M   = 0.84, SD = 0.06). Average CV correlation ( , bottom) on target subjects (  M   = 0.36, SD = 0.03) was lower than self-transfer correlations (  M   = 0.68, SD = 0.06). \n\n\n### Mapping spatial distributions of CEM coefficients onto large-scale resting state networks \n  \nAfter validating the individualized CEMs, we sought to use the trained models to characterize the function of large-scale resting-state networks (RSNs) as an additional way of validating the results with regard to known functional associations. To do so, regional model coefficients from subjects\u2019 CEMs were aggregated across subjects and then within RSNs, allowing for estimates of feature importance within each RSN. \n\nFirst, we tested the hypothesis that RSNs possess a degree of functional specialization, finding that CEM coefficients were more similar between regions in the same network than in different networks (  and  ) (within-network:   M   = 0.37,   SD   = 0.33; between-network:   M   = 0.05,   SD   = 0.36;   p   < .01, based on permutation tests randomizing   within   and   between   labels for 100 iterations). While this result is expected given that our encoding models as well as the definitions of regions and networks we used are both activity-based, it supports the hypothesis of functional specialization in large-scale RSNs while also grounding their function in an interpretable and data-driven manner. \n\nNext, we visualized the function of each network by generating word cloud representations of relative feature importances ( ). Many of the dominant features across the networks reflect known functional associations. For example, the most positively weighted feature in the Visual Network (VN) is   visual object recognition   and in the Default Mode Network (DMN) is   theory of mind  . Interestingly, a few of the dominant features across networks did not match expectations based on prior work (e.g.,   autobiographical recall   in the Somatomotor Network). \n\n\n\n## Discussion \n  \nIn the present work we developed encoding models based on a cognitive ontology and applied them to an fMRI dataset that included a broad range of cognitive task conditions, with the goal of predicting activation patterns for held-out task conditions. The results demonstrated that these models can effectively predict activation patterns for novel tasks based on the annotation of cognitive processes engaged by the task, both within individuals and across individuals. The amount of variance accounted for across tasks by the predictive model varied, in some cases accounting for nearly all of the explainable variance in the maps. Assessment of the model parameters in relation to well-established large-scale brain systems demonstrated a mapping of functions largely consistent with known functional neuroanatomy. \n\nThe present findings extend previous work that had established the ability of encoding models to predict task activation maps for broad sets of cognitive tasks ( ;  ), by demonstrating the ability to use expert annotations of cognitive functions as the basis for the encoding model. This provides the potential to use cognitive encoding models to test cognitive theories; whereas cognitive theories rarely make specific predictions regarding locations of brain activation, they nearly always make predictions regarding the specific processes engaged by a particular set of tasks, and hence the similarity or overlap of task-related activation maps. By implementing encoding models based on competing cognitive theories and testing their predictive accuracy on out-of-sample data, this approach has the potential to adjudicate theoretical questions using neuroimaging data, thus addressing the longstanding question of whether neuroscience data can inform cognitive theories (e.g.,  ). \n\n### Future directions \n  \nThe present approach could be used to learn a data-driven cognitive ontology that optimally predicts activation on new tasks. Although the generation of theories with scopes that are broad enough to span all of cognition is a major challenge, this approach can still be applied to more restricted functional domains with the goal of increasing scope over time. Accordingly, future studies using this approach could potentially leverage unsupervised learning applied to the task behavior ( ), additional information regarding the brain networks engaged by those tasks ( ), or information from computational models fit to those tasks (  ;  ). Alternatively, completely unsupervised methods could be used to learn a novel \u201ccognitive basis set\u201d optimized solely for prediction of activation maps. Another possibility is to use structural equation modeling on the covariance matrix between cortical activation maps to construct, test, and refine different latent representations of the underlying data-generation process. \n\nWhile the use of data-driven approaches to refine existing ontologies is appealing, it is challenging because there are multiple factors that must be considered, which include: the ontology itself, the mapping of the ontology to tasks, the ontological breadth of the measured tasks, the learning algorithm, and the quality of the neuroimaging data. For example, if it is observed that Theory of Mind (ToM) is a poor predictor, it could be the case that a) ToM is an ill-defined construct, b) ToM is not mapped appropriately to tasks, c) there is an insufficient breadth of tasks that engage both ToM and mixtures of other constructs (such that the effect of ToM cannot be disentangled from that of other constructs), d) the chosen learning algorithm fails to capture the true underlying relationship between ToM and cortical activation, and/or e) the quality of the data acquired on the ToM tasks is poor. \n\nThe CEMs presented in this study model cortical activation univariately at the level of brain regions, whose collective predictions as an ensemble are then used for task classification. To more directly model patterns of brain activation, future work should also consider multivariate approaches that predict whole-brain responses from cognitive features, such as partial least squares or redundancy analysis. Another possibility is to use a CEM to inform how much difference one would expect between cortical activation patterns, as measured by an appropriate distance metric (e.g., a visual and an auditory task should be more different than two visual tasks). This type of prediction can be accomplished with multivariate distance matrix regression ( ), which has already seen application to fMRI data ( ). \n\nRelatedly, while explaining differences in statistical activation maps is a valuable first step in building CEMs, future work should consider bypassing the use of general linear model estimates of task activation altogether, opting instead to more directly relate ontologies to minimally preprocessed BOLD time courses. This strategy of directly mapping ontological entities to brain dynamics may ultimately provide the most illuminating application of CEMs, in part due to the reduction of bias that may arise if a modeler (however implicitly) uses their own ontological knowledge in constructing the task regressors. \n\nFinally, the present study assessed the ability of a formal ontology to predict cortical activation patterns for unseen tasks, while holding the learning algorithm constant. Although this is a necessary first step in a proof-of-concept study for the utility of ontology-based cognitive encoding models, future work should systematically examine how variability in algorithmic decisions (e.g., using different linear or nonlinear modeling frameworks) contributes to the predictive capacities of CEMs. \n\n\n### Limitations \n  \nWhile the present work showed that a feature space based on expert manual annotations of cognitive functions affords high predictive accuracy for unseen tasks, the labor-intensive nature of the labeling method limits its scalability. The labeling method can be thought of as a function from feature space to tasks, and, in order to generalize to new tasks, the same function needs to be applied to any new tasks. Thus, to increase consistency and scalability, future work will need to explore automated labeling schemes that operate upon feature spaces of cognitive processes. Unfortunately, standard text-mining approaches are likely too imprecise to provide the level of functional detail needed to develop such models, but one promising approach is to harness methods for automatic knowledge extraction or for human-in-the-loop programmatic or semi-supervised labeling (e.g.,  ). Such approaches will likely be required to generalize from the current limited annotations available to the broad range of tasks needed to develop \u201ccognition-wide\u201d models. \n\nAnother limitation of the current work involves simplifying assumptions regarding how the features relate to tasks. It was assumed that all subjects performed tasks using the same mixture of psychological functions (i.e., what differed across subjects was the region-wise encoding of these features). Instead, it is likely that the psychological functions actually used to perform a given task differs across subjects. Future work could explore ways of optimizing the personalization of such labels, such as using models of task performance to infer task strategies (e.g.  ). Relatedly, this work is limited in its modeling of regional activation in terms of linear combinations of the psychological features. Future work should consider exploring interactions between features, for example by explicitly including interaction terms in a linear model or by using models capable of capturing non-linear interactions. Despite these simplifying assumptions, CEMs were still able to learn a generalizable mapping from ontological features to brain activation quite well, approaching the noise ceiling. \n\nFinally, because the parcellation scheme used in this study computed regional activity by averaging across voxels, we caution readers that our analyses provide little, if any, interpretive value at the level of individual voxels. \n\n\n\n## Conclusion \n  \nUsing a uniquely rich fMRI dataset, in which individuals were densely sampled performing 44 diverse tasks, we demonstrated the predictive and interpretive utility of ontology-based feature spaces in encoding models that generate subject-specific cortical activation patterns for unseen tasks. These results build on previous work that uses datasets of diverse tasks for functional brain mapping, providing further evidence for the utility of cognitive ontologies in consistently mapping cognitive functions to tasks and in selectively associating cognitive functions with their neural bases. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 9981816, "text_md5": "f644b2a1df4e2b9c7f71cbf63457d0bd", "field_positions": {"authors": [0, 126], "journal": [127, 137], "publication_year": [139, 143], "title": [154, 237], "keywords": [251, 326], "abstract": [339, 2529], "body": [2538, 43026]}, "batch": 1, "pmid": 36064138, "doi": "10.1016/j.neuroimage.2022.119610", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9981816", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9981816"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9981816\">9981816</a>", "list_title": "PMC9981816  Predicting brain activation maps for arbitrary tasks with cognitive encoding models"}
