{"text": "Lehmann, Brieuc and White, Simon\nStat Comput, 2024\n\n# Title\n\nA Bayesian multilevel model for populations of networks using exponential-family random graphs\n\n# Keywords\n\nExponential random graph model (ERGM)\nBayesian linear regression\nMarkov chain Monte Carlo (MCMC)\nBrain networks\n\n\n# Abstract\n \nThe collection of data on populations of networks is becoming increasingly common, where each data point can be seen as a realisation of a network-valued random variable. Moreover, each data point may be accompanied by some additional covariate information and one may be interested in assessing the effect of these covariates on network structure within the population. A canonical example is that of brain networks: a typical neuroimaging study collects one or more brain scans across multiple individuals, each of which can be modelled as a network with nodes corresponding to distinct brain regions and edges corresponding to structural or functional connections between these regions. Most statistical network models, however, were originally proposed to describe a single underlying relational structure, although recent years have seen a drive to extend these models to populations of networks. Here, we describe a model for when the outcome of interest is a network-valued random variable whose distribution is given by an exponential random graph model. To perform inference, we implement an exchange-within-Gibbs MCMC algorithm that generates samples from the doubly-intractable posterior. To illustrate this approach, we use it to assess population-level variations in networks derived from fMRI scans, enabling the inference of age- and intelligence-related differences in the topological structure of the brain\u2019s functional connectivity. \n\n## Supplementary Information \n  \nThe online version contains supplementary material available at 10.1007/s11222-024-10446-0. \n\n \n\n# Body\n \n## Introduction \n  \nThe statistical analysis of network data is becoming increasingly commonplace, with applications across various disciplines, such as epidemiology, social science, neuroscience and finance (Kolaczyk  ). Over the last four decades, a number of statistical models for networks have been developed, including stochastic blockmodels (Holland et\u00a0al.  ), latent space models (Hoff et\u00a0al.  ) and\u2014the focus of this article\u2014exponential random graph models (ERGMs) (Frank and Strauss  ). \n\nAn exponential random graph model is a set of parametric statistical distributions on network data (see Schweinberger et\u00a0al. ( ) for a recent review). The aim of the model is to characterise the distribution of a network in terms of a set of   summary statistics  . These summary statistics are typically comprised of topological features of the network, such as the number of edges and subgraph counts. The summary statistics enter the likelihood via a weighted sum; the weights are (unknown) model parameters that quantify the relative influence of the corresponding summary statistic on the overall network structure and must be inferred from the data. ERGMs are thus a flexible way in which to describe the global network structure as a function of network summary statistics. \n\nTo date, statistical network models, including ERGMs, have largely focused on the analysis of a single network. Formally, a network consists of a set of nodes and a set of edges between these nodes. Let   be a finite set of nodes, each of which may be associated with covariates  . An edge from node   i   to node   j   is denoted by  , so that the network is encoded by the adjacency matrix  . For our purposes, the set of nodes   and their covariates   are considered fixed, while the edges are considered to be random variables. Denote   to be an instantiation, or outcome, of the random adjacency matrix   and write   for the probability that   takes the value  . A statistical network model specifies a parametrised probability distribution on the adjacency matrix   where   is a vector of model parameters. \n\nA population of networks consists of   adjacency matrices   defined on a common set of nodes  . We will assume for simplicity that the nodal covariates are the same across networks, though in principle this not need be the case. A common example of a population of networks arises in neuroimaging, where a typical study consists of brain data across a number of participants, each constituting an individual network. Network analyses of the brain can provide insight into cognitive function by revealing how distinct brain areas work in conjunction (Fuster  ). These analyses aim to identify salient topological features of the brain\u2019s connectivity structure that are common across individuals or that vary with a given covariate. \n\nWhile one could fit a single model to each individual network separately, it is not straightforward to combine these individual results into a single coherent result that is representative of the whole population. An alternative approach is to construct a group-representative network by, for example, taking the mean of the edges across the individual networks and applying a threshold to the resulting weighted network (Achard et\u00a0al.  ). These approaches ignore the individual variability present in the networks and, moreover, typically do not accurately summarise the topological information across the individual networks (Ginestet et\u00a0al.  ). \n\nA more statistical approach is to treat each individual networks as distinct statistical units arising from a joint probability distribution   (Ginestet et\u00a0al.  ). Here, we describe how to perform Bayesian linear regression where the outcome of interest is a network-valued random variable whose distribution is described by an exponential random graph models. By modelling the networks jointly, this framework provides a principled approach to characterise the relational structure for an entire population, and allows one to assess how network structure varies with a given set of network-level covariates. In the case of binary covariates, our method can be used to infer group-level differences in the network structure between   sets of networks  . We demonstrate on both simulated networks and real networks derived from resting-state functional magnetic resonance imaging (fMRI) scans from an ageing study. \n\nInference for Bayesian ERGMs is challenging due to the double-intractability of the ERGM posterior distribution; standard Markov chain Monte Carlo (MCMC) schemes such as the Metropolis algorithm are not feasible as it is not possible to evaluate the acceptance ratio. A common workaround is to apply the exchange algorithm (Murray et\u00a0al.  ), which was first employed in the context of Bayesian ERGMs by Caimo and Friel ( ). To perform inference for our framework for populations of networks, we implemented an   exchange-within-Gibbs   algorithm that combines the exchange algorithm with the Gibbs sampler (Geman and Geman  ) to produce samples from the target posterior distribution. The parameterization of general multilevel models can play an important role in the overall efficiency of a MCMC scheme (Gelfand et\u00a0al.  ; Papaspiliopoulos et\u00a0al.  ,  ). To improve the mixing properties of the algorithm, we use an ancillarity-sufficiency interweaving strategy (ASIS) that interweaves between the   centered   and non-centered parameterizations (Yu and Meng  ). To further boost efficiency, we also employ adaptation of the random-walk proposal parameters in the algorithm (see e.g. Roberts et\u00a0al. ( )). \n\n### Related work \n  \nOur work builds on that of Slaughter and Koehly ( ) who studied multiple approaches to building Bayesian hierarchical models for populations of networks based on ERGMs, including an example of Bayesian linear regression with a single covariate per network. We extend the approach of Slaughter and Koehly ( ) to explicitly handle multiple network-level covariates, employing a matrix Normal prior on the regression coefficients which admits (partial) conjugacy. As noted by Slaughter and Koehly ( ), multilevel models frequently exhibit poor mixing for some of the parameters. Our use of the ASIS algorithm greatly improves the efficiency of the sampler, allowing us to perform linear regression on larger populations of networks, and with a larger number of nodes in each network. We now describe some alternative approaches to modelling populations of networks. \n\n\n### Hierarchical ERGMs \n  \nMultilevel networks are networks with a nested hierarchical structure such that nodes may be grouped into subsets of nodes which may further be grouped into subset of subsets of nodes, and so on. It is worth emphasising that the hierarchical nature of a multilevel network corresponds to the grouping of nodes, as opposed to model parameters as might be typical in a Bayesian hierarchical model. A population of networks represents a two-level network such that each subset of nodes correspond to a separate network, with no connections between distinct subsets (see Fig.\u00a0 ). Wang et\u00a0al. ( ) proposed ERGMs for multilevel networks, introducing a range of model specifications to account for a range of multilevel structures for two-level networks. Yin and Butts ( ) develop a preprocessing approach to efficiently fit a \u2019pooled\u2019 ERGM to multiple networks drawn from the same model. Yin et\u00a0al. ( ) proposed a mixture of ERGMs to model populations of networks in which the group membership is unknown, which was extended to a data-adaptive Dirichlet process mixture of ERGMs by Ren et\u00a0al. ( ). Schweinberger and Handcock ( ) introduced exponential random graph models with local dependence, providing a general framework encompassing multilevel networks (and thus populations of networks) and establishing a central limit theorem for this class of models.   \nA population of networks (bottom) can be seen as a special case of a multilevel network (top) in which each subset of nodes contains the same number of nodes and there are no edges between each subset of nodes. (colour figure online) \n  \n\n\n### ERGMs for brain networks \n  \nExponential random graph models have been applied to resting-state fMRI brain networks (see Simpson et\u00a0al. ( ) for an early example). Simpson et\u00a0al. ( ) constructed group-representative networks by taking the mean of the parameter estimates from ERGMs fit to each individual network. Sinke et\u00a0al. ( ) constructed group-representative networks directly from individual diffusion tensor imaging (DTI) brain networks and then fit Bayesian ERGMs to the resulting group networks. Obando and Fallani ( ) applied ERGMs to functional connectivity brain networks derived from electroencephalographic (EEG) signals. In each of these approaches, the networks are fit independently from each other and, unlike the hierarchical approach described here, there is no pooling of information across networks. \n\n\n### Other models for populations of networks \n  \nOther statistical network models have recently been extended to handle populations of networks. Sweet et\u00a0al. ( ) proposed a general framework of hierarchical network models (HNMs), which encompasses the model described in this article. They focus on a hierarchical representation of latent space models (Hoff et\u00a0al.  ) applied to social networks. Sweet et\u00a0al. ( ) studied stochastic blockmodel in the HNM framework to infer clusters of nodes shared across networks. Durante et\u00a0al. ( ) develop an alternative extension of the latent space model (Hoff et\u00a0al.  ) to populations of networks based on a low-dimensional mixture model representation. Durante et\u00a0al. ( ) applied this model in the context of groups of networks to test for differences. Mukherjee et\u00a0al. ( ) used graphons to detect clusters among multiple networks within a population (as opposed to clusters within networks). Signorelli and Wit ( ) use a model-based clustering method based on generalized linear (mixed) models to cluster networks that share certain network properties of interest. \n\n\n\n## Model formulation \n  \n### Exponential random graph models \n  \nThe family of exponential random graph models define probability distributions over the space of networks in terms of sets of summary (or sufficient) statistics. We will focus on the case of undirected, binary networks, with  . Let   be the range of  , i.e. the set of all possible outcomes. Let   denote a vector of   p   summary statistics, such that each component is a function  . \n\nAn ERGM is specified by a particular set of   p   summary statistics and a map  . The probability mass function of   under the corresponding ERGM is given by Here,   is a vector of   p   model parameters that must be estimated from the data and   is the normalising constant ensuring the probability mass function sums to one. Given data, that is, a realisation  , the goal is to infer which values of   best correspond to the data under this distribution. To reduce the notational burden, we will henceforth omit the dependence on the nodal covariates  , considering this to be implicit in the specification of the probability distribution. \n\n\n### A Bayesian multilevel model for populations of networks \n  \nThe ERGM provides a flexible family of distributions for a   single   network. Our aim is to extend this to a model for a   population   of networks in which each network is accompanied by a set of covariates. To do so, we use ERGMs as the basis of a Bayesian multilevel model. Let   be a set of   n   networks, and let   be a matrix of   q   network-level covariates. Identify each network   with its own vector-valued ERGM parameter  . Write   for the set of network-level parameters. \n\nWe model each individual network   as an exponential random graph, which we denote  . Each individual ERGM must consist of the same set of   p   summary statistics  . We then propose the following multilevel model: where   is a   matrix   of parameters, and the   q  -vector   corresponds to the   column of the matrix   X  . We assume that, conditional on their respective network-level parameters  , the   are independent. We highlight the connection to multivariate linear regression: we have vector-valued \u2018response\u2019 variables   whose dependence on a set of   q   explanatory variables   X   we would like to assess, allowing the components of the residuals   to be correlated. The difference with standard multivariate linear regression is that the responses are not observed but are instead latent parameters of an ERGM model. With this specification comes the flexibility associated with multivariate linear regression; the   X   matrix can include polynomial terms and interactions between the covariates of interest. \n\n#### Prior specification \n  \nThe full conditional likelihood described by ( ) can be written To complete this model, we must therefore specify a prior on  . Motivated by computational simplicity, we opt for the (conditional) conjugate prior  , with where   is a   prior mean matrix,   is a   positive definite matrix,   is a   positive definite matrix, and  . Here,   denotes a matrix-normal distribution with location matrix   M  , row-based scale matrix   U  , and column-based scale   V  .   is an inverse-Wishart distribution with scale   and   degrees of freedom. \n\nEquipped with this prior, we can factorise the posterior of   given the matrix   X   and the network-level parameters  , into   with where Note that conditional on   X   and  , the networks   Y   are independent of   and hence do not appear directly in the (conditional) posterior. However, as we shall see below, the networks are present in the posterior for  . This motivates a Gibbs sampling approach, whereby we iteratively draw from the required conditional distributions. \n\nRegarding the choice of values for the prior hyperparameters  , studies of single-network Bayesian ERGMs typically assume relatively flat multivariate normal prior distributions on the model parameters (Caimo and Friel  ; Sinke et\u00a0al.  ; Thiemichen et\u00a0al.  ). In this spirit, we suggest default priors of   (i.e. the   zero matrix),  ,  , and  . Informative priors can be used given information from previous studies (see e.g. Caimo et\u00a0al. ( ), Caimo et\u00a0al. ( )) though we note that the appropriate setting of informative priors can be a challenging task due to the typically high levels of dependence between parameters (Koskinen et\u00a0al.  ). \n\n\n\n\n## Posterior computation \n  \nThe double-intractability of the ERGM posterior distribution means that standard MCMC schemes such as the Metropolis algorithm are not suitable. This is due to the presence of the intractable normalising constants   in the denominator, rendering calculation of the Metropolis acceptance rates computationally infeasible. Several methods have been proposed in recent years to perform Bayesian inference in the presence of intractable normalising constants (see Park and Haran ( ) for a review). We focus here on the exchange algorithm (Murray et\u00a0al.  ), employed in the context of single-network Bayesian ERGMs by Caimo and Friel ( ). We first recap the exchange algorithm in the context of Bayesian ERGMs before describing a   exchange-within-Gibbs   scheme to generate samples from the joint posterior. \n\nConsider a Metropolis update for a single-network Bayesian ERGM. The acceptance probability for a proposal   from current value   requires evaluation of the ratio  , which is computationally intractable. The exchange algorithm is an MCMC scheme designed to circumvent this obstacle. This is achieved by introducing an auxiliary variable  , i.e. a network drawn from the same exponential random graph model with parameter  . \n\nThe algorithm targets an augmented posterior where   is the original (target) posterior,   is an arbitrary, normalisable proposal function, and   is the likelihood of the auxiliary variable. For simplicity, we assume   to be symmetric. Each of the three terms on the right-hand side of Eq. ( ) can be normalised, so the left-hand side is well-defined as a probability distribution. \n\nThe algorithm proceeds as follows. At each iteration, first perform a Gibbs\u2019 update of   by drawing   followed by  . Next,   exchange   and   with probability  , where Crucially, the ratio of intractable normalising constants cancel out, and so this acceptance ratio can indeed be evaluated. The stationary distribution of the Markov chain constructed through this scheme is   (Murray et\u00a0al.  ). Thus, by marginalising out   and  , the algorithm yields samples from the desired posterior, namely  . \n\n  \nThe exchange algorithm update for a Bayesian ERGM (Caimo and Friel  )  \n  \n\nThe exchange algorithm update requires a sample   from the ERGM   in order to compute the acceptance ratio. Although perfect sampling for ERGMs is possible, it is computationally impractical except for a few special cases (Butts  ). A pragmatic alternative, employed in Caimo and Friel ( ) and Wang and Atchad\u00e9 ( ), is to use the final iteration of a Metropolis\u2013Hastings algorithm as an approximate sample from   (Hastings  ; Hunter et\u00a0al.  ). A theoretical justification of this approach is given by Everitt ( ): under certain conditions, despite using an approximate sample, the algorithm nevertheless targets an approximation to the correct posterior distribution. Further, this approximation improves as the number of iterations of the inner MCMC increases. \n\n### The exchange-within-Gibbs algorithm \n  \nWe now extend the exchange algorithm in order to generate samples from our full posterior on a population of networks. As the name suggests, the exchange-within-Gibbs algorithm combines the exchange algorithm with the Gibbs sampler (Geman and Geman  ) to produce samples from the desired posterior. Note that we can treat the unknown parameters of the model   as components of a single multi-dimensional parameter. We iteratively sample each component from its conditional distribution given the remaining components. \n\nThe full exchange-within-Gibbs scheme is outlined in Algorithm\u00a02. Since each step samples from the respective full conditional distribution, the algorithm ensures that the stationary distribution of the resulting Markov chain is indeed the joint posterior   (Tierney  ). As with the exchange algorithm for the single-network Bayesian ERGM, the most computationally expensive step is sampling   from  , i.e. simulating an exponential random graph with parameter  . Moreover, this step must be performed for each of the individual-level parameter   updates. Thus, the computational cost of each iteration increases linearly with the number of networks in the data. However, these updates may be performed in parallel so, with access to a sufficient number of computing cores, the actual computational time per iteration typically increases sub-linearly with the number of networks. \n\n  \nThe exchange-within-Gibbs algorithm for a multilevel Bayesian ERGM  \n  \n\n#### Choice of parametrisation: centering vs. non-centering \n  \nThe parametrisation of general multilevel models in the context of MCMC computation has been studied in some detail (Gelfand et\u00a0al.  ; Papaspiliopoulos et\u00a0al.  ,  ; Yu and Meng  ). Here, we discuss the two most commonly used parametrisations: the \u2018centered\u2019 and the \u2018non-centered\u2019. Let   be the mean for the   ERGM parameter  . The parametrisation presented thus far is known as the centered parametrisation (CP) Gelfand et\u00a0al. ( ), Papaspiliopoulos et\u00a0al. ( ), in which the parameters ( ) are independent of the data  : In contrast, the   non-centred   parametrisation (NCP) can be written as follows: The identity   confirms the equivalence of the two parametrisations. Note that the parameter   enters the likelihood directly in ( ) via the  , so the conditional distribution of   given the remaining parameters has an intractable normalising constant  . As above, this can be dealt with via an exchange update, in this case requiring simulation of   n   networks for the normalising constants to cancel in the acceptance ratio, now given by The centred parametrisation and the non-centred parametrisation tend to be complementary: when one performs poorly, the other tends to perform better (Papaspiliopoulos et\u00a0al.  ). More precisely, the centred parametrisation tends to lead to more efficient MCMC performance when   is well-identified by the data  , whereas the non-centered parametrisation can be more competitive when   is weakly-identified (relative to  ). However, if we are in an intermediate setting, and when the parameters of interest are the higher-level parameters  , it is possible to combine both approaches using an ancillarity-sufficiency interweaving strategy (ASIS; Yu and Meng ( )). ASIS works by combining the updating schemes of the CP and NCP approaches, introducing an intermediate step to first draw ( ) under the centred parametrisation, and then redrawing the parameters under the non-centered parametrisation. The ASIS algorithm for a multilevel Bayesian ERGM is described in Algorithm\u00a03. \n\n  \nThe ASIS algorithm for a multilevel Bayesian ERGM \n  \n\n\n#### Proposal adaptation \n  \nWe use multivariate normal random walk proposals in the respective exchange updates of both   and  , for example The choice of the the proposal covariance matrix   is crucial to the overall efficiency of the MCMC algorithm; we wish to make large proposals that are likely to be accepted in order to explore the posterior in as few iterations as possible. A common approach to tuning covariance proposals for a wide range of random walk based algorithms, including Metropolis-within-Gibbs, is to target an acceptance rate close to 0.234, with acceptance rates between 0.1 and 0.5 often yielding satisfactory results (Roberts et\u00a0al.  ,  ; Roberts and Rosenthal  ). Since manual tuning of the   proposal covariance matrices would be impractical, we instead implement an adaptive proposal scheme. \n\nFor each proposal, we use a version of the adaptive Metropolis algorithm (Haario et\u00a0al.  ) considered by Roberts and Rosenthal ( ). Specifically, for the first 1000 iterations, we adapt every 20 iterations, with proposals of the form where   is the sample covariance matrix of the posterior samples   and   is an additional scaling factor that is varied to control the magnitude of the proposals. Following Roberts and Rosenthal ( ), we set  . The role of   is to adapt the direction of the proposals to the MCMC run so far, while   serves to target an acceptance rate of 0.234. Specifically, we start with   and increase (resp. decrease)   by   if the acceptance rate was below (resp. above) 0.234 in the previous 20 iterations. \n\n\n\n### Posterior predictive assessment \n  \nHaving produced a sufficient number of samples from the posterior distribution, we then assess whether the model adequately describes the data. Since determining the distribution of appropriate test quantities is difficult, assessing such goodness-of-fit for ERGMs is typically performed graphically (Hunter et\u00a0al.  ). For a single ERGM fit, one can simulate a large number of networks from the fitted model and compare these \u2018posterior predictive networks\u2019 to the observed network. This comparison is usually done via a set of network metrics. If a model fits the data well then the network metrics of the posterior predictive networks should be similar to those of the observed network. \n\nFor a population of networks, we can apply the same principles. To do so, we choose uniformly at random   S   values from the posterior samples of  . For each value, we simulate a network from  . We can then compare these posterior predictive networks to the observed networks based on a set of network metrics. For this purpose, we will use three important network metric distributions that are not explicitly modelled, namely degree distribution, geodesic distance distribution (length of shortest paths) and edge-wise shared partners distribution. \n\n\n\n## Results \n  \nTo illustrate our method, we apply it to a set of simulated networks, demonstrating that it is capable of recovering the ground truth. We also apply our method to resting-state fMRI networks from the Cam-CAN project, a study on healthy ageing (Shafto et\u00a0al.  ), to assess how network structure varies with age and fluid intelligence. The R scripts used to generate these results can be found at  . \n\n### Simulation \n  \nWe generated sets of 30-node networks with nodes split into two \u2018hemispheres\u2019 of 15 nodes each. We simulated the networks from an exponential random graph model with three terms: total number of edges (\u2018edges\u2019), total number of edges between nodes in the same hemisphere (\u2018nodematch.hemisphere\u2019), and the geometrically-weighted edgewise-shared partner (GWESP) statistic (\u2018gwesp.fixed.0.9\u2019). The GWESP statistic of a network   y   is a measure of clustering and is given by: where   is the number of connected node pairs having exactly   w   shared partners and   is a decay parameter, which we fix at  . The decay parameter attenuates the effect of the number of higher-order edgewise shared partners relative to lower-order edgewise shared partners. \n\nWe simulate networks under three distinct settings, varying the number of networks in each case: (i) a population of networks with no additional covariate information, (ii) a population of networks where each network is associated with a single continuous covariate, and (iii) a population of networks with two subgroups, so that each network is associated with a binary covariate indicating group allocation. \n\n#### No covariate information \n  \nTo simulate the networks, we first generated individual-level parameters   where We then used the ergm R package (Hunter et\u00a0al.  ) to simulate   n   networks  . The simulation procedure is based on an MCMC algorithm, initialised at a network with the prescribed number of nodes and covariates (in this case, hemisphere labels). With these simulated networks, we applied our exchange-within-Gibbs algorithm with ASIS (Algorithm\u00a03) to generate 12,000 posterior samples, adapting the random-walk proposals for the first 1,000 iterations, and discarding the first 2,000 as burn-in. \n\nFigure\u00a0  displays summaries of the posterior samples for the group-level mean parameter   of the model fit to   networks. The true value of   is covered by the posterior density, while the trace and autocorrelation plots indicate that the MCMC has mixed well. To assess the goodness-of-fit, we generated   networks from the model at posterior samples of   chosen uniformly at random. Figure\u00a0  shows the degree distribution, geodesic distance distribution and edgewise shared partner distribution of these simulated networks against those to which the model was fit.   \nPosterior samples produced by the exchange-within-Gibbs algorithm for the group-level mean parameter,  , of a single group of ten simulated networks. The true value of   is indicated by the red line. (colour figure online) \n    \nGraphical goodness-of-fit assessment for a single group of ten simulated networks. The box plots correspond to the simulated networks, while the ribbons represent 90% credible intervals corresponding to the posterior predictive networks. Note that a geodesic distance of infinity between two nodes means that there is no path connecting the nodes. (colour figure online) \n  \n\nTo complete our analysis of a single group of networks, we compare the density of the posterior samples between groups of size  . Figure\u00a0  illustrates how the posterior samples of   concentrates around the true value as the number of networks in the group increases. We also investigated different settings for the prior hyperparameters (  and  ) with  , finding that these did not have an appreciable effect on the posterior density (Supplementary Figure 2).   \nPosterior density plots for the effect parameters   in a single group of networks with no covariate information. As the number of networks increases, the posterior concentrates around the true value, depicted by the red vertical line. (colour figure online) \n  \n\nAs a supplementary analysis, we investigated our model\u2019s performance for increasing network size  , where   N   corresponds to the number of nodes in each network. We kept the number of auxiliary MCMC iterations used to simulate each network within the exchange algorithm fixed at  . This ensured that the computation time for each of these settings was of similar order, ranging from 50\u00a0min for   to 90\u00a0min for   using using 10 Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz processors on a computing cluster. However, the number of auxiliary iterations necessary for convergence increases with network size (Krivitsky and Handcock  ) and hence these auxiliary draws may not adequately represent draws from the desired ERGM required in the exchange algorithm. Supplementary Figure 1 illustrates this, with model performance degrading significantly for  . \n\n\n#### Continuous covariate \n  \nWe now consider a simulation setting where each network is a associated with a single continuous covariate, such as age. We again consider three cases with   networks in the population, with model matrix   and   where   and  , so that the network-level parameter means are uniformly spaced between the vectors   a   and   b  . We then generate   with   as above, and Again, the posterior samples of   concentrate around the true values for both the intercept and the covariate effect parameters as the number of networks in the group increases (Fig.\u00a0 ).   \nPosterior density plots for the effect parameters   in a single group of networks associated with a single continuous covariate   x  . As the number of networks increases, the posterior concentrates around the true value, depicted by the red vertical line. (colour figure online) \n  \n\n\n#### Binary covariate \n  \nTo complete our simulation study, we consider a multilevel setting in which the networks are split into two distinct groups  , so that   if   and   if  . We set   so that   if   and   if  . As above, we first generated individual-level parameters  , where   denotes the group membership of the   network, and then simulated networks  . We considered a range of numbers of networks,   each of the two groups. The true values were Figure\u00a0  shows the density of the posterior samples for the group-level parameters   for increasing number of networks   n   per group. We see that, as in the single-group setting, the posteriors concentrate around the true values for each group as the number of networks increases.   \nPosterior density plots for a two-group model with   simulated networks in each group. (colour figure online) \n  \n\n\n\n### Application to human functional connectivity brain networks \n  \nWe now turn our attention to a real data example: networks derived from resting-state fMRI scans of human brains from the Cambridge Centre for Ageing and Neuroscience (Cam-CAN) research project Shafto et\u00a0al. ( ), a study on the effect of healthy ageing on cognitive and brain function. The Cam-CAN dataset consists of a range of cognitive tests and functional neuroimaging experiments for approximately 650 healthy individuals aged 18\u201387. Our aim will be to assess how the functional connectivity structure of the brain varies with age and fluid intelligence, as measured by the Cattell score. \n\nFull details of data collection and preprocessing can be found in Lehmann et\u00a0al. ( ). To summarise, both structural (T1 and T2) and eyes-closed, resting-state fMRI scans (261 volumes, lasting 8min 40\u00a0s) were acquired for each individual. The fMRI scans were motion-corrected and co-registered to the respective structural scans and then mapped to the common Montreal Neurological Institute (MNI) template to ensure comparability across individuals. The fMRI time series were then extracted from 90 cortical and subcortical regions of interest (ROIs) from the AAL atlas (Tzourio-Mazoyer et\u00a0al.  ) and adjusted for various confounds using the optimised pipeline of Geerligs et\u00a0al. ( ). \n\nTo construct networks for each individual, we followed a thresholded correlation matrix approach. For individual   i  , we computed the pairwise Pearson correlation between each of the   preprocessed time series, yielding a   correlation matrix  . We then applied a threshold   r   to   to produce an   adjacency matrix,  , with entries: The adjacency matrix defines an individual\u2019s network,  , with an edge between nodes   k   and   l   if and only if  . The threshold   r   was chosen to yield an average node degree of 3 across all the networks, as recommended by Fallani et\u00a0al. ( ). See Table\u00a0  for summary statistics on the resulting networks for these individuals, as well as their ages and Cattell scores.   \nSummary of age, Cattell score, network density, and network transitivity for all individuals in the Cam-CAN fMRI dataset, as well as the youngest 100 individuals, and the oldest 100 individuals \n  \nCattell scores were missing for 14 out of the 587 individuals \n  \n\nWe model the population of networks using the framework described in Sect.\u00a0  with an exponential random graph model with four terms: total number of edges (\u2018edges\u2019), total number of edges between nodes in the same hemisphere (\u2018nodematch.hemisphere\u2019), total number of edges between homotopic nodes (mirror ROIs in each hemisphere; \u2018nodematch.homotopy\u2019) and the geometrically-weighted edgewise-shared partner (GWESP) statistic with decay parameter   (\u2018gwesp.fixed.0.9\u2019). \n\n#### Young vs. old \n  \nWe first turn our attention to an age-only analysis, comparing the functional connectivity network structure between the 100 youngest individuals, indexed  , aged 18-33, and the 100 oldest individuals,  , aged 74\u201387. As in the simulation experiment with a binary covariate, we have   if   and   if  . \n\nWe used the exchange-within-Gibbs algorithm with ASIS to generate 22,000 posterior samples, discarding the first 2,000 samples as burn-in. Figure\u00a0  shows summaries of the posterior samples for  , with the trace and autocorrelation plots demonstrating that the MCMC has mixed well. The posterior density plots show that the clearest difference between the old group and the young group was the difference in the parameter associated with the number of edges between homotopic nodes (\u2018nodematch.homotopy\u2019). While this parameter is large and positive for both groups, it is moderately smaller in the old group, indicating that the propensity for homotopic connections is lower in old age. On the other hand, there is no clear evidence for group differences in the remaining parameters. The edges parameters are large and negative, pointing to the overall sparsity of the networks; the intrahemisphere parameters (\u2018nodematch.hemisphere\u2019) are small and positive, indicating a moderate propensity for connections between nodes in the same half of the brain; and the GWESP parameters are also positive, indicating a propensity to form triangles and thus a degree of functional segregation (Bullmore and Sporns  ).   \nMCMC output from the exchange-within-Gibbs algorithm for the group-level mean parameters of a population of resting-state fMRI networks from a group of 100 young individuals and a group of 100 old individuals. (colour figure online) \n  \n\nTo assess goodness-of-fit, for both groups we generated   networks from the model at posterior samples of   chosen uniformly at random. Figure\u00a0  indicates a reasonable fit for both groups, with the geodesic distance and edgewise shared partner distributions showing a good correspondence between the simulated networks and the observed networks. There appears to be a slight discrepancy in the degree distributions, with the simulated networks in the young group in particular having fewer nodes of degree 4 to 6 relative to the observed networks.   \nGraphical goodness-of-fit assessment for resting-state fMRI networks from a young group and an old group, fitted in a joint model. The box plots correspond to the observed networks, while the ribbons represent 95% credible intervals corresponding to the posterior predictive networks. Note that a geodesic distance of infinity between two nodes means that there is no path connecting the nodes. (colour figure online) \n  \n\n\n#### Age and fluid intelligence \n  \nFinally, we consider a model that jointly assesses the effect of age and fluid intelligence on the brain\u2019s functional connectivity structure. We use the same ERGM summary statistics as before - edges, \u2018nodematch.hemisphere\u2019, \u2018nodematch.homotopy\u2019, GWESP - and set where   and   denote the age and Cattell score (a measure of fluid intelligence), respectively, of individual   i  . For this model, we took a subset of 100 individuals across the range of non-missing Cattell scores. We highlight the use of the interaction term between age and fluid intelligence to capture the joint effect of these two covariates over and above their corresponding main effects. We again used the exchange-within-Gibbs algorithm with ASIS to generate 22,000 posterior samples, and discarded the first 2,000 samples as burn-in. \n\nFigure\u00a0  shows the density plots for the resulting posterior samples. As with the previous analysis comparing a group of young individuals and a group of old individuals, the clearest age-related effect was associated with the number of edges between homotopic nodes. Higher fluid intelligence, as measured by the Cattell score, was associated with a higher propensity for the total number of edges, but a lower propensity for both intrahemispheric connections and homotopic connections. Reduced homotopic connectivity has previously been observed in rs-fMRI networks, with evidence suggesting that reduced synchrony between brain hemispheres at rest may be predictive of higher intelligence (Santarnecchi et\u00a0al.  ). The parameter estimates for the age - fluid intelligence interaction term were centred around zero, indicating no additional effect on top of the additive effects associated with age and fluid intelligence separately. To explore possible non-linear effects of age and fluid intelligence, we also fit a model with quadratic terms for age and Cattell score, finding no quadratic effects for age but a small quadratic effects on intrahemispheric connections (positive) and triangle propensity (GWESP; negative) (Supplementary Figure 3).   \nMCMC output from the exchange-within-Gibbs algorithm for the effect parameters of a population of 100 resting-state fMRI networks. (colour figure online) \n  \n\n\n\n\n## Discussion \n  \nThe main contribution of this article is to introduce a multilevel framework for modelling populations of networks with network-level covariate information, along with a novel MCMC procedure for performing inference with the framework. While the framework itself is a natural multilevel extension of a single ERGMs, the inference procedure is more involved due to the intractability of the ERGMs likelihood and the challenges associated with MCMC for hierarchical models. We have presented how our framework can be applied to resting-state fMRI data to assess how the brain\u2019s functional connectivity network structure varies with age and intelligence score. Although we chose here to focus on networks constructed from resting-state fMRI scans, our framework could also be applied to networks derived from other neuroimaging modalities such as magnetoencephalography (MEG) or diffusion tensor imaging (DTI). \n\nAn important extension to the framework would be to use weighted exponential random graph models (Krivitsky  ; Desmarais and Cranmer  ). These are an extension of the binary ERGMs that can be applied to weighted networks, thus avoiding the thresholding step in the construction of functional connectivity networks. Indeed, one version of a weighted ERGMs, the generalised exponential random graph model (GERGM) (Desmarais and Cranmer  ) was recently applied to a 20-node functional connectivity network (Stillman et\u00a0al.  ). This approach has the additional advantage of modelling the mean connectivity directly and thus would avoid any confounding due to differences in mean connectivity. However, the GERGM is at present extremely computational intensive, rendering it infeasible for a population of networks. \n\nOne of the key challenges in applying our framework to real data is the choice of which network summary statistics to include in the model. A fully Bayesian model selection method based on reversible-jump MCMC has been developed for exponential random graph models on single networks (Caimo and Friel  ). A similar approach could be developed for our framework, though the computational cost is likely to be prohibitive. A more pragmatic approach would be to develop a graphical goodness-of-fit method by comparing the posterior predictive distributions under different models. More flexible specifications of the relationship between the covariates and ERGM parameters, such as spline-based models, would also be a fruitful avenue for future work. \n\nThe computational cost of our MCMC algorithm is considerable. Even with a 20-core computing cluster (Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz), the algorithm took over 5\u00a0h to produce the 22,000 posterior samples in the real data example presented above. The main computational bottleneck lies in simulating the exponential random graphs at each MCMC iteration. While the computational cost should increase roughly linearly in the number of networks, Krivitsky and Handcock ( ) provide empirical evidence indicating that the cost may grow on the order of   where   p   is the number of summary statistics,   N   is the number of nodes, and   E   is the number of edges. It may be possible to reduce the number of ERGMs simulations at each MCMC iteration using noisy Monte Carlo methods (Alquier et\u00a0al.  ). Other promising avenues include variational inference for ERGMs (Tan and Friel  ), or pseudolikelihood methods (Bouranis et\u00a0al.  ), which could both be extended to our framework to yield approximate Bayesian inference at a much reduced computational cost relative to MCMC. \n\n\n\n### Supplementary Information \n  \nBelow is the link to the electronic supplementary material. \n\n\n \n", "metadata": {"pmcid": 11186958, "text_md5": "a5b9e23b1d1b6a79a8aae0e2f6f32e3e", "field_positions": {"authors": [0, 32], "journal": [33, 44], "publication_year": [46, 50], "title": [61, 155], "keywords": [169, 281], "abstract": [294, 1877], "body": [1886, 44001]}, "batch": 2, "pmid": 38911222, "doi": "10.1007/s11222-024-10446-0", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11186958", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=11186958"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11186958\">11186958</a>", "list_title": "PMC11186958  A Bayesian multilevel model for populations of networks using exponential-family random graphs"}
