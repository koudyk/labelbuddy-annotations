{"text": "Williams, Brendan and Hedger, Nicholas and McNabb, Carolyn B. and Rossetti, Gabriella M. K. and Christakou, Anastasia\nFront Neurosci, 2023\n\n# Title\n\nInter-rater reliability of functional MRI data quality control assessments: A standardised protocol and practical guide using pyfMRIqc\n\n# Keywords\n\nfMRI\nresting state fMRI\ntask fMRI\nquality control\ninter-rater reliability\n\n\n# Abstract\n \nQuality control is a critical step in the processing and analysis of functional magnetic resonance imaging data. Its purpose is to remove problematic data that could otherwise lead to downstream errors in the analysis and reporting of results. The manual inspection of data can be a laborious and error-prone process that is susceptible to human error. The development of automated tools aims to mitigate these issues. One such tool is pyfMRIqc, which we previously developed as a user-friendly method for assessing data quality. Yet, these methods still generate output that requires subjective interpretations about whether the quality of a given dataset meets an acceptable standard for further analysis. Here we present a quality control protocol using pyfMRIqc and assess the inter-rater reliability of four independent raters using this protocol for data from the fMRI Open QC project ( ). Data were classified by raters as either \u201cinclude,\u201d \u201cuncertain,\u201d or \u201cexclude.\u201d There was moderate to substantial agreement between raters for \u201cinclude\u201d and \u201cexclude,\u201d but little to no agreement for \u201cuncertain.\u201d In most cases only a single rater used the \u201cuncertain\u201d classification for a given participant\u2019s data, with the remaining raters showing agreement for \u201cinclude\u201d/\u201cexclude\u201d decisions in all but one case. We suggest several approaches to increase rater agreement and reduce disagreement for \u201cuncertain\u201d cases, aiding classification consistency. \n \n\n# Body\n \n## Introduction \n  \nFunctional magnetic resonance imaging (fMRI) data are inherently multi-dimensional with many potential sources of artefacts that can lead to spurious results ( ;  ). Therefore, ensuring data are of sufficient quality for analysis is an essential step in the processing of fMRI data. This is especially important for large multi-site studies such as the Adolescent Brain Cognitive Development study ( ), and the Human Connectome Project ( ), where time required to perform detailed, manual screening of individual data can quickly become intractable. To address this, many quality control tools and pipelines now exist to help users make informed decisions about quality in their datasets ( ;  ;  ). These tools\u2013which automate part of the quality control process\u2013aim to decrease the time taken to assess data quality, minimise the amount of prior knowledge needed to make informed decisions, and reduce errors during assessment. \n\nSeveral tools currently exist for assessing the quality of fMRI data, including MRIQC ( ), and Visual QC ( ). This list also includes pyfMRIqc, which we developed at the Centre for Integrative Neuroscience and Neurodynamics (CINN), University of Reading ( ). Many of our neuroimaging facility users at CINN are Ph.D. students and early career researchers, who join a community that prioritises practical training and learning opportunities. As part of this commitment, we develop software which is user-friendly and empowers individuals to become confident and informed researchers. pyfMRIqc helps users to make informed decisions about the quality of their data by generating various image quality metrics and presenting them in an easily interpretable way in a visual report. pyfMRIqc also has extensive online documentation that describes to users how these plots are generated and what they show, and aids their interpretation with examples. Users of pyfMRIqc can generate these reports with minimal programming experience, requiring only a single line of code to run the software and without the need for using containerised environments for generating output. As part of the work presented here, we additionally developed a piece of software, \u201ccinnqc,\u201d which we used to automate the minimal pre-processing and curation of data for pyfMRIqc, and to identify cases where data deviate from the expected acquisition parameters for the dataset. \n\nPrevious reports describe the use of inter-rater reliability for the quality assessment of structural imaging data ( ;  ;  ;  ). For instance,   developed a method for quickly assessing the registration of T1 weighted images to standard MNI space. Raters included citizen scientists who had no previous experience with MRI data, as well as expert raters. Their protocol resulted in good reliability, particularly with respect to which images were deemed to fail quality assessment, between expert raters, with citizen scientists also showing agreement. The study therefore demonstrated that this straightforward approach for assessing registration quality was consistent between individuals with different skill levels. Another protocol assessed for reliability between raters was presented by  , who aimed to provide a workflow for the quality control assessment of T1 images both during and after image acquisition to maximise useful sample size. Images were classified into three categories (pass, check, fail), and these three categories were associated with significant differences in cerebral cortex, left amygdala, and total grey matter volume estimations. Reliability between two raters for the three classification categories was high [intra-class correlation coefficient (\u03b1 = 0.931)], in line with results from  , who found good consistency between expert raters when a three category rating system was used (although notably concordance was significantly lower when using five categories). Lastly,   demonstrated fair to moderate agreement between two raters when assessing the quality of T1 data from the ABIDE dataset. These studies demonstrate that reasonable reliability can be expected of subjective decisions about the quality of structural imaging data, particularly when three categories are used to classify data. However, in the case of functional data, and despite its potential utility, inter-rater reliability has not been similarly evaluated to help understand the consistency of subjective decisions about data quality. To assess whether experienced raters are reliable in their classifications of functional data quality across datasets, we used data from the fMRI Open QC project,  which included data with different acquisition parameters from multiple sites. \n\nWe assess the inter-rater reliability of fMRI data quality assessments for task-based and resting state data. We describe quantitative and qualitative criteria for classifying data quality, present a quality control protocol for assessing raw fMRI data quality using pyfMRIqc, assess reliability between four independent raters using this protocol, and provide example cases of different data quality issues using output from pyfMRIqc. Raters classified data into one of three assessment categories, \u201cinclude,\u201d \u201cuncertain,\u201d or \u201cexclude.\u201d Using our protocol, we find moderate to substantial reliability between raters, particularly for \u201cinclude\u201d and \u201cexclude\u201d decisions, but less agreement between raters for the uncertain classification. \n\n\n## Materials and methods \n  \n### Participants \n  \n#### Imaging data participants \n  \nImaging data from 129 subjects were included. Each subject had a T1 weighted high-resolution anatomical image, and a single-band echo-planar imaging (EPI) image for either task-based or resting state functional magnetic resonance imaging (fMRI) acquisition. Task-based fMRI data were included for 30 subjects. Resting-state fMRI data were included for 99 subjects; resting-state data originated from five sites, with approximately 20 subjects per site. Data originated from the following publicly available datasets: ABIDE, ABIDE-II, Functional Connectome Project, and OpenNeuro ( ;  ;  ). Data from each site were treated as separate datasets for the purpose of performing quality assessment. The expected acquisition parameters for data from each site are summarised in  . The data presented here are available on the Open Science Framework page of the fMRI Open QC project (see text footnote 1). \n  \nExpected acquisition parameters for subjects in each site in the main dataset. \n    \n\n#### Quality control raters \n  \nQuality control assessments were completed by four independent raters (BW, NH, CBM GMKR), who were all postdoctoral research fellows, and all raters had previous experience in quality assessment, processing and analysis of functional neuroimaging data. Two raters (BW and GR) had previously used pyfMRIqc to perform quality assessment of fMRI data. Additionally, BW was involved in the development of pyfMRIqc. Each rater reviewed data for 104 of the 129 subjects, using outputs from cinnqc and pyfMRIqc. Subject assignment ensured at least four subjects from each site were reviewed by all four raters, and every other subject was reviewed by three raters. Assignments were also balanced so that the proportion of overlapping cases was equal across raters (see   for details of rater assignments). \n\n\n\n### Data processing \n  \nMinimal pre-processing of anatomical T1 weighted and functional EPI data was performed using the FSL toolbox (version 6.0) from the Oxford Centre for Functional MRI of the Brain (FMRIB\u2019s Software Library ) ( ). Data pre-processing, curation, and quality control was automated using \u201ccinnqc.\u201d  cinnqc provides wrapper scripts for executing and curating output from FSL pre-processing functions (e.g., motion correction, registration, and brain extraction), and also generating pyfMRIqc reports for minimally pre-processed data. To pre-process data, the T1 image was skull stripped using the Brain Extraction Tool ( ), then grey matter, white matter, and cerebrospinal fluid tissue segmentation was performed using FMRIB\u2019s Automated Segmentation Tool ( ). Functional EPI data were motion corrected with MCFLIRT ( ), using affine transformations to align the first volume of functional data with each subsequent volume. Functional EPI and anatomical T1 data were then co-registered using the epi_reg function,  and a linear affine transformation was used to convert a brain extracted mask of the T1 anatomical image to functional EPI space using FMRIB\u2019s Linear Image Registration Tool ( ;  ). The brain mask in functional EPI space was then re-binarised using a threshold of 0.5. Image quality metrics and plots were generated using pyfMRIqc ( ) to aid data quality assessment, e.g., the identification of artefacts that were participant-, sequence-, technique-, or tissue-specific. pyfMRIqc was run with the following input arguments: -n < motion corrected EPI data >, -s 25, -k < brain extracted mask in functional space > -m < motion parameter output from MCFLIRT >. \n\n\n### Resources \n  \nUbuntu 20.04.4 LTS \n\nFSL version 6.0 (see text footnote 2). \n\nAnaconda 4.10.1 \n\nPython 3.8.8 \n  \ncinnqc 0.1.0 \n  \neasygui 0.98.3 \n  \nmatplotlib 3.3.4 \n  \nnibabel 3.2.1 \n  \nnumpy 1.20.1 \n  \npandas 1.2.4 \n  \n\n### Quality assessment protocol \n  \nRaters were given the following instructions before beginning quality assessment: \n\nThe following criteria need to be used to classify all images  : \n  \nInclude\u2013no quality assessment issues that indicate the dataset is problematic. \n  \nUncertain\u2013some quality assessment issues that makes the inclusion of dataset marginal. \n  \nExclude\u2013quality assessment issues that mean the data should not be included. \n  \nEach image classified as either \u201cuncertain\u201d or \u201cexclude\u201d should include an explanation of why the given classification was made. Please be as descriptive as possible when explaining your decision-making. \n\nQuality assessment decision-making should be supported by the output produced by cinnqc and pyfMRIqc. cinnqc and pyfMRIqc derivatives can be found online in the directories /cinnqc/ examples/{fmriqc-open-qc-task, fmriqc-open-qc-rest-100, fmriqc-open-qc-rest-200, fmriqc-open-qc-rest-300, fmriqc-open-qc-rest-400, fmriqc-open-qc-rest-500, fmriqc-open-qc-rest-600, fmriqc-open-qc-rest-700}/derivatives/cinnqc/of the cinnqc GitHub page (see text footnote 3). \n\n#### Quantitative data assessment \n  \nQuantitative quality assessment criteria for T1 and EPI data based on acquisition parameters and derived metrics from the data are summarised in  . Thresholds for absolute and relative motion, as calculated using MCFLIRT, are given to limit its effect on data quality. Motion thresholds are defined in   and are summarised in the pyfMRIqc report. Yet, even motion that is sub-threshold could still impact data quality. Qualitative data assessment should be carried out to check whether any motion incidents coincide with a problematic change in signal. Temporal signal to noise (TSNR, referred to as SNR in pyfMRIqc) is calculated as mean intensity divided by the standard deviation of voxels (25th centile mean intensity) outside the brain-extracted mask in functional space. It is calculated by pyfMRIqc on minimally pre-processed data. Slice-wise TSNR should be checked in the pyfMRIqc report, and potentially problematic slices should be followed up using qualitative assessments. Field of view, number of volumes, and scans are checked using cinnqc, and a file with the suffix   *_notes.txt   is generated to describe any potential issues. Note, some voxel dimensions may appear to be different due to rounding, but if they are equal to 2 decimal places then subjects do not need to be excluded. T1 and EPI data should have whole brain coverage, which includes the cerebral cortex and subcortical brain regions (but not necessarily the cerebellum). A summary of quantitative assessment criteria can be found in  , and a summary of the expected acquisition parameters can be found in  . \n  \nQuantitative criteria for determining dataset inclusion/exclusion. \n    \n\n#### Qualitative data assessment \n  \npyfMRIqc generates a number of plots and tables that can be helpful in the qualitative assessment of data. Mean and slice-wise scaled squared difference (SSD) is calculated by squaring the difference in voxel intensity between consecutive volumes, and dividing by the global mean squared difference. In the QC plots section, mean and slice-wise SSD graphs can be used to identify global, and slice-wise changes in signal intensity, respectively. SSD is also plotted alongside the global normalised mean voxel intensity, normalised SSD variance, plus absolute and relative motion to visualise relationships between changes in SSD, signal intensity, and motion. Further, mean, minimum, and maximum SSD is plotted slice-wise to determine whether issues are present in specific slices. \n\nThe plot of the \u201cMean voxel time course of bins with equal number of voxels\u201d is generated by binning voxels into 50 groups, based on their mean intensity, and calculating the mean intensity for voxels in each bin for each volume. Bins are ordered top-down from lowest mean intensity voxels (non-brain/cerebrospinal fluid) to highest (grey matter, then white matter voxels). This plot enables easy visualisation of signal variance and was originally described by  , where further information can also be found. \n\nThe \u201cMasks\u201d plot can be helpful in indicating whether there were issues during acquisition or processing (such as brain extraction and/or registration of T1 and EPI data). For instance, there may be many brain voxels that are not highlighted in blue. If this is the case, then scans should be carefully checked for signal distortion (described below), or processing steps may need to be manually re-run with adjusted input parameters. Poor registration (for instance, misalignment of gross anatomical structures including brain surface, or grey matter/white matter/cerebrospinal fluid boundaries) may be indicative of other data quality issues. \n\nThe \u201cVariance of voxel intensity\u201d plot visualises the variance in signal in each voxel over the timeseries of the functional run. The png image given in the pyfMRIqc report is thresholded (voxel intensities are divided into 1,000 equal width bins, and the intensity of the highest bin with at least 400 voxels is used) to aid visualisation, however a nifti version of the image is also included which is unthresholded. This nifti image is useful for more in-depth investigation if there are potential quality issues or the figure appears problematic. The \u201cSum of squared scaled difference over time\u201d plot presents the voxel-wise sum of SSD over the functional run. Similarly to the \u201cVariance of voxels intensity\u201d plot, we applied a threshold for the png figure for readability (sum of squared scaled differences are divided into 50 equal width bins, and the upper threshold of the fifth bin is used), but the nifti image does not have a threshold. \n\nTo inspect data for signal distortion, load T1 images from the subject\u2019s BIDS directory; for EPI images, load the image with the suffix   *_example-func.nii.gz   from the subject\u2019s cinnqc BIDS derivative directory, and the mean voxel intensity nifti file from pyfMRIqc. If visual abnormalities are present, this could impact the signal (e.g., image distortion, signal loss, artefacts such as ringing or ghosting), or processing (e.g., brain extraction, registration, motion correction) of T1 or EPI data. To determine if this is the case, the plots from pyfMRIqc can be used to aid subject classification. Detailed explanations for interpreting pyfMRIqc plots and tables can be found in the pyfMRIqc User Manual.  A summary of qualitive assessment criteria can be found in  . \n  \nQualitative criteria for determining dataset inclusion. \n  \n\n\n### Rater calibration and reliability assessment \n  \nEach rater independently assessed and classified subjects using the quality assessment protocol described above. To ensure quality assessment criteria were interpreted consistently, BW used the quality assessment protocol to identify exemplar subjects for issues and presented these training cases to the other raters ( ). \n  \nExample cases that were used during the calibration session for raters before independently assessing the whole dataset. \n  \nFleiss\u2019 kappa ( ) was calculated using the \u201cirr\u201d package in R ( ) to assess pair-wise and category-wise inter-rater reliability between raters; to correct for multiple comparisons we used the Holm method to control the family-wise error rate using the \u201cp.adjust\u201d function in R ( ;  ). We chose to use Fleiss\u2019 kappa instead of Cohen\u2019s kappa, because Fleiss\u2019 kappa also allows us to determine how similar pairs of raters are across classifications by calculating category-wise agreement. We used the criteria described by   to interpret Fleiss\u2019 kappa using the following benchmarks to describe the strength of agreement: poor agreement < 0.00; slight agreement 0.00\u20130.20; fair agreement 0.21\u20130.40; moderate agreement 0.41\u20130.60; substantial agreement 0.61\u20130.8; almost perfect agreement 0.81\u20131.00. Overall agreement across raters and categories was calculated using Krippendorff\u2019s alpha ( ), which is useful as a measure of overall agreement because it is not restricted by the number of raters or the presence of missing data in the sample ( ). Krippendorff\u2019s alpha and bootstrap 95% confidence intervals (1,000 iterations, sampling subjects with replacement) were calculated in R using scripts from  . \n\n\n\n## Results \n  \nEach subject was categorised as either \u201cinclude\u201d (rater one: 68, rater two: 73, rater three: 80, rater four: 74), \u201cuncertain\u201d (rater one: 10, rater two: 12, rater three: 3, rater four: 9), or \u201cexclude\u201d (rater one: 26, rater two: 19, rater three: 21, rater four: 20) by the four raters. Overall percentage agreement between raters is summarised in  . \n  \nOverall percentage agreement between raters for \u201cinclude\u201d/\u201cuncertain\u201d/\u201cexclude\u201d assignments. \n  \nInter-rater reliability between pairs of raters was calculated using Fleiss\u2019 Kappa; overall agreement between all pairs of raters was moderate and significantly greater than chance level (rater 1\u20132: \u03ba = 0.536,   z   = 6.143,   p   < 0.001; rater 1\u20133: \u03ba = 0.437,   z   = 4.639,   p   < 0.001; rater 1\u20134: \u03ba = 0.456,   z   = 5.17,   p   < 0.001; rater 2\u20133: \u03ba = 0.448,   z   = 5.071,   p   < 0.001; rater 2\u20134: \u03ba = 0.596,   z   = 6.818,   p   < 0.001; rater 3\u20134: \u03ba = 0.578,   z   = 6.022,   p   < 0.001). Category-wise Kappa for all raters was moderate and substantial for \u201cinclude\u201d and \u201cexclude\u201d assignments respectively and was significantly greater than chance level (\u201cinclude\u201d: \u03ba = 0.514,   z   = 6.661,   p   < 0.001; \u201cexclude\u201d: \u03ba = 0.731,   z   = 9.472,   p   < 0.001). However, this was not the case for \u201cuncertain\u201d assignments, where agreement between raters was slight (\u03ba = 0.013,   z   = 0.166,   p   = 1.0). We also calculated Fleiss\u2019 Kappa category-wise for pairs of raters ( ). All raters had moderate to substantial agreement, and performed at significantly greater than chance level for \u201cinclude\u201d (rater 1\u20132:   z   = 5.697   p   < 0.001; rater 1\u20133:   z   = 3.849   p   < 0.001; rater 1\u20134:   z   = 3.591   p   < 0.001; rater 2\u20133:   z   = 4.409   p   < 0.001; rater 2\u20134:   z   = 5.269   p   < 0.001; rater 3\u20134:   z   = 5.253   p   < 0.001) and \u201cexclude\u201d (rater 1\u20132:   z   = 5.405   p   < 0.001; rater 1\u20133:   z   = 5.3   p   < 0.001; rater 1\u20134:   z   = 6.645   p   < 0.001; rater 2\u20133:   z   = 5.231   p   < 0.001; rater 2\u20134:   z   = 5.887   p   < 0.001; rater 3\u20134:   z   = 6.525   p   < 0.001) assignments, but not for \u201cuncertain\u201d (rater 1\u20132:   z   = 1.471   p   = 1.0; rater 1\u20133:   z   = \u22120.537   p   = 1.0; rater 1\u20134:   z   = 1.0   p   = 0.716; rater 2\u20133:   z   = 0.529   p   = 1.0; rater 2\u20134:   z   = 4.272   p   < 0.001; rater 3\u20134:   z   = \u22120.415   p   = 1.0) assignments ( ). Overall agreement in the dataset, as assessed using Krippendorff\u2019s alpha was 0.508 [95% bootstrap confidence intervals (0.381, 0.615)]; removing instances where \u201cuncertain\u201d was assigned increased Krippendorff\u2019s alpha to 0.694 [95% bootstrap confidence intervals (0.559, 0.802)]. In total, at least two raters categorised 97 subjects as include, 6 subjects as uncertain, and 26 subjects as exclude.   summarises the subject-wise group majority classification (\u201cinclude\u201d/\u201cuncertain\u201d/\u201cexclude\u201d). \n  \nReliability between pairs of raters for each category was assessed using Fleiss\u2019 Kappa. Agreement between raters at significantly greater than chance level is denoted as ***  p   < 0.001. \n  \n### QC \u201cexclude\u201d criteria examples \n  \n#### Data acquisition artefacts \n  \nImaging acquisition artefacts were identified in five subjects by at least one rater. These issues included ghosting (aliasing), ringing, and wraparound artefacts ( ). In   we present these three artefacts, with the relevant output from pyfMRIqc used to identify the issue. For the first subject, ghosting (aliasing) in the mean functional image from pyfMRIqc was detected ( ). This can be detected visually as the presence of spurious signal outside the perimeter of the head. In the second case, wraparound of the functional signal was detected in the mean functional image ( ). Wraparound can be detected when part of the head is partially occluded by the field of view. In this case, the most posterior portion of the head appears instead in the anterior portion of the image and is most noticeable visually on axial and sagittal slices. The third case contained in-plane artefacts in the data due to eye movements ( ). In this case, both the variance in voxel intensity, plus a peak in the maximum and sum of the scaled squared difference in affected slices (particularly slices 15\u201317) indicated the presence of physiologically unrelated changes in signal. These effects are especially pronounced around volumes 19\u201321, where there is a peak in the variance of the sum of squared difference. A video of flickering in affected slices is included in  . \n  \nNumber of datasets excluded by single, majority, and all raters for each of the relevant exclusion reasons. \n      \nExample cases of three different types of data acquisition artefacts detected using output from pyfMRIqc. \n  \n\n#### Motion \n  \n30 datasets were classified as \u201cexclude\u201d by at least one rater with issues relating to motion described in the notes ( ). Of these cases, 17 exceeded acceptable values set out in our quantitative criteria for absolute and relative motion ( ). The remaining cases were classified as \u201cexclude\u201d based on the residual effects of motion upon the data, despite the quantitative measure of motion being sub-threshold ( ). This includes decreases in global signal coinciding with the onset of motion events ( ), plus peaks in scaled squared difference and banding in the binned carpet plot ( ;  ). \n  \nExample cases of three different types of motion artefacts detected using output from pyfMRIqc. One subject (sub-013) was classified as uncertain by all raters, while sub-010 and sub-016 were classified as exclude by all raters. \n    \nAn example case of global signal loss following motion detected using output from pyfMRIqc. \n  \n\n#### Signal loss \n  \nSudden changes in global signal can be assessed in several ways using pyfMRIqc. For instance,   demonstrates when motion artefacts lead to a sudden decrease in global signal ( ). The onset of head motion around volumes 12 and 70, identified by the peaks in the mean and variance of the scaled squared difference plus the sum of relative and absolute movements, is immediately followed by a decrease in the normalised mean voxel intensity of around two standard deviations for approximately ten volumes ( ). Banding is also present in the binned carpet plot, where sudden changes in signal coincide with changes in intensity across all bins. Six of the reviewed subjects were reported as having SNR related issues by at least one of the four raters, and eleven were reported as having global signal issues ( ). \n\n\n#### Atypical brain structure \n  \nTwo subjects were excluded by one rater due to the presence of atypical brain structure in the T1 weighted anatomical image ( ). Both cases are detailed in  , with one subject having a right ventricle that was enlarged and covering greater than both the extent of the left ventricle and where we would typically expect the ventricle to cover. The second subject had an unexpected mass in their left ventricle, and hypointensities in white matter across the whole brain. We are unable to comment on the clinical relevance of these anatomical features as none of the authors have clinical expertise. \n  \nT1 weighted images for two cases of atypical brain structure that were present in the dataset. \n  \n\n\n### Uncertain cases \n  \n27 subjects were classified as \u201cuncertain\u201d by at least one rater; \u201cuncertain\u201d was used as a classification by a single rater for 21 subjects, and more than one rater for six subjects. For the subjects classified as \u201cuncertain\u201d by one rater, the other two/three raters gave the same classification (\u201cinclude\u201d/\u201cexclude\u201d) for 20 of the 21 subjects; one subject received one \u201cinclude,\u201d one \u201cuncertain,\u201d and one \u201cexclude\u201d classification. For the remaining six cases where more than one rater classified subjects as \u201cuncertain,\u201d the notes for four subjects indicated the presence of issues related to residual motion that were below our threshold, while the notes for the other two subjects indicated the presence of possible pathology in the T1 image and aliasing in functional data. Lastly, only one dataset was rated as \u201cuncertain\u201d by all raters ( ), with raters \u201cuncertain\u201d about the effects of sub-threshold residual motion on the data. \n\n\n\n## Discussion \n  \nThis work aimed to describe a protocol for assessing the quality of raw task-based and resting state fMRI data using pyfMRIqc, and to assess the reliability of independent raters using this protocol to classify data with respect to whether it meets an acceptable standard for further analysis. We used data from the fMRI Open QC Project [(see text footnote 1), data were derived from ABIDE, ABIDE-II, Functional Connectome Project, and OpenNeuro ( ;  ;  )]. Overall, we found moderate agreement between raters, and moderate to substantial category-wise agreement between raters for include/exclude classifications. Poor to moderate category-wise agreement was found for the uncertain classification, with reliability at significantly greater than chance level for only one pair of raters. Krippendorff\u2019s alpha for the include/exclude categories across all raters was sufficient to tentatively accept the raters\u2019 classifications were reliable ( , p. 241). We also provide examples for different types of quality issues that were identified in the dataset. \n\nFor the \u201cuncertain\u201d classification we found that there was a lack of reliability between raters, with two pairs of raters having negative \u03ba values, indicating no agreement ( ), and a further three pairs having coefficients close to 0. The lack of reliability between raters for the \u201cuncertain\u201d classification appears to be driven by the uncertainty of a single rater for a given subject. Of the 27 subjects rated \u201cuncertain\u201d by any rater, 21 (78%) were not rated \u201cuncertain\u201d by the other raters. Of the 6 subjects rated \u201cuncertain\u201d by more than one rater, uncertainty related to concerns about motion (  N   = 4), aliasing (  N   = 2), and possible pathology (  N   = 2). This included one subject (sub-013) who was classified by all raters as \u201cuncertain\u201d due to residual effects of motion, yet other similar subjects (e.g., sub-010 and sub-016) were unanimously classified as \u201cexclude\u201d despite having visually similar plots, and less maximum absolute motion ( ). In our quantitative exclusion criteria ( ), we give explicit thresholds for absolute and relative movement events, and though 17/30 excluded data sets were excluded by at least one rater due to exceeding our quantitative movement thresholds, 13/30 were excluded based on qualitative assessment of movement effects on data quality. These thresholds are relatively arbitrary, and despite being a helpful heuristic, they did not appear to capture all cases where motion had an adverse effect on data. pyfMRIqc counts the number of relative motion events > voxel size, 0.5 mm and 0.1 mm, and though we set our thresholds for the number of relative motion events > voxel size and 0.5 mm based on previous guidelines,  we did not set a threshold for motion events > 0.1, but < 0.5 mm. In cases where motion was sub-threshold but still an issue, persistent but small motion events could negatively impact data as we did not include a threshold for small (0.1 < motion < 0.5) motion events. Nevertheless, it is worth mentioning that data reviewed here by raters was only minimally pre-processed and that approaches such as ICA-based denoising ( ), the inclusion of motion parameters in a model ( ), and removing volumes affected by motion ( ) can, and often are used during data pre-processing to decrease the negative effects of motion on the signal in fMRI data. However, though these approaches are helpful for cleaning data that may otherwise be discarded, we feel that consensus guidelines for (un)acceptable levels of motion are needed to improve consistency within the neuroimaging community, in the same way the BIDS standard ( ) has been widely adopted as the   de facto   data formatting structure. \n\nIt is important to note that despite the data only being minimally pre-processed, the purpose of pyfMRIqc is not to determine whether data processing steps worked as expected, but to assess the quality of the data itself. We motion corrected data so that our metrics (e.g., scaled squared difference) are calculated for contiguous voxels in time and space but we do not directly measure whether all physical motion was corrected for. Brain extraction, spatial normalisation, distortion correction, and denoising, are all commonly used and important pre-processing steps in the pipeline of fMRI data analysis, and the efficacy of these pre-processing steps should also be checked as part of a robust analysis pipeline for ensuring data quality. Therefore, the output generated by pyfMRIqc should be treated as one part of a broader data processing procedure. Additionally, because the image quality metrics generated by pyfMRIqc have no absolute reference \u2013 that is they cannot be compared to a reference value since there is no ground truth \u2013 the detection of data quality issues is dependent on individual interpretation. One way to address this issue is by generating a database of reference values to aid outlier detection. This is the process used by MRIQC, which crowdsources image quality metrics to generate population-level distributions ( ). However, we are currently unable to generate these distributions with pyfMRIqc. \n\nCognitive biases may also influence subjective decision-making about the quality of fMRI data. The acquisition and preparation of an fMRI dataset involves great economic and time cost, and researchers may be motived more by these sunk costs to minimise loss from their own data than from secondary datasets. People tend to be loss averse ( ), and the thought of \u201cwasting\u201d the resources put into acquiring the dataset could bias individuals to perceive data quality issues as less problematic than if the data were collected independently. For instance,   found that people are less loss averse when making decisions for others compared to themselves, and that this reduction of loss aversion may be due to a decreasing effect of cognitive bias on decision making. Compared with others, people also disproportionately value things they have created themselves ( ), and may therefore be reluctant to discard data they perceive as having value. Reappraisal is one strategy that can be used to decrease loss aversion ( ,  ), and could improve decision-making by changing the perspective of discarding data from a waste of spent resource to a way of maximising ability to detect effects and improve data quality. The adoption of open research practices, such as the preregistration of data quality control procedures and acceptable thresholds could also decrease the risk of biases influencing decision-making, while at the same time reducing questionable research practices more generally ( ). However this has not yet been widely adopted in the neuroimaging community ( ). \n\nThere are several limitations in the protocol and software as presented here. Firstly, our finding that often only a single rater classified a dataset as \u201cuncertain\u201d suggests that the quality control protocol presented (which is published unedited from its pre-assessment state), lacked nuance for interpreting edge cases that would otherwise have been classified as either \u201cinclude\u201d or \u201cexclude.\u201d Given that pyfMRIqc was initially designed to aid decision-making about the quality of raw/minimally pre-processed fMRI data, we suggest that future users err on the side of caution with respect to marking datasets for exclusion, and first fully pre-process data using their pipeline of choice and then determine whether this had a positive impact and reduced data quality issues. Second, cinnqc, and by extension pyfMRIqc, do not formally quantify the success of the minimal pre-processing steps. When designing software for users with minimal programming experience, prioritising ease of use over functionality can reduce the freedom of more advanced users. For instance, brain extraction currently uses default arguments in FSL to identify brain and non-brain tissue ( ). This process can sometimes exclude brain voxels (particularly at the boundary of the brain), or include non-brain voxels in the brain extracted image. However, these issues can be ameliorated   via   optional arguments that change the default values, but this requires fine tuning on a per-subject basis, or the use of other software like HD-BET or ANTsX ( ;  ). A method for integrating these features would improve the computational reproducibility of the quality control procedure, as currently users would need to generate these files separately and use the cinnqc nomenclature to integrate output with the rest of the pipeline. A third limitation is that pyfMRIqc does not currently provide visualisation for distributions of \u201cno-reference\u201d image quality metrics. As previously mentioned, MRIQC currently crowdsources these values from users by default to generate robust distributions ( ). Though pyfMRIqc does not currently have the userbase to make this an effective method for identifying outliers at the population level, visualising the distribution of these values for at least the group level would help users to make more informed decisions about the quality of data they have in their sample. Future versions of pyfMRIqc would be improved by focusing on including these features in the software, and could potentially integrate reference values from the MRIQC Web-API for equivalent metrics in a similar way to how MRIQCEPTION  works. \n\nIn summary, we present a quality control protocol for pyfMRIqc ( ), implement it on data from the fMRI Open QC project (see text footnote 1), and assess its reliability using four independent raters. Data were classified by each rater as either \u201cinclude,\u201d \u201cuncertain,\u201d or \u201cexclude,\u201d based on the protocol and output generated by pyfMRIqc and cinnqc, which automated minimal pre-processing, data curation, and identification of deviated acquisition parameters in the dataset. Our results indicate that our reliability between raters was good for \u201cinclude\u201d and \u201cexclude\u201d decisions, with \u03ba values that ranged from moderate to substantial agreement. However, coefficients for the \u201cuncertain\u201d classification demonstrated little reliability between raters, and below chance level for all but one pair of raters. Furthermore, we found that in all but one cases where only one rater used the \u201cuncertain\u201d classification the other raters agreed with each other. We suggest that improvements in agreement between raters could be made by consulting sample-wide distributions of image quality metrics, increasing the clarity of the quality control protocol, and implementing further separate pre-processing steps before reassessing the data and deciding whether or not to exclude them. \n\n\n## Data availability statement \n  \nEach rater\u2019s quality control report can be found in   as  \u2013 , and at the University of Reading Research Data Archive  . pyfMRIqc and cinnqc output can be found on the cinnqc GitHub page  . \n\n\n## Ethics statement \n  \nEthical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. Written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements. \n\n\n## Author contributions \n  \nBW: conceptualization, methodology, software, formal analysis, investigation, data curation, writing\u2014original draft, review and editing, visualization, and project administration. NH, CM, and GR: methodology, analysis, and review and editing. AC: resources, supervision, and review and editing. All authors contributed to the article and approved the submitted version. \n\n \n", "metadata": {"pmcid": 9936142, "text_md5": "df5e09a5e441c7212a6c32e3ea3a8036", "field_positions": {"authors": [0, 117], "journal": [118, 132], "publication_year": [134, 138], "title": [149, 283], "keywords": [297, 371], "abstract": [384, 1836], "body": [1845, 39194]}, "batch": 2, "pmid": 36816136, "doi": "10.3389/fnins.2023.1070413", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9936142", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9936142"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9936142\">9936142</a>", "list_title": "PMC9936142  Inter-rater reliability of functional MRI data quality control assessments: A standardised protocol and practical guide using pyfMRIqc"}
