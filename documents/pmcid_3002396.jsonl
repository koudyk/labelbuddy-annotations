{"display_title": "PMID: 20858544 batch: 6", "list_title": "batch 6 PMID20858544 FMRI investigation of cross-modal interactions in beat perception: Audition primes vision, but not vice versa ", "metadata": {"batch_nb": 6, "journal": "NeuroImage", "origin_file": "documents_season_1.jsonl", "pmcid": 3002396, "pmid": 20858544, "publication_year": 2010, "text_md5": "a07256acce86a2d0f1b8aac81b77086a", "title": "FMRI investigation of cross-modal interactions in beat perception: audition primes vision, but not vice versa."}, "text": "PMID20858544        TITLE         FMRI investigation of cross-modal interactions in beat perception: Audition primes vision, but not vice versa        ABSTRACT           How we measure time and integrate temporal cues from different sensory modalities are fundamental questions in neuroscience. Sensitivity to a \u201cbeat\u201d (such as that routinely perceived in music) differs substantially between auditory and visual modalities. Here we examined beat sensitivity in each modality, and examined cross-modal influences, using functional magnetic resonance imaging (fMRI) to characterize brain activity during perception of auditory and visual rhythms. In separate fMRI sessions, participants listened to auditory sequences or watched visual sequences. The order of auditory and visual sequence presentation was counterbalanced so that cross-modal order effects could be investigated. Participants judged whether sequences werespeeding uporslowing down, and the pattern of tempo judgments was used to derive a measure of sensitivity to an implied beat. As expected, participants were less sensitive to an implied beat in visual sequences than in auditory sequences. However, visual sequences produced a stronger sense of beat when preceded by auditory sequences with identical temporal structure. Moreover, increases in brain activity were observed in the bilateral putamen for visual sequences preceded by auditory sequences when compared to visual sequences without prior auditory exposure. No such order-dependent differences (behavioral or neural) were found for the auditory sequences. The results provide further evidence for the role of the basal ganglia in internal generation of the beat and suggest that an internal auditory rhythm representation may be activated during visual rhythm perception.\u25baPerception of a beat occurs more easily with auditory than visual temporal sequences. \u25baPre-exposure to auditory sequences enhances visual beat perception. \u25baPre-exposure to visual sequences does not alter auditory beat perception. \u25baPutamen activity predicts strength of visual beat perception.           BODY            Introduction    Understanding of how brains keep time is central to understanding many aspects of perceptual, cognitive and motor function. In this regard, it is notable that although much perceptual information gleaned from the environment is   modality-specific   (i.e., exclusively conveyed through a single sense), temporal information, such as the duration of a stimulus or the temporal pattern of a series of stimuli, is   amodal   in nature (i.e., conveyed by more than one sense). For example, consider the periodic \u201cbeat\u201d produced by the repetitive striking of a bass drum; this beat can be heard, seen, or even felt depending on how close one is to the drummer. Thus, an important component of how brains keep time is the issue of how brains manage to successfully combine temporal cues from different sensory modalities to effectively guide behavior. One consideration that has influenced much research at the intersection of time perception and cross-modal processing is whether, despite the amodal nature of time, the auditory system may demonstrate a specialization (and priority) for processing temporal information over other sensory systems (e.g.,  ). The focus of this article is the neural basis of auditory specialization for one aspect of temporal processing\u2013the perception of a periodic \u201cbeat\u201d\u2013with special emphasis on possible cross-modal effects on visual temporal patterns following exposure to auditory patterns with identical temporal structure.    Broad behavioral support for auditory temporal processing dominance comes from a number of sources (  ). Of note, in the temporal ventriloquism effect, there is a tendency to perceive a single visual \u201cflash\u201d and a single auditory \u201cclick\u201d that are in close temporal proximity as simultaneous, with misperception of the visual stimulus in the direction of the auditory stimulus accounting for most of the shift (  ). Related research has observed an auditory driving effect whereby a repetitive sound (auditory flutter) presented simultaneously with a repetitive visual stimulus (visual flicker) causes the perceived visual flicker rate to shift toward the auditory flutter rate, even though the auditory flutter and visual flicker rates are easily distinguishable (  ). Most directly relevant for the present investigation are findings that auditory rhythms, such as the sound of a beating drum, are much more likely to lead to the perception of a periodic beat or pulse than visual rhythms\u2014even when the same temporal information is present (  ). Auditory rhythms have additionally been found to be better discriminated and remembered than visual rhythms (  ), and show asymmetric interference effects whereby participants have much more difficulty making judgments about visual rhythms in the presence of to-be-ignored auditory rhythms than they do making the same judgments about auditory rhythms in the presence of to-be-ignored visual rhythms (  ).    Despite well-documented auditory\u2013visual behavioral differences in a range of different timing tasks, the neural systems underpinning auditory and visual timing tend to show remarkable overlap (  ). Commonly activated brain areas for visual and auditory timing tasks include the premotor and supplementary motor cortices, basal ganglia, and cerebellum (  ) with much of the debate focused on the locus of a central \u201ctimekeeper\u201d or clock. A few studies have specifically examined the neural bases of beat perception, but relatively little is known about the specific neural mechanisms mediating modality differences, as most investigations of beat perception have tested only the auditory modality (  ). Moreover, those studies that have compared across modalities have focused on production, rather than perception (  ). In the perceptual domain, cerebellar and premotor structures respond strongly to auditory rhythms, regardless of whether beat perception is occurring (  ). The basal ganglia and supplementary motor area (SMA), however, respond more strongly during beat perception than during listening to irregular auditory sequences that lack a steady beat (  ). SMA activity is also higher in those individuals who behaviorally show greater levels of beat perception in ambiguous sequences (  ). Finally, the basal ganglia response is greatest when beat perception is internally generated (for example, when the beat is not heavily emphasized by external features of the auditory stimuli, but rather imposed onto the stimuli by the listener) (  ). Thus, the basal ganglia and SMA are important structures for beat perception, with the former playing a greater role in internal generation, although this is only known for the auditory modality.    In order to address this gap in knowledge, the present study compared beat perception in the auditory and visual modalities, combining fMRI with a recently developed behavioral paradigm that uses tempo (rate) judgments to assess beat sensitivity (  ). Stimuli were sequences of discrete elements (tones or flashes), with the timing of the sequence elements chosen to be within a range of rates (~\u00a0600\u00a0ms) known to yield reliable beat perception (  ).Of central interest was the pattern of   speeding up   versus   slowing down   judgments for ambiguous \u201ctest\u201d sequences where the perception of a beat was possible, but for which the beat was not explicitly marked and its perception was not required to perform the task (see  ). Based on previous research, we expected that observers would be less sensitive to an implied beat in visual sequences than in auditory sequences. Of key interest here, however, was how the order in which participants experienced each modality affected their beat perception. By counterbalancing modality order (either auditory first or visual first), we could examine cross-modal influences. We hypothesized that during the auditory-first condition, participants would create an internal representation that would increase beat perception during the subsequent visual condition. No similar internal representation would be available for those in the visual-first condition, as they would not yet have had exposure to the auditory sequences. Given the previous findings connecting the basal ganglia to beat perception, particularly during internal generation, we predicted greater basal ganglia activity for auditory sequences than for visual sequences, and, moreover, that the basal ganglia response would be greater for visual sequences experienced after the auditory condition than before. Conversely, no modality-dependent order effects on behavior or brain activity were expected for the auditory sequences.      Materials and methods     Design    In our design, listeners judged two types of auditory and visual sequences, termed \u201ccontrol\u201d and \u201ctest\u201d (shown in  ). Each of these sequences contained one of seven variable-length final intervals. This resulted in a 2 (modality: auditory, visual)\u00a0\u00d7\u00a02 (sequence type: control, test)\u00a0\u00d7\u00a02 (order: auditory first, visual first)\u00a0\u00d7\u00a07 (final interval: 600\u00a0ms \u00b1\u00a04%, \u00b1\u00a012%, \u00b1\u00a020%, \u221250%) mixed-factorial design. Modality, sequence type, and final interval were manipulated within subjects, while order was a between-subjects factor. A range of final intervals was used in order to construct psychometric curves and measure discrimination thresholds for each subject in each condition.      Participants    Twenty-seven right-handed neurologically normal volunteers (   n   \u00a0=\u00a011, male) between the ages of 18 and 48\u00a0years (   M   \u00a0=\u00a028\u00a0years,   SD   \u00a0=\u00a07) participated in exchange for a cash payment. Participants all self reported normal hearing and had a range of formal musical training (   M   \u00a0=\u00a04.3\u00a0years,   SD\u00a0=   \u00a04.2). Informed consent was obtained from all participants.      Stimuli    Auditory stimulus sequences were composed of 50-ms sine tones with a frequency of 440\u00a0Hz, and were generated by Audacity 1.2.6 for Windows (  ). Visual sequences were composed of black squares appearing for 50\u00a0ms on a white background. There were two types of sequences: control and test. For the control (4-element) sequences, the onsets of the elements marked out three time intervals with the following durations: 600\u00a0ms, 1200\u00a0ms, and a variable final duration of 600\u00a0ms\u00a0\u00b1\u00a04%, \u00b1\u00a012%, \u00b1\u00a020%, or \u221250%. The test (5-element) sequences were identical except that an additional element was inserted in the middle of the initial 600-ms interval to create two 300-ms intervals; see  . The key aspect of the test sequences is that in addition to the pair of 300-ms intervals marked by the first three elements of the sequence, the temporal structure of the sequence can convey to individuals the sense of a periodic beat at regular 600-ms interval intervals (  ). In the test sequences, the initial 600-ms inter-beat-interval is marked by the onsets of the first and third elements, which introduces a missing beat halfway through the 1200-ms gap so that the first onset of the final interval is \u2018on the beat\u2019. From this perspective, the final element of the test sequences is always \u2018early\u2019 or \u2018late\u2019 with respect to the expected beat. Test sequences could also be judged on the basis of the initial 300-ms intervals, in which case the final interval will generally be heard as slowing down (as most of the variable final intervals were longer than 300\u00a0ms). Therefore, trials where the final interval was 300\u00a0ms (600 minus 50%) were included so that individuals who generally heard or saw test trials with the other final intervals as   slowing down   could not prepare their response earlier than individuals who judged test trials as either   slowing down   or   speeding up   . Test and control sequences did not differ in total duration.      Procedure    In separate sessions, participants were presented with either auditory or visual sequences and judged whether at the end of the sequence, they felt the sequence was   speeding up   or   slowing down   . Responses were made using a button box positioned under the right hand; the middle finger was used to indicate the sequence was speeding up and the index finger was used to indicate the sequence was slowing down. We emphasized to participants that we were simply interested in their impressions of the sequences and that it was acceptable for them to respond that all of the sequences were   speeding up   , that all of the sequences were   slowing down   or that they were sometimes   speeding up   and other times   slowing down   . Participants were not shown a diagram of the task or told anything about the sequences other than the number of elements.    Prior to scanning, participants completed eight familiarization trials outside of the scanner. Each participant completed four scanning sessions (two auditory and two visual). Participants were randomly assigned to receive either the two visual sessions followed by the two auditory sessions (VA order), or vice versa (AV order). Within a session, participants completed 80 trials in total, and the sequence types were presented in random order. For each sequence type, there were 5 trials of each final interval except for the \u221250% interval, for which there were 10 trials. Participants were given 2.5\u00a0s to respond, which was followed by an inter-trial interval of 1\u00a0s. Nine \u201cnull\u201d events, 4.5\u00a0s long, were randomly interspersed in each session in order to resolve the hemodynamic response in analysis.      Image acquisition and preprocessing    Participants were scanned in a 3T Siemens Tim Trio using a head coil gradient set. To ensure participants were comfortable, foam pads were placed around the head and supported the legs. Auditory stimuli were presented over headphones; attenuation of scanner noise was achieved with insert earplugs rated to attenuate by ~\u00a030 dB (3\u00a0M 1100 earplugs, 3\u00a0M United Kingdom PLC, Bracknell, UK). The participants also wore ear defenders. When wearing earplugs and ear defenders, none of the participants reported difficulty in hearing the stimuli or focusing on the task. Visual stimuli were projected to a mirror positioned over the participant's head. Participants were instructed not to move any part of their body during the scan other than to respond. Button press responses were recorded with millisecond accuracy.    Functional data were collected using a gradient echo echo-planar imaging (EPI) sequence (36 slices, slice thickness 3\u00a0mm\u00a0+\u00a00.75\u00a0mm inter-slice gap, matrix size of 64\u00a0\u00d7\u00a064, in plane resolution 3\u00a0\u00d7\u00a03\u00a0mm, total FOV\u00a0=\u00a019.2\u00a0\u00d7\u00a019.2\u00a0cm, TE\u00a0=\u00a030\u00a0ms, TR\u00a0=\u00a02.19\u00a0s, flip angle\u00a0=\u00a078\u00b0). 260 volumes were collected in each session (total duration ~\u00a09.5\u00a0min). These EPI parameters enabled whole-brain coverage, including the cerebellum, for all participants. High-resolution 1\u00a0\u00d7\u00a01\u00a0\u00d7\u00a01\u00a0mm MPRAGE anatomical images were collected for anatomic localization and coregistration. SPM5 was used for preprocessing and analysis (SPM5; Wellcome Department of Cognitive Neurology, London, UK). Images were slice-timing corrected, then realigned spatially (to correct for motion) to the first image in the series, using a least squares approach with 6 rigid-body parameters, and trilinear interpolation. The MPRAGE image was segmented and normalized (using affine and smoothly nonlinear transformations) to a brain template in Montreal Neurological Institute (MNI) space. The resulting normalization parameters were applied to the coregistered EPIs and EPI images were smoothed with an 8\u00a0mm full-width half-maximum Gaussian kernel.      Data analyses     Behavioral data    For each participant, proportions of   speeding up   responses were calculated for each of the 12 trial types (2 sequence types X 6 final intervals) in the auditory and visual modalities. A signal detection model was then fit to these values to obtain two measures for each participant in each modality: a threshold estimate that assessed temporal discrimination ability and an index of beat sensitivity,   w   : see supplementary material for full model details. In the model, it is assumed that participants compare the final interval to one of two temporal referents: a 300-ms referent corresponding to the explicit time interval marked by the initial three elements of the test sequences and a 600-ms referent corresponding to the implied beat. Previously,  showed that individuals vary in the extent to which they rely on the 300-ms and 600-ms temporal referents when making   speeding up   /   slowing down   judgments about the test sequences in the auditory modality. The value of the beat sensitivity index   w   \u2208 [0, 1] indicates the relative weighting of the two temporal referents. Larger values of   w   indicate greater sensitivity to the implied 600-ms referent (beat) than smaller values of   w,   with a   w   of zero corresponding to complete reliance of tempo judgments on the explicit 300-ms referent. For the control sequences, values of   w   are expected to be close to 1.0 for most participants as the 600-ms temporal referent is explicitly marked by the initial two sequence elements (and the 300-ms referent is not present). For test sequences, however, both a 300-ms referent (explicit) and a 600-ms referent (implicit) could be used. Judgments made on the basis of the 600-ms referent with the test sequences demonstrate sensitivity to the implied beat. Here, our primary interest in this index is not in using it to assess individual differences, but rather to use it to examine potential modality differences in beat sensitivity. That is, do   w   -values differ between auditory and visual modalities?      FMRI data    Stimuli were modeled using a regressor made from an on-off boxcar convolved with a canonical hemodynamic response function. The length of the boxcar function corresponded to the time between the onset of the first stimulus in a trial and the offset of the last stimulus. Button presses were modeled with a delta function convolved with a canonical hemodynamic response function. EPI volumes with more than 4\u00a0mm movement in any plane were included as covariates of no interest to minimize movement artifacts. Low-frequency noise was removed with a 128\u00a0s high-pass filter. Results estimated from single-subject models were entered into second-level random effects analyses for standard SPM group inference (  ).    For each participant, contrast images were generated comparing test and control sequences in both modalities to the implicit baseline. These were then entered into a second-level random effects analysis using a mixed-measures analysis of variance with three factors: modality (auditory, visual), order of presentation (AV order, VA order), and sequence type (test, control). Specific contrasts of interest included: auditory condition versus visual condition, a conjunction of the auditory and visual conditions, test sequences versus control sequences, and interactions between order and modality (differences in the visual condition between the AV and VA orders, and differences in the auditory condition between the AV and VA orders).        Results     Behavioral results     Discrimination thresholds    Initial inspection of the behavioral data revealed four participants who showed atypical performance on control sequences with very poor tempo discrimination of visual control sequences only. The average relative just-noticeable difference in tempo (JNDs) for visual control sequences for these participants was 45.0%\u00a0\u00b1\u00a022.2%. Analyses of the behavioral data were run both with and without these participants in order to assess their potential contribution to the pattern of results. No substantive differences were found in the two sets of analyses. Nonetheless, to be conservative, the four participants with very poor visual tempo sensitivity were excluded from the analyses reported below to avoid exaggerating any modality differences in beat perception that would be due simply to some participants not being able to reliably detect changes in the visual tempo. Without these participants, relative JNDs in tempo were still modestly higher for visual sequences (   M\u00a0=   \u00a019.3%\u00a0\u00b1\u00a02.5%) than for auditory sequences (   M   \u00a0=\u00a010.1%\u00a0\u00b1\u00a00.7%),   t   (22)\u00a0=\u00a03.87,   p   \u00a0<\u00a00.01), but critically the remaining participants were clearly able to detect tempo changes in both modalities. Relative JNDs for auditory sequences were somewhat lower for the VA order (   M\u00a0=   \u00a09.15%\u00a0\u00b1\u00a00.41%) than for the AV order (   M\u00a0=   \u00a012.23%\u00a0\u00b1\u00a02.07%),   t   (21)\u00a0=\u00a02.10,   p   \u00a0<\u00a00.05; however, relative JNDs for visual control sequences were not different between orders (   M    VA   \u00a0   =   \u00a018.48%\u00a0\u00b1\u00a02.32%;   M    AV   \u00a0=\u00a021.16%\u00a0\u00b1\u00a06.37%),   t   (21)\u00a0=\u00a00.50,   p   \u00a0=\u00a00.63.      Beat sensitivity index, w    shows average values of   w   (beat sensitivity) for auditory and visual sequences (control and test) for the VA order (Panel A) and the AV order (Panel B). A 2 (modality)\u00a0\u00d7\u00a02 (sequence type)\u00a0\u00d7\u00a02 (order) mixed-measures ANOVA on   w   revealed significant main effects of sequence type,   F   (1, 21)\u00a0=\u00a080.1,   MSE\u00a0=   \u00a00.04,   p   \u00a0<\u00a00.001, \u03b72p\u00a0=\u00a00.79, and modality,   F   (1, 21)\u00a0=\u00a045.1,   MSE\u00a0=   \u00a00.04,   p   \u00a0<\u00a00.001,   \u03b7   2p\u00a0=\u00a00.68, a significant modality\u00a0\u00d7\u00a0sequence type interaction,   F   (1, 21)\u00a0=\u00a037.3,   MSE\u00a0=   \u00a00.02,   p   \u00a0<\u00a00.001, \u03b72p\u00a0=\u00a00.64, and a significant three-way modality\u00a0\u00d7\u00a0sequence type\u00a0\u00d7\u00a0order interaction,   F   (1, 21)\u00a0=\u00a010.5,   MSE\u00a0=   \u00a00.03,   p   \u00a0<\u00a00.01, \u03b72p\u00a0=\u00a00.33. Values of   w   for control sequences were predicted to be close to 1 since a 600-ms temporal referent is explicitly marked. As expected, values of   w   were closer to 1 for control sequences (   M   \u00a0=\u00a00.85\u00a0\u00b1\u00a00.03) than for test sequences (   M   \u00a0=\u00a00.40\u00a0\u00b1\u00a00.04), indicating that tempo judgments were more likely to be based on the 600-ms referent. With respect to modality, beat sensitivity was much lower for visual sequences (   M   \u00a0=\u00a00.47\u00a0\u00b1\u00a00.04) than for auditory sequences (   M   \u00a0=\u00a00.78\u00a0\u00b1\u00a00.03), especially for the test sequences where the 600-ms referent was only implied by the temporal structure of the sequence. However, effects of modality were also influenced by the presentation order of the auditory and visual sequences. In general, participants picked up on the implied beat of the visual test sequences more when they had prior auditory exposure (AV order,   M   \u00a0=\u00a00.26\u00a0\u00b1\u00a00.07) compared to no prior auditory exposure (VA order,   M   \u00a0=\u00a00.05\u00a0\u00b1\u00a00.05),   t   (21)\u00a0=\u00a02.2,   p   \u00a0<\u00a00.05. However, prior visual exposure did not affect sensitivity to the implied beat of the auditory test sequences (AV order,   M   \u00a0=\u00a00.61\u00a0\u00b1\u00a00.09; VA order,   M   \u00a0=\u00a00.68\u00a0\u00b1\u00a00.06),   t   (21)\u00a0=\u00a0\u22120.6,   p   \u00a0=\u00a00.57. For the control sequences, in which the beat is explicitly marked, beat sensitivity was high across modalities in both groups of participants (see  ).    Finally, we considered potential correlations between discrimination thresholds, years of musical training, and the beat sensitivity index,   w   . Correlational analyses revealed that there was no relationship between JNDs and sensitivity to the implied beat of the test sequences for either the auditory sequences,   r   (21)\u00a0=\u00a0\u22120.17,   p   \u00a0=\u00a00.45 or the visual sequences,   r   (21)\u00a0=\u00a0\u22120.26,   p   \u00a0=\u00a00.26. Moreover, years of musical training was not correlated with auditory,   r   (21)\u00a0=\u00a0\u2212\u00a00.03,   p   \u00a0=\u00a00.91, or visual JNDs,   r   (21)\u00a0=\u00a00.02,   p   \u00a0=\u00a00.95. Musical training was also not correlated with beat sensitivity for either auditory,   r   (21)\u00a0=\u00a00.12,   p   \u00a0=\u00a00.60, or visual test sequences,   r   (21)\u00a0=\u00a0\u22120.34,   p   \u00a0=\u00a00.11.       fMRI results     Effects of modality    The contrast of auditory stimuli (test and control sequences) minus visual stimuli activated several areas, including bilateral superior temporal gyri, basal ganglia, hippocampi, cuneus, right inferior frontal cortex, and insula at a whole-brain corrected (pFDR\u00a0<\u00a00.05) level of significance. Visual stimuli (test and control sequences) minus auditory stimuli activated bilateral occipital cortex, fusiform gyri, cerebellum, superior parietal cortex, and right inferior frontal operculum at a whole-brain corrected (pFDR\u00a0<\u00a00.05) level of significance (maxima are reported in  ).      Effects of task    The auditory\u2013rest and visual\u2013rest contrasts were subjected to a conjunction analysis to find areas significantly activated in both modalities (pFDR\u00a0<\u00a00.05, conjunction null hypothesis). This analysis showed significant activity in the bilateral inferior frontal cortex (BA 44), SMA, premotor cortex, superior temporal gyri, supramarginal gyri (BA 40), putamen, cerebellum, and thalamus (  and  ).      Effects of sequence type    Overall, an F contrast of control versus test sequences showed no areas with significant activity differences. Conducting separate F contrasts for each modality still showed no areas with significant activity differences between test and control sequences.      Effects of order    The AV order group showed significantly greater activity in the inferior frontal cortex (   t   \u00a0=\u00a05.85, pFDR\u00a0<\u00a00.01;   x   \u00a0=\u00a060,   y   \u00a0=\u00a015,   z   \u00a0=\u00a09, BA 44/45) than the VA order group, across all conditions. No areas were significantly more activated in the VA order group compared to the AV order.      Interactions    There were no areas exhibiting significant interactions between sequence type (test, control) and order (AV, VA), or between sequence type and modality (auditory, visual). However, several areas did show a significant interaction between modality and order, with larger clusters in the bilateral basal ganglia, and smaller areas of activity in the right anterior insula, left SMA, right inferior temporal gyrus, and left inferior occipital gyrus (see  for maxima). To determine the nature of the interaction,   t   -tests were performed. For each   t   -test, whole-brain corrected results were inclusively masked by areas significantly activated in the interaction contrast at an uncorrected   p   value of   p   \u00a0<\u00a00.001. The masking conservatively restricts the   t   -test analysis to brain areas in which an interaction also exists. The   t   -test results showed that activation in the visual condition was influenced by whether it preceded or followed the auditory condition. Specifically, in the visual condition, significantly greater activity was observed for the AV order relative to the VA order (AV visual condition\u2013VA visual condition) in the bilateral putamen, right middle frontal gyrus (BA 46), right inferior temporal gyrus (BA 37), left inferior occipital gyrus (BA 19), and right cerebellum (lobule VIII) (see  and  ). The bilateral putamen clusters were of sufficient size to confidently rule out the possibility of false positives, however, the other activation clusters were small (1\u20136 voxels), and therefore could not be confidently ruled out as false positives (as each cluster was less than 5% of the total number of activated voxels in the contrast, and a pFDR\u00a0<\u00a00.05 threshold was used). Mean signal intensity was extracted from each activated region to further illustrate the nature of the significant activity differences between modality and order (  ). Again, no significant visual condition activity increases were observed for the VA order\u2013the AV order (VA visual condition\u2013AV visual condition). Moreover, when activity in the auditory condition for the AV order was compared to the VA order, no significant differences were found (i.e., the neural response was not significantly different in the auditory condition when it was completed before the visual condition versus after the visual condition).    To determine whether visual beat sensitivity was directly related to activity increases in the putamen and the right cerebellum during the visual sessions, regression analyses were performed using putamen and cerebellar activity to predict   w   values for visual test sequences. Activity in the left and right putamen was highly correlated (   r   (27)\u00a0=\u00a00.86,   p   \u00a0<\u00a00.001) so activity across both putamen regions was averaged together to create a single regressor. This, and average activity in the right cerebellar region, was entered into a multiple regression analysis predicting visual test   w   . Putamen activity was found to significantly predict   w   values,   \u03b2\u00a0=   \u00a00.46,   t(27)\u00a0=\u00a02.59, p\u00a0<   \u00a00.016, but right cerebellar activity did not   \u03b2   \u00a0=\u00a00.17,   t(27)   \u00a0=\u00a00.91   , p\u00a0=   \u00a00.37   .   A scatterplot showing the relationship between putamen activity and   w   value is shown in  . One caveat is that the activated right cerebellar region was only composed of 4 voxels, so the estimate of average activity was likely noisier than the average across the larger putamen clusters.      Effects of time in scanner    Because the visual condition was experienced earlier in the scanning experiment by the VA order group, and later in the experiment for the AV order group, the potential for a confound with time in the scanner exists. This possibility was explored with additional analyses to determine if the visual condition differences between the groups were attributable to the effects of time rather than order. A time confound is unlikely to be the explanation, as the two groups   also   experienced the auditory condition at different times, and therefore differences arising solely from time in the scanner would be expected between groups in the auditory condition. Our previous analyses of the auditory VA order\u2013auditory AV order contrast did not find any significant differences between orders in the auditory condition. However, subthreshold differences could exist, so mean signal intensity was examined in each activated region for the two groups. If the simple passage of time accounts for the observed activity increases in these areas, then the VA order group should show greater activity during the auditory condition (which occurred second) than the AV order group in these areas. This was not the case (auditory activity in each area for each group is shown in  ). A final analysis to rule out time confounds exploited the fact that there were two sessions in each modality, which enabled the effects of time to be broken down within each modality. If any observed differences in visual condition activity are due to time in the scanner rather than order, they should be stronger for the second visual session (which occurs later) compared to the first visual session. As shown in Supplementary Figure\u00a01, the activity shows the opposite pattern: the VA order visual condition activity differences are larger in the first visual session than the second visual session, indicating the effects are not the result of increased time in the scanner, and are most parsimoniously explained by the recent exposure to the auditory condition.      Correlations with JND    To determine how brain areas that were sensitive to tempo compared to brain areas that were sensitive to the beat, we also conducted whole-brain analyses using JND scores as regressors. Visual JND scores were used as a regressor for the visual 4- and 5-element fMRI contrast images, and auditory JND scores were used as a regressor for the auditory 4- and 5-element fMRI contrast images. No areas that significantly correlated with JND score were found in either modality, even at a very liberal statistical threshold (pFDR\u00a0<\u00a00.40).        Discussion    The current study investigated modality differences in beat perception by combining fMRI with a tempo (rate) judgment paradigm that permitted assessment of both temporal discrimination thresholds and beat sensitivity. We consider the behavioral findings first.     Behavioral results    Tempo discrimination thresholds were slightly lower in the auditory than the visual modality, indicating greater sensitivity to tempo changes in auditory rhythms. This is consistent with previous behavioral studies that have also found superior auditory (versus visual) performance in temporal interval discrimination (  ), temporal sequence reproduction and discrimination (  ), and synchronization tasks (  ).    Tempo judgments in the current study also revealed substantially greater beat sensitivity for auditory than for visual sequences, despite the identical temporal structure of stimulus sequences in both modalities. This finding fits both with intuitive notions that beat perception occurs more readily for listening than for watching, and empirical work indicating the same (  ). A key behavioral finding was that auditory beat sensitivity was unchanged by watching visual sequences, but visual beat sensitivity was significantly higher after listening to auditory sequences. One possible explanation for this is that listening to the auditory sequences made the subsequent visual tempo discrimination easier, which somehow enabled greater beat sensitivity for visual rhythms. However, we found no evidence for this: the tempo discrimination thresholds in the visual modality did not differ between the VA and AV groups, indicating that overall sensitivity to tempo changes in the visual modality was unchanged by prior auditory exposure. In addition, temporal discrimination thresholds did not correlate with beat sensitivity, suggesting that general timing abilities are independent from sensitivity to an implied beat. Instead, we suggest that exposure to auditory rhythms primes an internal representation of a beat, which can then be exploited during visual performance to promote visual beat perception. Our fMRI results, discussed below, support this account.    Although other studies have shown that audition exerts greater influence over vision during temporal processing than vice versa, these studies have mainly considered concurrent, rather than sequential, stimulus presentations (  ). Our results indicate that exposure to auditory stimuli can impact visual perception even when the different modalities are separated by up to several minutes. The persistence of the auditory influence across time suggests a mechanism that is internally mediated, rather than stimulus-driven. An interesting question that remains to be addressed is how much auditory exposure is needed to induce the visual beat perception. The current design always used two auditory modality sessions (5\u00a0min each) before testing the visual modality. However, as beat perception with auditory stimuli can occur very rapidly, it is possible that exposure to just a few auditory sequences would be enough to generate an internal representation that can subsequently induce visual beat perception.    The current behavioral results fit within the broader literature related to modality differences in timing. It is generally the case that the auditory modality has an advantage when it comes to processing temporal information. The temporal ventriloquism and auditory driving phenomena are familiar examples of the ability of auditory temporal information to alter perception in the visual modality (  ). One explanation for the auditory advantage over visual timing is a cost associated with transforming visual temporal information into an auditory code (  ).  tested this hypothesis using an interference paradigm in which participants made same-different judgments about pairs of rhythms that were presented alone or concurrently with a rhythm in the other modality. They found that same-different judgments for visual rhythm pairs were worse when presented concurrently with incongruent than congruent auditory rhythms (indicating that auditory information influences visual timing). Task-irrelevant visual information also reduced accuracy, but the decrement was larger for irrelevant auditory information (indicating that auditory information was harder to ignore). Finally, task-irrelevant incongruent auditory information was more disruptive when presented concurrent with the first visual rhythm in a pair than the second rhythm (indicating that the encoding stage was more vulnerable than the comparison stage).  concluded that temporal structure is automatically and obligatorily abstracted from visual rhythms and represented using an auditory code, such that even irrelevant auditory information is disruptive to visual temporal encoding. Contrary to the conclusion reached by Guttman et al., our findings suggest that auditory encoding of visual sequences is neither obligatory nor automatic, as visual test sequences showed very different patterns of tempo judgments from auditory sequences; see also  . However, the auditory recoding of visual sequences can be facilitated by exposure to auditory versions of the rhythms; tempo judgments for visual test sequences more closely resembled auditory judgments only when the auditory condition had been experienced first.      fMRI results     Activation differences and similarities between modalities    We observed an expected modality difference in brain areas activated by the auditory and visual tasks. fMRI showed greater activity for auditory than visual sequences in bilateral superior temporal gyri, insula, basal ganglia, and cuneus. Activity in the superior temporal gyri is certainly expected in auditory tasks, and the insula is also routinely associated with auditory processing (  ), particularly during discrimination of auditory temporal sequences and perception of rhythm (  ). Especially relevant for the current study,  previously showed that the left insula was more active for individuals with strong beat perception (high   w   -values) than for individuals with weak beat perception (low   w   -values) in auditory stimuli identical to those used in the current study. As   w   -values were higher for the auditory than the visual modality, insula activation may reflect greater overall beat perception. Higher overall levels of beat perception would also be expected to activate the basal ganglia, as previous studies have shown greater basal ganglia activity during beat perception (  ), and that basal ganglia dysfunction also leads to poorer discrimination of rhythms with a regular beat (  ).    Cuneus activation for auditory rhythms is less expected; however, the cuneus is often less active during task performance than during rest (  ), and the activity for this region was baseline > auditory > visual, suggesting that the auditory condition was easier for participants, resulting in less task-induced deactivation. This interpretation is supported by the somewhat lower thresholds for the auditory than visual judgments.    Greater visual than auditory activity was found in bilateral occipital cortex, fusiform gyri, superior parietal cortex, and cerebellum. For the visual modality, the activation foci in occipital, parietal, and right inferior frontal areas closely match those observed by  for tapping to a visual pacing signal compared to an auditory one. The authors suggested that these areas may be involved in visuomotor transformation processes, since a tapping response was required throughout the task. In the current study no production was required, suggesting a more general role in visual timing processes shared by perception and production. The cerebellum is implicated in timing and rhythm perception/production by both neuroimaging and neuropsychological work (  ), and the cerebellum has been argued to be a crucial component of timing in the milliseconds to seconds range (  ), the same range that characterized our stimuli. However, counter to the view that the cerebellum comprises a supramodal timer (  ), we observed greater activation for the visual task than the auditory task in this area. Given higher thresholds in the visual task relative to the auditory task, this finding is consistent with an imaging study showing that cerebellum activity increases with increasing difficulty in timing tasks (  ).    We observed activations common to both modalities in bilateral inferior frontal cortex (BA 44), SMA, premotor cortex, posterior superior temporal gyri, supramarginal gyri (BA 40), putamen, and cerebellum. The overlap between modalities in these areas is in line with the previous research of  , which compared auditory and visual rhythm perception and found a similar network of brain areas activated across modalities. Our findings are also consistent with those of  , who examined influences of modality on rhythm production (rather than perception). In that study, auditory or visual rhythms were learned prior to scanning. During scanning, an auditory or a visual pacing signal cued participants to reproduce a learned rhythm. Reproduction of rhythms that were both trained and paced in the visual modality activated essentially the same areas as the reproduction of auditory rhythms (including SMA, premotor cortex, frontal operculum, posterior superior temporal gyri, cerebellum, and basal ganglia). One difference is that activations in our study and Schubotz et al. are generally more bilateral, perhaps because our tasks involved perception rather than reproduction (  ).      Auditory exposure influences on visual condition activity    A novel contribution of the current work concerns activation for visual sequences with and without prior auditory exposure (AV versus VA order). For the visual condition, there were robust increases for the AV order in bilateral putamen, as well as more circumscribed and less robust activations in right cerebellum, middle frontal gyrus, inferior temporal gyrus, and left inferior occipital gyrus. These activation changes accompanied a behavioral increase in visual beat sensitivity. In particular, as shown in  , the activity in putamen and right cerebellum closely paralleled the order effect observed for the behavioral index of beat perception. That is, these areas showed significant activity during the auditory condition (compared to rest), regardless of order; similarly, behavioral data showed high beat sensitivity in both auditory conditions regardless of order. In the visual condition (compared to rest), only the AV order group showed activity in these regions; similarly, behavioral data revealed increased beat perception during the visual condition in the AV order only. To further explore the relationship between activation and behavior, a regression analysis predicting   w   values for visual test sequences using putamen and cerebellum activation was conducted. Putamen, but not cerebellar, activity predicted the beat sensitivity in visual test sequences. Thus, individual increases in beat sensitivity in the AV group are directly related to the activity levels in the putamen (but not in the cerebellum). Critically, we did not observe any areas of significantly increased activation during the visual condition for the VA order relative to the AV order.    One potential explanation for the observed modality by order interaction that needs to be considered is time in the scanner; the visual session was experienced later for the AV group than the VA group, meaning that changes in activation could be due to practice effects, learning, fatigue, etc., i.e., effects that are confounded with time. Although the AV group experienced the visual sessions later than the VA group, several analyses indicated that the areas described above did not correlate with time spent in the scanner. First, as shown in Supplementary Figure\u00a01, activity in these areas tended to decrease over time, whereas   increases   were observed in the visual condition for the AV group compared to the VA group. Second, there were no differences in auditory condition activity between the VA and AV groups in these brain areas (shown in  ), even though the auditory condition in the VA group was experienced later than the AV group. Therefore, the interaction effects in the cerebellum and basal ganglia are due to the order in which the participants experienced the auditory and visual modalities, and do not show linear trends for increasing activity related to time in the scanner or doing the task. A related consideration is whether the observed order effects are specific to having performed that task in the auditory domain or would also have occurred if the participants performed the visual task for more time. The activation patterns for the two sessions of the visual task suggest that the order effects are specific to auditory influences, as activation decreased from the first to the second visual session. However, future research should investigate whether sensitivity to a beat in visual rhythms, and corresponding brain activation, can be increased by prolonged exposure to visual rhythms with an implied beat.    Activity in the putamen directly predicted increases in   w   values for the visual session after auditory exposure. The putamen is part of the striatum, which is known to play a crucial role in timing more generally. For example, impaired timing is found in disorders that affect the basal ganglia, such as Parkinson's disease (  ) and Huntington's disease (  ). Data from the neurophysiological domain also implicates the striatum, particularly oscillations in thalamo-cortico-striatal loops (  ). These findings have led to the formation of an influential model (the striatal beat-frequency model), which suggests that medium spiny neurons in the basal ganglia act as coincidence detectors for cortical neural oscillators (  ). The coincidence detection of activity in different neural populations is suggested to underpin a variety of cognitive functions, including timing. Although the striatal beat frequency model has been proposed mainly for timing in the seconds to minutes range, oscillator approaches in the sub-second range have also proved to be a fruitful approach to modeling aspects of time and rhythm perception (  ).    Empirically, a role for the basal ganglia in beat-based timing has already been shown in the auditory modality with fMRI and with Parkinson's disease patients (  ). Here we show that the basal ganglia may even contribute to beat perception in the visual modality, particularly when beat perception is promoted by previous auditory exposure. Although a beat is not readily induced in the visual modality (  ), we propose that when an internal representation of a beat is available because of prior auditory experience, this representation can be exploited to support beat perception in the visual modality. This view is consistent with the finding that greater basal ganglia activity is observed when internal generation of a beat is required (  ). One way that increased visual beat perception could be accomplished is by using an internal beat representation to facilitate auditory imagery: after experiencing the auditory condition, participants can imagine the auditory tones occurring during the visual presentation of the sequence. We did not instruct participants to use auditory imagery, but some participants did report imagining sounds during the visual condition. This possibility is further supported by evidence that the basal ganglia are also involved in imagery (  ). Another potential mechanism is that an amodal beat representation, for example a motor representation, is created when listening to the auditory stimuli and can subsequently be accessed during either auditory or visual stimulus presentation. One way to evaluate these two possibilities would be to examine order effects for beat perception based on tactile exposure. Behaviorally, a beat is perceivable in the tactile modality (  ), and the basal ganglia have been shown to be active during temporal processing of somatosensory temporal stimuli (  ). This suggests that a tactile\u2013visual exposure order should produce similar effects to the auditory\u2013visual order. If the effects did occur for tactile\u2013visual ordering, then an auditory imagery account of the current findings becomes less likely, and an explanation based on an amodal representation (or transformation into a motor representation) would be more fitting.    Our regression analysis did not find any significant predictive value of cerebellar activity for visual beat sensitivity. Cerebellum activation is often reported for timing tasks (  ), but has not been shown to be specifically activated during beat perception in fMRI (  ). In addition, no evidence for a cerebellar role in beat perception has been found in neuropsychological patient work. Specifically, patients with cerebellar ataxia show no deficits in timing of rhythmic sequences based on a regular beat, but are significantly impaired only on timing tasks explicitly designed   not   to be based on a beat (e.g., comparing durations of two intervals) (  ). Taken together, these findings suggest that the increased activation during visual sequences after auditory exposure seems unlikely to be related to beat perception. Several reports implicate the cerebellum in acquisition of sensory information and generation of motor output (  ), indicating a role for the cerebellum in sensorimotor transformation. It has been suggested that auditory rhythm information is automatically converted to a motor code (  ); increased cerebellum activation during visual sessions following auditory exposure may indicate that prior auditory exposure promotes visuo-motor transformation.    The only previous modality order effect of which we are aware was a deactivation of the left angular gyrus following auditorily\u2013but not visually\u2013paced rhythm reproduction (  ). We found no modulation of angular gyrus activity, but our design blocked modality within sessions (as opposed to intermixing of auditory and visual stimuli, cf.  ), obscuring any transient effects occurring immediately after the modality switch. Though  counterbalanced the order of presentation of auditory and visual sequences across participants, they did not report analyses of order-related differences.        Conclusions    Prior auditory experience increases the likelihood of perceiving an implied beat in visual sequences. Moreover, an increase in visual beat perception is associated with an increase in basal ganglia activity. These results provide further evidence for a basal ganglia role in the internal generation of a beat and suggest that hearing an auditory rhythm creates an internal representation that encourages an auditory recoding of a subsequently presented visual rhythm.      Appendix A.\u2003Supplementary data", "utf8_text_md5_checksum": "a07256acce86a2d0f1b8aac81b77086a"}
