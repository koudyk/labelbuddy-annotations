{"text": "Lee, Juhyeon and Lee, Jong\u2010Hwan\nHum Brain Mapp, 2023\n\n# Title\n\nDiscovering individual fingerprints in resting\u2010state functional connectivity using deep neural networks\n\n# Keywords\n\nDeep neural networks\nFingerprints\nFunctional connectivity\nFunctional magnetic resonance imaging\nHuman Connectome Project\nIndividual identification\nTransfer Learning\n\n\n# Abstract\n \nNon\u2010negligible idiosyncrasy due to interindividual differences is an ongoing issue in resting\u2010state functional MRI (rfMRI) analysis. We show that a deep neural network (DNN) can be employed for individual identification by learning important features from the time\u2010varying functional connectivity (FC) of rfMRI in the Human Connectome Project. We employed the trained DNN to identify individuals from an independent dataset acquired at our institution. The results revealed that the DNN could successfully identify 300 individuals with an error rate of 2.9% using 15\u2009s time\u2010window and 870 individuals with an error rate of 6.7%. A trained DNN with nonlinear hidden layers led to the proposal of the \u201cfingerprint of FC\u201d (fpFC) as representative edges of individual FC. The fpFCs for individuals exhibited commonly important and individual\u2010specific edges across time\u2010window lengths (from 5\u2009min to 15\u2009s). Furthermore, the utility of our model for another group of subjects was validated, supporting the feasibility of our technique in the context of transfer learning. In conclusion, our study offers an insight into the discovery of the intrinsic mode of the human brain using whole\u2010brain resting\u2010state FC and DNNs. \n  \nBy using deep neural networks (DNNs), reliable and robust fingerprints of resting\u2010state functional connectivity for individuals were found. The trained DNN showed its efficacy in the identification of individuals from an independent dataset via transfer learning.   \n \n\n# Body\n \n   Practitioner Points   \n  \nDeep neural network (DNN) identified individuals using resting\u2010state functional MRI (rfMRI). \n  \nThe fingerprint of functional connectivity (fpFC) for an individual was obtained from the trained DNN. \n  \nImportant edges in the fpFC were shared across individuals for varying time\u2010window lengths in rfMRI. \n  \nThe number of important edges in the fpFC showed a gradual change across the time\u2010window lengths. \n  \nThe trained DNN was applied to the identification of individuals from an independent dataset. \n  \n \n\n## INTRODUCTION \n  \nFunctional connectivity (FC) from the functional MRI (fMRI) modality has been widely used to investigate positively or negatively synchronized neural activations across the whole brain (Biswal et al.,\u00a0 ; Fox et al.,\u00a0 ; van den Heuvel & Hulshoff Pol,\u00a0 ). Many studies have consistently reported the existence of various functional networks (FNs) commonly found across subjects (Noble et al.,\u00a0 ). However, despite these similarities, there remains significant heterogeneity across individuals in terms of FC (Finn et al.,\u00a0 ; Gordon, Laumann, Adeyemo, & Petersen,\u00a0 ; Gordon, Laumann, Gilmore, et al.,\u00a0 ; Gratton et al.,\u00a0 ; Vanderwal et al.,\u00a0 ). Resting\u2010state fMRI (rfMRI) is dominated by spontaneous activity without the involvement of a particular stimulus or task. Because resting\u2010state FC is thought to characterize intrinsic functions (Biswal et al.,\u00a0 ), it has been frequently employed in various studies. For example, previous reports have identified individuals using their FC information obtained from rfMRI (Cai et al.,\u00a0 ; Chen & Hu,\u00a0 ; Demeter et al.,\u00a0 ; Finn et al.,\u00a0 ; Finn et al.,\u00a0 ; Horien et al.,\u00a0 ; Pallar\u00e9s et al.,\u00a0 ; Sarar et al.,\u00a0 ). In this context, there have been attempts to uncover the distinguishing interindividual differences of FCs. In previous works, the representative profile of an individual has often been termed \u201cfingerprint\u201d (Amico & Go\u00f1i,\u00a0 ; Cai et al.,\u00a0 ; Demeter et al.,\u00a0 ; Finn et al.,\u00a0 ; Liu et al.,\u00a0 ; Van De Ville et al.,\u00a0 ). \n\nTime\u2010varying FC (tvFC), also known as dynamic FC, has been conventionally estimated using one of two approaches: (a) calculating the changes in FC across regions directly using the observed BOLD signals and (b) modeling underlying neural dynamics such as by using neural mass and neural field models (Lurie et al.,\u00a0 ). The ordinary sliding window approach refers to calculating a series of FC patterns using a fixed time window of seconds (s) or minutes (min) at shifting time points (Hindriks et al.,\u00a0 ; Hutchison et al.,\u00a0 ). There are alternative approaches to the sliding window approach with a fixed window length, such as adaptive windowing, Hidden Markov models, co\u2010activation pattern analysis, and clustering (Yaesoubi et al.,\u00a0 ). Numerous studies have employed static FC to extract a single FC pattern from time series data spanning the entire scan duration. However, other studies have explored the application of brief window lengths to extract a series of tvFC patterns, revealing distinct temporal dynamics unique to each individual (Fong et al.,\u00a0 ; Preti et al.,\u00a0 ; Savva et al.,\u00a0 ). In the present study, we examined whether the tvFC utilizing short window lengths could exhibit reliable individual\u2010specific information, comparable to the expectations of static FCs derived from extended window lengths, while concurrently accommodating the temporal variation inherent in tvFC patterns. A very short window is limited in producing reliable information due to the lower signal\u2010to\u2010noise ratio arising from confounding factors such as physiological artifacts and scanner hardware noise disrupting the measurement of the synchrony of FNs within the brain (Murphy et al.,\u00a0 ; Power et al.,\u00a0 ). The use of tvFC with a short window can thus be challenging, but it remains potentially useful. Therefore, the development of computational models that can extract robust FC features from noisy tvFC patterns is required to enhance the utility of tvFC. Although previous studies have investigated the effects of various time window lengths on tvFC, consensus on the optimal window length for significant findings remains elusive (Gonzalez\u2010Castillo et al.,\u00a0 ; Preti et al.,\u00a0 ). This lack of definitive evidence prompted our investigation into the feasibility of individual identification across various window lengths, from less than a minute to several minutes. Prior attempts to distinguish individuals using FC features faced limitations, with error rates of less than 1% being achieved using static FC or a confined subject pool (Cai et al.,\u00a0 ; Demeter et al.,\u00a0 ). The pursuit of minimal error rates in the identification of a large cohort using tvFC patterns computed from brief time\u2010windows has presented substantial challenges (Chen & Hu,\u00a0 ; Sarar et al.,\u00a0 ; Van De Ville et al.,\u00a0 ). To the best of our knowledge, no study has yet been able to identify hundreds of participants with high accuracy using time\u2010windows under 1\u2009min, nor delineated common or unique edges across various window lengths. \n\nDeep neural network (DNN) models have been widely employed in classification tasks because of their superior performance and ability to organize hierarchical feature representations (Abrol et al.,\u00a0 ; Bengio et al.,\u00a0 ; Plis et al.,\u00a0 ). Despite their efficacy, however, overfitting has been a long\u2010standing issue for DNNs due to the large number of tunable weight parameters, which can easily compromise generalization performance (Baum & Haussler,\u00a0 ; MacKay,\u00a0 ; Neal,\u00a0 ; Schmidhuber,\u00a0 ). To prevent overfitting and to enhance the generalization performance of a DNN, a regularization penalty using weight parameters is frequently added to the cost function of the DNN, such as an L1\u2010norm term (e.g., LASSO), an L2\u2010norm term (e.g., ridge regression), or both (e.g., elastic net) (Zou & Hastie,\u00a0 ). An explicit weight sparsity control scheme for a DNN combines the advantages of both L1 (which helps to select essential features from input patterns) and L2 regularization (which helps to stabilize and collect important features) (Kim et al.,\u00a0 ). DNN models with explicit weight sparsity control have exhibited superior performance for both classification and regression problems due to their excellent hierarchical feature extraction capability, which is crucial for high\u2010dimensional and noisy whole\u2010brain fMRI data (Jang et al.,\u00a0 ; Kim et al.,\u00a0 ; Kim, Bandettini, & Lee,\u00a0 ; Kim & Lee,\u00a0 ). Recent efforts have been directed at extracting individual\u2010specific information from FC patterns using DNN (Cai et al.,\u00a0 ; Chen & Hu,\u00a0 ; Sarar et al.,\u00a0 ) and machine learning (ML) approaches (Vergara et al.,\u00a0 ). Both DNN and ML employ model\u2010based approaches capable of identifying crucial features for the model's objective through learning from the input. However, DNNs not only demonstrate superior feature extraction capabilities but also facilitate the derivation of hidden representations through the input\u2013output mapping process, which promotes transfer learning. This capability extends beyond the more limited scope of shallow ML models that are confined to learning decision boundary approaches (Abrol et al.,\u00a0 ). \n\nIn the present study, we investigate the possibility of individual identification across different days using a weight\u2010sparsity\u2010controlled DNN as the computational model and tvFC as the input. From the trained DNN model, we report representative FC edges from the tvFC patterns of the human brain and refer to these as the fingerprint of FC (fpFC). The performance of the DNN is systematically evaluated using rfMRI data available from the Human Connectome Project (HCP) and compared with that of alternative linear classifiers. We also investigate the feasibility of using the trained DNN to classify individuals using rfMRI data from our institute. We hypothesize that (1) DNN identification performance will be superior to that of shallow ML models, (2) individuals have a distinct tvFC fingerprint during a resting state even for shorter window lengths, and (3) important FC edges for individual identification are robust across various window lengths despite the presence of a moderate level of variability. This study presents that the DNN model can distinguish hundreds of individuals, even when FC is measured for a very short period of time (only a fraction of a minute). Moreover, the trained model is capable of providing a representative fingerprint\u2010like feature for each individual (i.e., fpFC). These fpFCs could serve as individual \u201cneuromarkers\u201d (Finn et al.,\u00a0 ), offering the potential to associate with and predict behavioral phenotypes and/or dysfunction toward the development of prognosis prediction and personalized treatment options (Kong et al.,\u00a0 ; Van De Ville et al.,\u00a0 ). \n\n\n## MATERIALS AND METHODS \n  \n### Overview \n  \nFigure\u00a0  presents a schematic diagram of our DNN model for individual identification from rfMRI data of the HCP dataset (Van Essen et al.,\u00a0 ). The identity of an individual was predicted in the output layer based on the input tvFC patterns. The trained DNN was validated and tested using tvFC patterns acquired on another day. \n  \nIndividual identification using resting\u2010state tvFC and a DNN. (a) DNN classifier to identify individuals. The number of input dimensions for the DNN is 62,835 (i.e., FC edges from paired ROIs across all 355 ROIs). The number of output units corresponds to the number of individuals to be identified. The Softmax in the output layer was used to obtain the probability for each individual. The fpFCs were computed from the trained parameters of the DNN. (b) The tvFC patterns from one of the two visits were used for training, and the tvFC patterns from the other visit were used to validate and test the trained DNN model. (c) TvFC patterns were calculated from four rfMRI runs in the HCP dataset obtained from two visits. DNN, deep neural network; FC, functional connectivity; HCP, Human Connectome Project; rfMRI, resting\u2010state fMRI; ROIs, regions\u2010of\u2010interest; tvFC, time\u2010varying FC. \n  \n\n###  fMRI   data and preprocessing \n  \nWe used rfMRI data available from the HCP S1200 dataset (healthy young adults; age\u2009=\u200922\u201335) (Van Essen et al.,\u00a0 ). The rfMRI data were acquired on a customized Siemens 3\u2009T Connectome Skyra with a standard 32\u2010channel head coil at Washington University (Elam et al.,\u00a0 ). A total of 1200 volumes (approximately 14.4\u2009min) were obtained from each rfMRI run (TR/TE\u2009=\u2009720/33.1\u2009ms, voxel size\u2009=\u20092.0\u2009mm isotropic, slice thickness\u2009=\u20092.0\u2009mm without a gap, flip angle [FA]\u2009=\u200952\u00b0, field\u2010of\u2010view [FoV]\u2009=\u2009208\u2009\u00d7\u2009180\u2009mm ). Subjects were instructed to open their eyes and gaze at a crosshair while scanning. Each subject visited twice (on Days 1 and 2), and went through two runs of rfMRI where echo\u2010planar imaging (EPI) volumes were acquired in two different phase\u2010encoding directions (i.e., right\u2010to\u2010left [R\u2010L] and left\u2010to\u2010right [L\u2010R]). \n\nSeveral preprocessed versions of the raw rfMRI data are readily available within the HCP S1200 dataset (Glasser et al.,\u00a0 ; Glasser et al.,\u00a0 ; Griffanti et al.,\u00a0 ; Smith et al.,\u00a0 ). We used the preprocessed data from the HCP pipeline scripts v3.13.2. with MSMAll DeDrifting and Resampling based on MSMAll registration ( ). More specifically, the BOLD time series at cortical vertices has been registered to a template using MSMAll pipelines and optionally denoised using ICA\u2010FIX algorithms ( ), which is effective for ruling out connectivity due to spurious signals (Xifra\u2010Porxas et al.,\u00a0 ). In addition, the BOLD time series of rfMRI data underwent band\u2010pass filtering in the range of 0.01\u20130.1\u2009Hz. A total of 870 individuals out of approximately 1200 had four intact rfMRI runs with 1200 volumes per run and their rfMRI volume\u2010series files were available for several preprocessing strategies (i.e., MSMAll/MSMSulc registration and with/without ICA\u2010FIX denoising). The MSMSulc only used myelin maps (cortical folding patterns) to guide the registration process, whereas the MSMAll also incorporated rfMRI. The primary focus of our analysis was on datasets processed using MSMAll; however, we conducted comparative evaluations on datasets using MSMSulc as well. \n\n\n### Regions\u2010of\u2010interest and functional networks \n  \nThe 1200\u2010volume rfMRI data were registered to a template in the gray ordinate coordinate system, in which there are 59,412 vertices across the bilateral hemispheres. The multimodal parcellation (MMP) map was used to acquire the time series for 360 cortical regions\u2010of\u2010interest (ROIs) (Glasser et al.,\u00a0 ). We grouped our 360 ROIs based on seven cortical FNs (Thomas Yeo et al.,\u00a0 ), which include the visual network (VN), somatomotor network (SMN), dorsal attention network (DAN), ventral attention network (VAN), limbic network (LN), frontoparietal network (FPN), and default\u2010mode network (DMN) (Figure\u00a0 ). Because the seven FNs had been defined using voxels, we projected them onto vertices then assigned FN membership for each ROI based on the most extensive overlap (i.e., a winner\u2010takes\u2010it\u2010all approach). Then, 5 out of 360 ROIs, residing in the medial wall near the cingulate cortex, did not overlap with any of the 7 FNs. Thus, 355 ROIs in total were assigned to the seven FNs. \n\nIn addition, we compared the identification performance using brain templates with fewer brain regions than present in the Glasser 360 atlas. To this end, we chose Schaefer's 100 and 200 ROIs defined using a gradient\u2010weighted Markov Random Field (gwMRF) model (Schaefer et al.,\u00a0 ). All ROIs were distributed among the seven FNs ( ) (Figure\u00a0 ). \n\n\n### Estimation of tvFC patterns \n  \nWe averaged the BOLD time series in each of 355 ROIs across the vertices, yielding a 355\u2009\u00d7\u20091200 matrix for each rfMRI run. The tvFC patterns were obtained based on the sliding window approach. We calculated the Pearson's correlation coefficients between the time series for the 355 ROIs within each time window (Figure\u00a0 ). A vector of the upper diagonal edges of the 355\u2009\u00d7\u2009355 correlation matrix was 62,835 dimensional (cf. 4950 and 19,900 for Schaefer's 100 and 200 ROIs, respectively), which was Fisher's r\u2010to\u2010z transformed and pseudo   z  \u2010scored. The window slid forward from the first volume with no overlap with the previous window to prevent dependency between adjacent tvFC patterns. We obtained tvFC patterns for each of several window lengths ranging from seconds to minutes (i.e., 15\u2009s, 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min). Considering the passband for the band\u2010pass filtering (0.01\u20130.1\u2009Hz, or equivalently, 10\u2010s to 100\u2010s\u2009cycles), we set the shortest window length as 15\u2009s. The number of tvFC patterns for each rfMRI run (14.4\u2009min) was 57, 28, 14, 4, and 2 for the 15\u2010s, 30\u2010s, 1\u2010min, 3\u2010min, and 5\u2010min windows, respectively. For each window length, corresponding tvFC patterns from all the four rfMRI runs per subject were used for training and the validation/testing of prediction models. \n\n\n### Individual identification \n  \nA fully connected DNN was employed to identify individuals (i.e., subjects) using the input tvFC patterns, where each of the output units from the DNN was assigned for each individual. Each hidden layer consisted of 2000\u2009units. Thus, the number of units per layer was 62,835 for the input layer, 2000 for each of the hidden layers 1 and 2, and   N   for the output layer, where   N   is the target number of individuals to identify. The DNN was trained to classify an individual's identity as a target label ( ). This multi\u2010class classification was implemented via the Softmax layer in the output ( ). The cost function for the DNN used to train the weight parameters was as follows: where  ,   is the output of the last layer corresponding to input  ,   is the L1\u2010norm parameter for the weights in the   l  th layer of the DNN ( ), and   is the L2\u2010norm parameter. \n\nThe weight sparsity levels between the DNN layers were explicitly controlled via the strength of  , and the optimal sparsity level was identified (see the supplementary materials, \u201c  Weight Sparsity Optimization  \u201d for more details including hyperparameter tuning). The   was fixed as 10 . The weight sparsity\u2010controlled DNN has shown better performance in classification/regression tasks than elastic net regularization (Kim, Bandettini, & Lee,\u00a0 ). This is because of the fine\u2010grained optimization of sparsity level using L1\u2010norm penalty via adaptive   during the training phase, compared to the optimization of the L1\u2010norm penalty term without an explicit weight sparsity control. The optimizer was stochastic gradient descent with an initial learning rate of 0.03 and a momentum of 0.9. The mini\u2010batch size varied from 13 to 1102 depending on the total number of individuals and the number of samples available for each individual (varying across window lengths). The activation function in each hidden layer was a hyperbolic tangent (tanh), and batch normalization was applied to the hidden layers. A rectified linear unit did not outperform tanh in our study. The DNN classifier was implemented using the deep learning library TensorFlow 1.15 ( ) in Python 3.6. The code and sample data are publicly available at our GitHub repository ( ). Several configurations for the target number of individuals for identification were tested (i.e., 100, 300, 600, and 870 out of the 870 individuals from HCP S1200 with 4 rfMRI runs). This target number corresponded to the number of output nodes (  N  ) of the DNN. \n\nThe training set consisted of the tvFC patterns calculated from the EPI volumes in the two rfMRI runs with R\u2010L and L\u2010R phase\u2010encoding on Day 1. The EPI volumes from Day 2 were used as the validation (rfMRI run with R\u2010L phase\u2010encoding) and test (rfMRI run with L\u2010R phase\u2010encoding) sets, respectively. Classification performance was evaluated for the training, validation, and test set, by calculating the error rate using all tvFC patterns/samples across all the subjects in each set. We additionally ran a slightly modified version of train\u2010and\u2010evaluate: training DNNs with all window lengths but not a 15\u2010s window and validation/test with a 15\u2010s window. It was to evaluate whether DNNs trained with varied window lengths were valid for noisy tvFCs from the shortest window as well. \n\nBecause physiological noise and head motion alter the BOLD time series and subsequently the FC patterns, the possibility that these confounding artifacts contributed to individual identification was investigated. Specifically, the identification performance using ICA\u2010FIX\u2010based time series denoising was evaluated. In addition, we monitored head motion on Day 1 using the root mean square (RMS) of frame\u2010wise displacement available in the \u201cMovement_RelativeRMS\u201d file in the HCP dataset. Then, the identification performance was compared at a subject level between individuals with high and low head motions. \n\nPattern matching (PM) between the training samples and the validation/test samples was conducted based on Pearson's correlation coefficients in MATLAB 2017b. Logistic regression (LR) and linear support vector machine (SVM) were tested as shallow linear ML models as a comparison with our DNN model. The LR (the \u201cLogisticRegression\u201d class) and SVM (the \u201cLinearSVC\u201d class) algorithms implemented in the Scikit\u2010learn library (version 0.22.1; Python 3.6) were used with an L2 penalty. For LR, we chose the \u201cnewton\u2010cg\u201d solver from among the five optional solvers available in Scikit\u2010learn because of its superior performance for large HCP datasets. The error rate for LR was investigated for both the multinomial scheme (to allow a comparison with the DNN) and one\u2010versus\u2010rest (OvR) schemes (to allow a comparison with SVM, whose algorithm only supports OvR). The L2 penalty term was optimized using a grid search (C \u2208 {10 , 5\u2009\u00d7\u200910 , 10 , 3\u2009\u00d7\u200910 , 5\u2009\u00d7\u200910 , 10 , 3\u2009\u00d7\u200910 , 5\u2009\u00d7\u200910 } for LR; C \u2208 {10 , 10 , 5\u2009\u00d7\u200910 , 10 , 5\u2009\u00d7\u200910 , 10 , 5\u2009\u00d7\u200910 , 10 , 5\u2009\u00d7\u200910 , 1, 5, 10, 50, 10 , 5\u2009\u00d7\u200910 , 10 , 5\u2009\u00d7\u200910 , 10 } for SVM). We performed a nonparametric test, specifically the Wilcoxon signed\u2010rank test, to compare the error rates of DNN with the error rate of LR, due to the small sample size and non\u2010normal distribution. A one\u2010tailed test was selected, positing the null hypothesis that DNN's performance would not surpass that of LR, against the alternative hypothesis that DNN would outperform LR. To determine whether error rates varied with the number of layers, we performed the Wilcoxon rank\u2010sum test, a nonparametric analogue to the two\u2010sample   t   test. Furthermore, we conducted a one\u2010sample   t   test to assess the difference in error rates across all subjects compared with the chance level and a paired\u2010sample   t   test to compare subject\u2010wise error rates between two DNN models. \n\n\n### Fingerprint of FC \n  \nWe extracted an FC signature for individual identification that is hidden within the tvFC patterns from a trained DNN model. To this end, the weight feature vectors   (62,835\u2009\u00d7\u2009  N  ) were estimated by multiplying the weight parameter matrices   across all layers from the input layer to the output layers (Equation\u00a0 ) (Erhan et al.,\u00a0 ; Jang et al.,\u00a0 ; Kim et al.,\u00a0 ; Kim, Bandettini, & Lee,\u00a0 ): \n\nThe   z  \u2010scored vector   (62,835\u2009\u00d7\u20091) yielded an fpFC for an individual, where   n   indicates an index of output unit corresponding to the individual. Based on both qualitative and quantitative criteria, we labeled the edges within the fpFCs with four designated categories: (a) reliable edge, (b) distinct edge, (c) important edge, and (d) robust edge (Table\u00a0 ). For the quantitative definition of an edge based on fpFCs, we calculated the standard deviation (STD) across the target individuals and an intra\u2010class correlation coefficient (ICC; two\u2010way random effects with absolute agreement) (McGraw & Wong,\u00a0 ) across the target individuals and training repetitions using MATLAB. Only the edges whose ICCs were greater than 0.6 (Cicchetti,\u00a0 ; Landis & Koch,\u00a0 ) were considered to be (a) reliable and preserved in the subsequent analysis to determine the distinct edges. The heterogeneity of the fpFCs across the individuals was also evaluated. The STD was calculated for each edge of an average fpFC from the five repetitions across individuals. The upper 98th percentile of the STD was reported as a (b) distinct edge. The edges that met the thresholds for both reliability (i.e., ICC\u2009>0.6) and distinctiveness (i.e., STD\u2009>\u200998th percentile) were defined as (c) important edges. Important edges that were common across the five window lengths were obtained and classified as (d) robust edges which are invariant to window length. The number of important edges for each FN pair was compared between window lengths. Unless otherwise stated, this ICC and STD analysis was conducted using the fpFCs obtained from two\u2010hidden\u2010layer DNN models trained to identify 300 individuals from the HCP. To evaluate the stability of error rates and the significance of FC edges, we repeated the training of DNN models an additional 20 times, culminating in a total of 25 iterations. FC edges were visualized using circular plots and a 3D brain ( ). Generating an activation maximization map   can be an alternative method to interpret the trained DNN model, rather than weight features. To this end, input FC edges that maximized each of the output units were estimated using a gradient ascent approach via back\u2010propagation (Equation\u00a0 ) (Erhan et al.,\u00a0 ): fpFCs were similarly obtained by z\u2010scoring a vector at an output unit. \n  \nGlossary for connectivity edges defined from fpFCs. \n    \n\n### Reproducibility of the   fpFC   for various training configurations \n  \nAn individual fpFC should be consistent between visits (i.e., sessions; Days 1 and 2) and not be biased toward a particular group of individuals. We thus evaluated the specificity of the obtained fpFC for six configurations, with distinct groups of individuals used for identification and the training and validation/test data shuffled. Specifically, we created three groups of individuals within the training dataset for the DNN model, each encompassing one common target individual (Figure\u00a0 ): (I) an initial group with 300 individuals including the target individual, (II) a second group with 300 individuals including 299 new individuals and the target individual, and (III) a third group that adds another 300 new individuals to the second group (i.e., a total of 600 individuals). We obtained six independent training outcomes by employing the three distinct groups while alternating the training and validation/test sets (i.e., Day 1 data for training and Day 2 data for validation/testing; Day 2 data for training and Day 1 data for validation/testing). Similarity scores across the fpFCs were measured using Pearson's correlation coefficients for the configurations. For each pair of configurations, the similarity of the target individual's fpFCs was compared with the similarities of all other individuals' fpFCs. \n\n\n### Genetic influence on the   fpFC  \n  \nDue to previous reports indicating that genetic relatedness is reflected in the fpFC (Finn et al.,\u00a0 ; Liu et al.,\u00a0 ; Mueller et al.,\u00a0 ), we were motivated to examine whether our reported fpFC originated in part from genotypical characteristics. We divided individuals from the HCP into four groups according to their genetic relatedness: (a) monozygotic twins (MZ), (b) dizygotic twins (DZ), (c) siblings, and (d) others. The similarity of the fpFC was calculated for paired individuals within each group and compared between groups. More specifically, first, we obtained the average fpFC for each individual across the five window lengths using mean fpFC from the five trained DNNs to classify 300 individuals with random initial weights for each window length. We stratified the 300 individuals into pairs of individuals based on their genetic relatedness: MZ, who shared identical genes, DZ, siblings, and others. The averaged fpFCs were compared across these stratified groups of individuals. The correlation coefficients for the average fpFCs were Fisher's r\u2010to\u2010z transformed. The   z  \u2010scored similarity of the representative fpFCs was compared across the groups using one\u2010way ANOVA followed by a post hoc two\u2010sample   t   test. The resulting   p   values were Bonferroni\u2010corrected. \n\n\n### Generalization of the trained   DNN   to an independent dataset via transfer learning \n  \nIn this study, we evaluated the application of a DNN, which had been trained on HCP data in the processing of independent datasets through the extrapolation of learned characteristics. For this purpose, we conducted a similar rfMRI experiment at our institute following the HCP protocol using a 3\u2010T MRI scanner (MAGNETOM Tim Trio; Siemens, Erlangen, Germany) and a 12\u2010channel head coil (to display cross\u2010fixation for subjects with their eyes open during rfMRI runs via MR\u2010compatible visual binocular goggles). A multiband (MB) gradient\u2010echo EPI pulse sequence developed by the Center for Magnetic Resonance Research (Department of Radiology, University of Minnesota) was used to acquire the rfMRI data at our institute. The specific imaging parameters were as follows: MB factor\u2009=\u20092; Generalized Auto\u2010Calibrating Partial Parallel Acquisitions (GRAPPA) on; TE\u2009=\u200930\u2009ms; FA\u2009=\u200971\u00b0; FoV\u2009=\u2009192\u2009\u00d7\u2009192\u2009mm ; 50 axial slices without a gap; TR\u2009=\u20091.44\u2009s (twice that of the HCP [0.72\u2009s] due to the lower parallel imaging factor to cover the whole brain); and voxel size\u2009=\u20093\u2009\u00d7\u20093\u2009\u00d7\u20093\u2009mm  (Kim, Tegethoff, et al.,\u00a0 ; Tozzi et al.,\u00a0 ). In addition, we collected a gradient\u2010echo field map to correct EPI distortion as required from the HCP pipelines ( ). \n\nThe overall study protocol was approved by the Institutional Review Board (IRB) at Korea University. The participants submitted written consent forms and were compensated as outlined in the IRB documents. Fifteen healthy young adults participated in the experiment (age\u2009=\u200923\u201334, seven females). Fourteen of these subjects (all except one female) made two visits on consecutive days. They were instructed to gaze at a white cross\u2010fixation on a black screen during the rfMRI run. Their eyes were monitored via eye trackers attached to the goggles throughout the rfMRI run (NordicNeuroLab;  ). Two rfMRI runs with R\u2010L and L\u2010R phase\u2010encoding and a T1\u2010weighted image were acquired on the first day, while two rfMRI runs with R\u2010L/L\u2010R phase\u2010encoding and T2\u2010weighted images were acquired on the second day. The rfMRI runs were preprocessed using the HCP pipelines (Glasser et al.,\u00a0 ; Glasser et al.,\u00a0 ; Griffanti et al.,\u00a0 ). Subsequently, the tvFC patterns were obtained from the preprocessed rfMRI BOLD time series described in Section\u00a0 . The number of time points used for FC calculation with the same window length was half that of the HCP because the TR for our dataset was twice as long. \n\nA pretrained DNN for the 870 individuals from the HCP using tvFC patterns from a 15\u2010s window was restored for transfer learning. The TensorFlow variables of the pretrained DNN, such as the weights, biases, and batch\u2010normalization\u2010related parameters from the first to the last hidden layers, were restored. A new output classification layer was created with 314 units to accommodate the 14 new individuals from our institute and the 300 individuals from the HCP, and randomly initialized. During the transfer learning phase, the DNN model was trained under either of two conditions: (1) freezing parameters from the first to the last hidden layers and fine\u2010tuning only the classification layer, and (2) fine\u2010tuning all layers. A DNN model trained using randomly initialized weights across all layers was evaluated as a baseline model. \n\n\n\n## RESULTS \n  \n### Individuals are more clearly distinguished using a   DNN   than using shallow linear models \n  \nFigure\u00a0  shows the error rates across various scenarios. For 300 individual identification cases, the longer the time window for the tvFC patterns, the lower the error rate, particularly for PM (Figures\u00a0  and  ). Model\u2010based approaches, including linear SVM, LR, and the DNN exhibited a remarkably lower error rate compared to PM. Notably, there was a substantial reduction in the error rates of LR and DNN for a 30\u2010s window (e.g., 83.2% for PM, 0.4% for LR, and 0.2% for the DNN) compared with a 15\u2010s window (e.g., 91.4% for PM, 5.1% for LR, and 2.9% for the DNN). For all window lengths, two\u2010hidden\u2010layer DNNs had lower error rates than LR (DNN: 2.9, 0.2, 0.1, 0.1, and 0.2% for 15\u2009s, 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min\u2010windows, respectively; LR: 5.1, 0.4, 0.2, 0.3, and 0.3%; one\u2010tailed Wilcoxon signed\u2010rank test,   N  \u2009=\u20095,   W  \u2009=\u20090,   p  \u2009=\u2009.0312). The performance of the DNNs remained consistently superior through an additional 20 repetitions of the training process for the DNNs (DNN: 3.0, 0.2, 0.1, 0.1, and 0.1%;   N  \u2009=\u200925,   W  \u2009=\u20090,   p  \u2009=\u20093.0\u2009\u00d7\u200910 ). We could not obtain identification results for the 15\u2010s window from SVM because there was an overflow error due to the large sample size. An alternative approach is to use libsvm\u2010based SVC instead of LinearSVC; however, it produced an inferior performance and was computationally expensive. For example, the test error rate for a 15\u2010s window and 300 individuals was 39.3% using the trained libsvm\u2010based SVC model and took about a month to compute using our system (two Intel Xeon Gold 5120 CPUs). The error rate generally increased as the number of individuals increased (Figure\u00a0 ). The DNN was consistently the best performing model, with an error rate of 6.7% for a 15\u2010s window with 870 individuals (cf. 9.9% for LR). \n  \nTest error rates for the individual identification of Human Connectome Project (HCP) subjects for various scenarios. (a) Error rate for 300 individual identification scenarios for various window lengths. The error rate for LR was consistently higher using the OvR scheme compared to the multinomial scheme. We conducted five repetitions of training and testing for the DNN with random initialization at each window length. (b) Error rate for various numbers of individuals to be identified using 15\u2010s window\u2010based tvFC, time\u2010varying functional connectivity (tvFC). Multinomial target output was used for LR. A DNN identifying 300 individuals was repeated five times, and a DNN identifying 100 or 600 individuals was run with three repetitions. DNN, deep neural network; LR, logistic regression; OvR, one\u2010versus\u2010rest; PM, pattern matching; SVM, support vector machine. \n  \nThe DNN models trained using tvFC patterns from longer window lengths than 15\u2009s were tested using tvFC patterns from a 15\u2009s window. The error rates of 300 individual identification models from 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min windows were 3.7, 6.5, 16.5, and 52.0%, respectively. These rates are notably low given that the chance level is 1\u20131/300\u2009=\u200999.7% (one\u2010sample   t   test,   df  \u2009=\u2009299,   p  \u2009=\u2009.0). The DNN trained on the aggregate of the four longer windows (i.e., 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min) exhibited a test error rate of 4.0% on the 15\u2009s window, which was higher than 3.7% from the DNN trained solely on the 30\u2009s window (paired\u2010sample   t   test,   df  \u2009=\u2009299,   T  \u2009=\u20092.2,   p  \u2009=\u2009.028). This error rate was also higher than the 2.9% from the DNN trained using the FC patterns from the 15\u2009s window (paired\u2010sample   t   test,   df  \u2009=\u2009299,   T  \u2009=\u20096.3,   p  \u2009=\u20098.6\u2009\u00d7\u200910 ). \n\nRegistration by MSMSulc instead of MSMAll or time series denoising using ICA\u2010FIX improved the classification performance (Figure\u00a0 ). The error rate for the 21 individuals with high head motion (RMS [mm]\u2009>\u2009.14; Finn et al.,\u00a0 ) was 5.8%\u2009\u00b1\u20096.6, which was greater than the error rate of 2.7%\u2009\u00b1\u20094.0 for the remaining 279 individuals (two\u2010sample   t   test,   df  \u2009=\u2009298,   T  \u2009=\u2009\u22123.2,   p  \u2009=\u2009.0016). An adaptation of an atlas with fewer ROIs led to a higher error rate (Figure\u00a0 ). Error rates of DNN and LR using Schaefer's 100 and 200 ROIs were still substantially lower than the PM. However, DNN did not outperform LR for the 5\u2010min window using 200 ROIs, or 3\u2010 and 5\u2010min windows using 100 ROIs. \n\n\n### The weight\u2010sparsity\u2010optimized   DNN   is superior to alternative models \n  \nFor tvFCs with a 15\u2010s window, the DNN model trained with optimal weight sparsity levels performed better when there were two hidden layers (validation/test error rate\u2009=\u20094.1/2.9%) compared to one hidden layer (5.3/3.7%; Wilcoxon rank\u2010sum test,   N  \u2009=\u20095/5,   U  \u2009=\u200925/25,   p  \u2009=\u2009.0079/.0079). While the test set benefitted significantly from the two hidden layers, the improvement in the validation set was marginal when compared to the three\u2010layer setup (4.2/3.1%; Wilcoxon rank\u2010sum test,   N  \u2009=\u20095/5,   U  \u2009=\u200921/25,   p  \u2009=\u2009.0952/.0079) hidden layers (Figure\u00a0 ). The test error rate for the two\u2010hidden\u2010layer DNN (2.9%) was lower than that of the LR (5.1%; one\u2010tailed Wilcoxon signed\u2010rank test,   N  \u2009=\u20095,   W  \u2009=\u20090,   p  \u2009=\u2009.0312), representing an improvement of 43.1%. The optimal weight sparsity for each hidden layer differed according to the depth of the DNN. The results for optimal weight sparsity with varying numbers of hidden layers are described in the \u201c  Optimal Levels of Weight Sparsity with Varying Numbers of Hidden Layers  \u201d section of the Supplementary Materials. \n\n\n###  FpFC   characterizes individual connectivity profiles and suggests robust edges \n  \nOverall, the edges with high ICCs exhibited strong variability across individuals (Figures\u00a0  and  ). We primarily found window\u2010length invariant robust edges in the DMN and FPN, many of which are located in the frontal lobe (Figure\u00a0 ; Table\u00a0 ). Notably, the bilateral dorsolateral prefrontal cortex (DLPFC; i.e., area 9 anterior) was included in the top three most critical regions in the FPN. In addition, intra\u2010FN and intra\u2010hemispheric connections were dominant over inter\u2010FN and inter\u2010hemispheric connections. Robust edges accounted for almost half of the important edges for each window length; in particular, 47.3, 47.3, 47.3, 52.1, and 48.0% of the important edges identified for the 15\u2010s, 30\u2010s, 1\u2010min, 3\u2010min, and 5\u2010min windows, respectively, were classified as robust. Across different numbers of ROIs, we generally found noticeable engagement of intra\u2010hemispheric connections in the frontal lobe for individual identification (Figures\u00a0 ,  , and  ). There was a larger involvement of the occipital lobe using 100 and 200 ROIs from Schaefer's atlas than 355 ROIs from Glasser's atlas. The edges in intra\u2010DMN and DMN\u2010FPN were consistently significant (Figures\u00a0 ,  , and  ). However, the edges in intra\u2010VN, VN\u2010FPN, SMN\u2010DAN, or VAN\u2010DMN were more pronounced for 100 ROIs and 200 ROIs compared to 355 ROIs. On the other hand, the significance of edges in intra\u2010DAN, intra\u2010FPN, VN\u2010DAN, DAN\u2010VAN, DAN\u2010FPN, LN\u2010FPN, LN\u2010DMN, or FPN\u2010DMN was reduced. The fpFC from activation maximization maps resulted in patterns that were markedly similar to our weight feature representation based on the multiplication of weight matrices (Figure\u00a0 ). The weight feature vectors suggested 595 robust edges across window lengths, while activation maximization suggested 591 robust edges. A total of 582 robust edges were identical between these estimates. Nonetheless, some edges existed that were found by weight feature vectors but not by activation maximization, or vice versa (Figure\u00a0 ). Most of the edges significant only for weight feature vectors were connected to the ROIs in the frontal cortex, predominantly in FPN and DMN. In contrast, activation maximization not only revealed the important edges of the frontal lobe but also indicated associations with other brain areas. For example, there were edges in FPN\u2010VAN, including a connection between the frontal and the temporal lobe. \n  \nInformative edges in the fpFC for individual identification. (a) ICC and STD for each edge in the fpFCs across five window lengths. (b) Robust edges (c) The number of important edges from intra\u2010 and inter\u2010FNs. In each plot, the brighter bars at the bottom indicate the number of robust edges. The darker bars above represent important but not robust edges for each window length. The plots on the right present the number and percentage of important edges for the corresponding FN pair between window lengths. Please see Table\u00a0  for the details on the definition of robust and important edges. The fpFC patterns were obtained from DNN models for the identification of 300 individuals. DAN, dorsal attention network; DMN, default\u2010mode network; FN, functional network; fpFC, fingerprint of functional connectivity; FPN, frontoparietal network; ICC, intra\u2010class correlation coefficient; LN, limbic network; SMN, somatomotor network; STD, standard deviation; VAN, ventral attention network; VN, visual network. \n    \nList of the 30 most frequently involved ROIs in the robust edges of the fpFCs used for the identification of 300 individuals. The ROIs are listed based on how many robust edges were connected to the ROIs. The ROI labels were obtained from a 360\u2009MMP parcellation map (Glasser et al.,\u00a0 ), and the corresponding region from the AAL atlas with the highest overlap for each ROI is indicated (for more details, refer to the \u201c  ROIs on the AAL Atlas  \u201d section of the Supplementary Materials). \n    \n\n### Important edges for individual identification vary depending on the window length \n  \nFor all window lengths, the intra\u2010 and inter\u2010FN edges of the FPN and DMN had the highest number of important edges (Figure\u00a0 ). When normalized by the number of total edges, the intra\u2010FPN and intra\u2010DAN also exhibited a high proportion of important edges. Although we reported that approximately 50% of the important edges were commonly found across window lengths, there were some noticeable changes in the important edges depending on window length. For example, there were a greater number of important but not robust intra\u2010FN edges in the VN and DAN as the window length decreased (Figure\u00a0 ). On the other hand, a higher number of inter\u2010FN edges, such as those for FPN\u2010DMN, were important as the window length increased. The intra\u2010DMN and intra\u2010FPN also exhibited a slight change in the number of important edges between window lengths. From 25 repetitions instead of 5 repetitions of DNN training, 96.9, 96.4, 95.2, 32.8, and 11.5% of the total edges were replicated as important edges for the 15\u2010s, 30\u2010s, 1\u2010min, 3\u2010min, and 5\u2010min windows. When we lowered the threshold for ICC from 0.6 to 0.5, 96.9, 96.4, 95.2, 85.4, and 89.4% were replicated. Figure\u00a0  shows the average fpFC across five window lengths for each of five arbitrarily chosen individuals (Figure\u00a0 ) and the fpFC for each window length (Figure\u00a0 ). The combination of positive and negative values for the edges appeared to characterize distinct fpFCs between individuals. \n  \nFpFCs from five arbitrarily chosen individuals. (a) FpFCs showing the most significant of robust edges. (b) FpFCs showing the most significant of important edges for each of the five window lengths. Robust edges were important edges commonly observed across the five window lengths, and important edges were fpFC edges with ICC\u2009>\u20090.6 and STD\u2009>\u200999.5th percentile (instead of the 98th percentile for better visualization in the circular and glass brain plots). fpFC, fingerprint of functional connectivity; ICC, intra\u2010class correlation coefficient; STD, standard deviation. \n  \n\n### The   fpFC   is not bound by a target group of individuals in the   DNN  \n  \nThe fpFC of an individual was always closer to their own fpFC than to the fpFCs of other individuals across the tested configurations (Figure\u00a0 ). Specifically, others' fpFCs exhibited very low correlations (almost zero), while the target's own fpFC displayed much higher correlation. A qualitative examination of the robust edges' similarity to their corresponding fpFC revealed a tendency for the fpFC patterns to exhibit greater resemblance to their own rather than to those of others, irrespective of the configuration (Figure\u00a0 ). Moreover, the important edges for 15\u2010s window tvFC obtained from the DNN model used to identify 300 individuals heavily overlapped with those from the DNN models used to identify 100, 600, and 870 individuals, with an overlap of 70, 81, and 82%, respectively. \n\n\n### Genetic information is present in the   fpFC  \n  \nThe mean of the   z  \u2010transformed Pearson's correlation coefficients of the fpFC for paired individuals was 0.13, 0.08, 0.06, and 0.00, for the MZ, DZ, sibling, and others groups, respectively (Figure\u00a0 ). One\u2010way ANOVA revealed a significant group difference (  df  \u2009=\u200944,849,   F  \u2009=\u200982.5,   p  \u2009<\u200910 ). The similarities of the fpFC within the others group were significantly lower than for any of the family groups (  df  \u2009=\u200944,811,   T  \u2009=\u200912.2 for MZ;   df  \u2009=\u200944,804,   T  \u2009=\u20095.1 for DZ;   df  \u2009=\u200944,827,   T  \u2009=\u20098.6 for sibling; Bonferroni corrected   p  \u2009<\u200910  from a post hoc two\u2010sample   t   test). The similarities within the MZ group were more significant than within the sibling group (  df  \u2009=\u200942,   T  \u2009=\u20095.3, Bonferroni corrected   p  \u2009<\u200910 ) but the significance was reduced when compared to the DZ group (  df  \u2009=\u200919,   T  \u2009=\u20093.4, Bonferroni\u2010corrected   p  \u2009<\u2009.05) due to a potential outlier in the DZ group. These findings suggest that genetic information is reflected by the phenotypical fpFC. \n  \nSimilarity of the fpFC between paired individuals within each of four groups selected based on their genetic relatedness. Pearson's correlation coefficients followed by Fisher's r\u2010to\u2010z transform were used as a similarity measure. One\u2010way ANOVA was conducted on the four groups followed by a post hoc two\u2010sample   t   test. The resulting   p  \u2010values were Bonferroni\u2010corrected. fpFC, fingerprint of functional connectivity; MZ, monozygotic twins; DZ, dizygotic twins. \n  \n\n### The proposed approach can be generalized to an independent dataset \n  \nThe average error rate for the identification of the 314 individuals using 15\u2010s window tvFC was 15.9% when all hidden layers were frozen and only the output layer was trained (Figure\u00a0 ). The error rate obtained for the HCP data and our institute was 14.5 and 45.2%, respectively. For a 30\u2010s window, the average error rate declined to 4.5% (3.4% for HCP and 27.3% for our institute). This error rate remains higher in comparison to the scenario where the model was trained with random initial weights, which showed an error rate of 4.1% (3.1 and 25.5%) for a 15\u2010s window and 0.6% (0.2 and 8.7%) for a 30\u2010s window. However, these rates were significantly above the level of chance (1\u20131/314\u2009=\u200999.7%; one\u2010sample   t   test,   p  \u2009<\u200910 ) for both a 15\u2010s window (  df  \u2009=\u2009299,   T  \u2009=\u2009\u2212106.5 and   df  \u2009=\u200913,   T  \u2009=\u2009\u221213.7), and a 30\u2010s window (  df  \u2009=\u2009299,   T  \u2009=\u2009\u2212209.6 and   df  \u2009=\u200913,   T  \u2009=\u2009\u221214.9). This demonstrates that, despite not being flawless, the features learned for the individual identification of HCP subjects could hold informative value when applied to an independent dataset. \n\nWhen parameters of all layers were fine\u2010tuned, the average error rate was substantially reduced to 4.2% (3.3% for HCP data and 24.3% for our institute) for a 15\u2010s window, and 0.4% (0.1 and 4.9%) for a 30\u2010s window. In addition, false positives between the subjects in the HCP and at our institute (i.e., falsely identified as an HCP subject for the subject at our institute) were substantially decreased for the transfer learning model when all DNN parameters were fine\u2010tuned. Interestingly, training completion could potentially have been expedited during the fine\u2010tuning process. For a 15\u2010s window, the average error rate at the 200th epoch was the lowest as 3.8%. Error rates for subjects from our institute were marginally lower (  df  \u2009=\u200913,   T  \u2009=\u2009\u22121.9,   p  \u2009=\u2009.09) in comparison to those for HCP participants, which did not show a statistically significant decrease (  df  \u2009=\u2009299,   T  \u2009=\u2009\u22120.9,   p  \u2009=\u2009.4) relative to the random initialization case (Figure\u00a0 ). For a 30\u2010s window, the lowest error rate during fine\u2010tuning was noted at the 600th epoch, whereas for the randomly initialized case, this low was observed at the 500th epoch. The error rates for fine\u2010tuning involving both HCP subjects and those from our institute were substantially lower (  df  \u2009=\u2009299,   T  \u2009=\u2009\u22122.5,   p  \u2009=\u2009.01 and   df  \u2009=\u200913,   T  \u2009=\u2009\u22122.9,   p  \u2009=\u2009.01) compared with those of random initialization. \n  \n(a) Error rates and (b) confusion matrix at the 200th epoch from individual identification using transfer learning with fine\u2010tuning of all parameters in the deep neural network (DNN) trained for the identification of 870 individuals using 15\u2010s windows. The total number of individuals to be identified was 314, which included 300 from the Human Connectome Project (HCP) and 14 from our institute. The dotted black horizontal line indicates the average error rate. The color bar for the confusion matrix was set at 6% to better visualize misclassified samples. \n  \n\n\n## DISCUSSION \n  \n### Summary of the study \n  \nWe reported that tvFC from time windows as short as 15\u2009s could be used to identify individuals with a DNN. The error rate from the identification of 300 individuals using a two\u2010hidden\u2010layer DNN (2.9 and 0.2% for 15 and 30\u2009s, respectively) was substantially lower than that for alternative approaches (91.4 and 83.2% using PM; 5.1 and 0.4% using LR). The two\u2010hidden\u2010layer DNN also had the lowest error rate when compared to alternative models for 870 target subjects (95.1, 9.9, and 6.7% for the PM, LR, and DNN, respectively; 15\u2010s window). Individual fpFC, which characterizes the tvFC of individuals, contained important edges that were reliable and heterogeneous across the individuals. Approximately 50% of the important edges were robust, i.e., commonly found across the five window lengths (15\u2009s, 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min). The two most representative FNs associated with these robust edges were the FPN and DMN, many of which are located in the frontal lobe. We also raised the possibility that genotypical characteristics are reflected in the phenotypical traits of tvFC using stratified groups based on their genetic relatedness, which has been similarly reported in a recent study (Demeter et al.,\u00a0 ). Finally, we demonstrated the feasibility of generalizing a DNN model trained using HCP data to an independent dataset acquired from our institute. \n\n\n### The advantage of   DNN   in individual identification \n  \nOverall, the reported findings suggest that a multilayered DNN model is more suitable for individual identification using tvFC patterns than alternative shallow approaches. In particular, PM demonstrated weaker performance overall than model\u2010based classifiers, probably because PM treats all edges equally and thus is susceptible to noise. On the other hand, model\u2010based approaches can weigh important edges by learning the corresponding weighting factors as model parameters. This feature\u2010learning capability has been suggested as a key advantage of deep learning approaches over standard ML approaches (Abrol et al.,\u00a0 ). Thus, the superior performance of our DNN model is likely to be due to the inherent feature\u2010learning ability through nonlinear hidden layers. In contrast, shallow models such as LR learn only decision boundaries using the given input features. \n\nUsing PM, a considerable gap in error rates existed between the 1\u2010 and 3\u2010min windows. This may be explained with reference to a previous study demonstrating that the fidelity of FC fingerprints measured by ICC does not reach saturation until the window length reaches a size of 144\u2009s to 288\u2009s (Figure\u00a0 ) (Van De Ville et al.,\u00a0 ). Although the DNN and LR models both performed almost perfectly for the 30\u2010s and longer windows (error rate <0.5%), the DNN for the 15\u2010s window exhibited a substantially lower error rate than did the LR. This suggests that individual\u2010specific information of FC reflected in BOLD signals between 0.033\u2009Hz (=1/30\u2009s) and 0.1\u2009Hz (due to band\u2010pass filtering) is usefully exploited by both the DNN and LR. However, for BOLD signals between 0.067\u2009Hz (=1/15\u2009s) and 0.1\u2009Hz, the DNN was better able to harness the information. Therefore, it appears that there may be relatively straightforward and crucial individual\u2010specific information contained within BOLD signals between 0.033 and 0.067\u2009Hz. In contrast, individual\u2010specific information in BOLD signals between 0.067 and 0.1\u2009Hz may be more subtle and noisier, which may be handled better by the DNN. \n\n\n### Weight\u2010sparsity\u2010optimized   DNNs   for effective information extraction \n  \nThe identification performance was possibly higher because of the enhanced feature selection in each layer in our DNN model due to weight sparsity optimization within a grid search framework (Jang et al.,\u00a0 ; Kim et al.,\u00a0 ). Our findings highlight the importance of fine\u2010grained FC feature selection for individual identification from noisy tvFC patterns, particularly for short time windows (Figure\u00a0 ). A detailed discussion of weight sparsity can be found in \u201c  Optimization of Weight Sparsity for Enhanced Performance of DNN  \u201d in Supplementary Materials. \n\n\n### Shared features of   tvFCs   across window lengths detected by   DNNs  \n  \nUpon evaluating the tvFC patterns with a 15\u2010s window, DNN models trained on longer windows demonstrated error rates much lower than the chance level. Moreover, almost half of the important edges in the fpFC were commonly found across all window lengths. These observations imply that DNN models trained for different window lengths share FC features to a substantial degree, despite potential variability due to window length difference (Figure\u00a0 ). Upon performing multiple iterations of DNN, retention rates of important edges for 3\u2010 and 5\u2010min windows were only 32.8 and 11.5%, respectively, whereas retention exceeded 95% for the shorter windows. Interestingly, the retention rates for the 3\u2010 and 5\u2010min windows increased to 85.4 and 89.4% when the ICC threshold was lowered from 0.6 to 0.5. This indicates that the reliability of DNNs for longer windows might be compromised by the limited size of the training samples. Conversely, the specificity of the window length became evident through the analysis of error rates in tests using the shortest 15\u2009s\u2010window. Specifically, the DNN trained across the 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min windows exhibited a higher error rate compared with training solely with the 15\u2009s window. This suggests that the individual\u2010specific FC feature in the shorter window may not be recovered from the increased number of training samples with longer window lengths (4\u2009+\u20098\u2009+\u200928\u2009+\u200956\u2009=\u200996). \n\n\n### Edges characterizing individuals in the   fpFC  \n  \nWe demonstrated that individual\u2010specific characteristics of the FC are primarily embedded in the FPN and DMN, largely in the frontal lobe (Figure\u00a0 ; Table\u00a0 ). In particular, DLPFC in the FPN is known as one of the most important regions for cognitive control, and is believed to play a critical role in shaping individual traits in brain function (Goldberg,\u00a0 ; Miller & Cohen,\u00a0 ). Furthermore, DLPFC has also been found to be highly variable in its activation level, volume size, location, structural connectivity, and FC (Friedman & Miyake,\u00a0 ; Panikratova et al.,\u00a0 ). For example, the FC of DLPFC has been shown to be related to individual variability in cognition (Panikratova et al.,\u00a0 ; Turnbull et al.,\u00a0 ). In line with these studies, we also found that the intra\u2010 and inter\u2010FN edges from the FPN and DMN were critical for robust identification of individuals across all window lengths (Figure\u00a0 ). Many previous studies have investigated representative FCs associated with individual traits. Finn et al. reported that regions in the FPN are important in forming personal fingerprints (Finn et al.,\u00a0 ). In addition, Pallar\u00e9s et al. found that the frontal and cingulate systems were important for individual identification (Pallar\u00e9s et al.,\u00a0 ). The DMN is known to be closely involved in internal mentation (Andrews\u2010Hanna,\u00a0 ). In particular, regions within the DMN have been associated with the retrieval of past experiences and the contemplation of future events during periods of rest, as well as in the processes of self\u2010reflection and understanding others. Based on this report, personal experiences and thoughts may have contributed to the FC features of the DMN in characterizing individuals in our findings. Furthermore, the DMN has been suggested as a network depicting individual differences regarding mind\u2010wandering or daydreaming (Gorgolewski et al.,\u00a0 ; Kucyi & Davis,\u00a0 ). The highly robust individual\u2010specific edges in the fpFC in the higher\u2010order cortical areas could also be attributed to epigenetic factors during prolonged developmental maturation (Mueller et al.,\u00a0 ; Petanjek et al.,\u00a0 ). The regions are also known to be under substantial genetic influence, which is consistent with our results (Figure\u00a0 ) (Colclough et al.,\u00a0 ). Despite the dominant edges from the FPN and DMN in the fpFC, there was also considerable involvement of other FNs. Intra\u2010FN edges from the VN, SMN, DAN, and LN were relatively important for shorter window lengths (Figure\u00a0  and Figure\u00a0 ). Interestingly, the low inter\u2010individual variability for these FNs in resting\u2010state FC has previously been reported (Mueller et al.,\u00a0 ). Inter\u2010FN edges such as VN\u2010DAN, LN\u2010FPN, and LN\u2010DMN appeared to be influenced by window lengths. Previous works using HCP data suggested the importance of higher\u2010order cognitive systems and sensorimotor systems in FC fingerprinting (Liu et al.,\u00a0 ; Van De Ville et al.,\u00a0 ). Notably, Van De Ville et al. found that VN and SMN are informative for shorter windows, whereas DMN and FPN are most informative for longer windows. Our findings are also in line with another work reporting FPN and DMN as containing the most reliable FC edges with respect to test\u2013retest reliability (Noble et al.,\u00a0 ). Similarly, we also demonstrated the importance of intra\u2010VN and intra\u2010DAN for the shorter window and FPN\u2010DMN for the longer window. \n\nOur findings using Glasser's 355 ROIs were replicated using Schaefer's 100 and 200 ROIs with many robust edges in intra\u2010DMN and DMN\u2010FPN (Figures\u00a0  and  ). However, some differences were present, such as the relatively increased significance of intra\u2010VN, SMN\u2010DAN, or VAN\u2010DMN, and decreased significance of intra\u2010DAN, intra\u2010FPN, or DAN\u2010VAN for Schaefer's atlas compared to Glasser's atlas (Tozzi et al.,\u00a0 ). One reason might be a lower spatial resolution in FC edges due to a reduced number of ROIs. A coarse scale of ROIs might have blurred a strong interaction of specific regions by blending them with other regions into a single big ROI (Termenon et al.,\u00a0 ). The parcellation algorithms could lead to different boundaries of ROIs depicting different neural properties (Glasser et al.,\u00a0 ; Schaefer et al.,\u00a0 ). For example, the division of Glasser's ROIs is fine\u2010grained, while Schaefer's ROIs have a coarse division at the DLPFC of FPN (Figures\u00a0 ,  , and  ). Despite overall finer ROI divisions for Glasser's ROIs, the early visual cortex at the occipital lobe is very coarsely divided in that scheme, whereas Schaefer's ROIs feature more fine\u2010grained divisions (Figures\u00a0 ,  , and  ). Furthermore, a mismatch of network boundaries between the parcellation atlases could affect the interpretation of ROIs at the network level (Figures\u00a0 ,  , and  ). \n\nWe confirmed that the fpFC obtained from the multiplication of weight feature vectors across the layers performed similarly well as the fpFC from activation maximization, one of the most popular methods for interpreting trained neural networks. This was indicated by the good agreement between the fpFCs obtained by either method (Figure\u00a0 ). Despite substantial overlap of robust edges, differences in fpFC existed between the two methods (Figure\u00a0 ). The fpFC edges between FPN\u2010VAN were found using the activation maximization approach but not by the multiplication of weight matrices. Compared to the activation maximization approach, the weight feature approach may have been lacking the nonlinear information learned by the DNN. It can be concluded that the fpFC from activation maximization would supplement the findings of fpFC from weight feature. However, the weight feature\u2010based approach has the benefit of instant availability once the DNN is trained, without requiring an additional iterative learning procedure. As an alternative to weight feature or activation maximization, explainability tools can be applied to ascertain the fpFC for each individual (i.e., output class) and identify important edges thereafter. For example, tools such as SHAP (Lundberg & Lee,\u00a0 ) or DeepLIFT (Shrikumar et al.,\u00a0 ) can be used to evaluate the influence of each feature on the model output. \n\n\n### Influence of the surface registration scheme \n  \nBecause FC was calculated using the averaged BOLD time series within each ROI, both the degree of alignment of the subject's EPI to a template and the ROI parcellation strategy may have influenced the results. We attempted to minimize potential misalignment by adopting the MMP scheme of MSMAll, which utilizes myelin maps from T1\u2010weighted and T2\u2010weighted images and rfMRI data to match a subject's native space to a group template on the surface. MSMSulc surface registration utilizes only cortical folding patterns (i.e., the sulcal depth) to match the subject's space to the group template. Thus, MSMAll is superior to MSMSulc for inter\u2010subject registration (Bijsterbosch et al.,\u00a0 ; Dubois et al.,\u00a0 ; Glasser et al.,\u00a0 ). The tvFC patterns derived using MSMAll are expected to exhibit a relatively higher degree of authentic and subtle individual\u2010specific FC originating from neural activity by substantially reducing spurious yet relatively obvious individual\u2010specific FC patterns associated with individual\u2010specific anatomical cortical folding patterns (Bijsterbosch et al.,\u00a0 ). This may explain why MSMAll registration resulted in a higher error rate than did MSMSulc (Figure\u00a0 ). Fingerprints of cortical folding patterns during infancy have been reported for use in individual identification (Duan et al.,\u00a0 ). Multimodal fingerprints utilizing both anatomical information from cortical folding patterns and functional information from tvFC patterns may thus further enhance individual identification performance. \n\n\n### Influence of confounding artifacts in the   BOLD   time series \n  \nWe attributed the improvement using ICA\u2010FIX\u2010based time series denoising to the elimination of noise that obscured authentic individual\u2010specific neural activity (Griffanti et al.,\u00a0 ; Salimi\u2010Khorshidi et al.,\u00a0 ; Xifra\u2010Porxas et al.,\u00a0 ). It is likely that cardiac and respiratory\u2010related components had already been mostly removed using ICA\u2010FIX denoising and/or band\u2010pass filtering (0.01\u20130.1\u2009Hz) (Murphy et al.,\u00a0 ) because of additional physiological noise correction via RETROICOR (Glover et al.,\u00a0 ) did not alter the performance. It may be worth employing alternative physiological noise correction techniques such as ANATICOR (Jo et al.,\u00a0 ). The fact that individuals with high head motions showed inferior performance than those with low head motions suggests that individual identification using the DNN was not based on the head motion\u2010related artifacts in the BOLD signals. \n\nUsing our DNNs, the test performance was consistently better than the validation performance (Figure\u00a0 ). To interpret this finding, we conducted PM to evaluate the possibility that the tvFC patterns from the training data (R\u2010L and L\u2010R phase\u2010encoded rfMRI data on Day 1) were more similar to the test data (L\u2010R phase\u2010encoded rfMRI data on Day 2) than to the validation data (R\u2010L phase\u2010encoded rfMRI data on Day 2). PM results indicate that the error rates for the test data (91.4, 83.2, 65.8, 31.2, and 22.2% for windows of 15\u2009s, 30\u2009s, 1\u2009min, 3\u2009min, and 5\u2009min, respectively) were lower than the error rates for the validation data (91.9, 84.2, 68.8, 40.8, and 29.5%). On Day 2, two rfMRI runs with the L\u2010R and R\u2010L phase\u2010encoding had been consecutively acquired in order (Smith et al.,\u00a0 ). Since each run lasted about 15\u2009min, there was a reasonable chance that participants might have been drowsy during the later run (i.e., the R\u2010L phase\u2010encoding) (Tagliazucchi & Laufs,\u00a0 ). Thus, the lower degree of vigilance and the higher likelihood of falling asleep might have degraded the quality of tvFCs in the validation set (Allen et al.,\u00a0 ; Tagliazucchi & Laufs,\u00a0 ). Alternatively, the effect might be due to the varying signal quality depending on the phase\u2010encoding directions. Previous research suggests that the difference between R\u2010L or L\u2010R direction of phase\u2010encoding induces asymmetric dropout in the fMRI signal (Omidvarnia et al.,\u00a0 ; Smith et al.,\u00a0 ). Specifically, the R\u2010L phase\u2010encoding direction is known to have a more severe dropout in the left hemisphere than the L\u2010R phase\u2010encoding direction, which might lead to increased distortion of the FC edges in the left hemisphere under that encoding (Mori et al.,\u00a0 ). This may therefore constitute another possible explanation of the slightly increased error rates from the test data compared to the validation data. \n\n\n### Generalization to an independent dataset \n  \nFine\u2010tuning the classification of varying numbers of target subjects can be achieved through the utilization of our pretrained DNN from the input layer to the last hidden layer, coupled with the incorporation of a new output layer. This method is effective as the fpFC for each individual can be uniquely identified through feature extraction using our DNN. More specifically, as previously noted, the fpFCs of a single subject demonstrated considerable consistency across DNN models trained on different quantities of target subjects (i.e., 300 and 600) (Figure\u00a0 ). Additionally, our findings demonstrate the capacity of our pretrained DNN to employ transfer learning for predicting a larger population of target subjects (Figures\u00a0  and  ). Given the identification of important and robust FC edges for individual classification, we postulate that our DNN model, pretrained with the HCP dataset, can be effectively adapted for classifying alternative sets of subjects by fine\u2010tuning the pretrained DNN. Nonetheless, the number of subjects substantially exceeding the quantity of hidden nodes (i.e., 2000 in our study) may lead to challenges due to the possibility of underrepresented feature representations when the hidden nodes are outnumbered by output nodes. To mitigate this, expanding the network's capacity while maintaining a constant count of input nodes by adopting techniques that ensure over\u2010complete representation at the hidden nodes (Jiang et al.,\u00a0 ; Le et al.,\u00a0 ; Vincent et al.,\u00a0 ) could be a promising strategy. \n\nWe generalized our DNN model trained using the HCP rfMRI dataset to a rfMRI dataset acquired from our institute. Despite the significantly higher error rates for the data collected from our institute compared with those for the HCP data (Figures\u00a0  and  ), the resulting performance was significantly better than would be expected by chance. Compared to the scenario of freezing hidden layers, fine\u2010tuning all layers of the DNN enabled the fine\u2010grained adjustment of model parameters for the samples in the new dataset. Consequently, fine\u2010tuning substantially reduced error rates for both datasets. Moreover, fine\u2010tuning resulted in much faster convergence but marginally lower error rates for the 15\u2010s window and similar convergence speed but substantially lower error rates for the 30\u2010s window, when compared to the DNN trained with randomly initialized parameters. This demonstrated the advantage of transfer learning in the prediction of an unseen dataset by increasing the generalization performance of the DNN. \n\nThe lower performance compared to the HCP data may not be due to the limited capability of our DNN model but rather to the lower quality of our rfMRI measurements, which were taken using inferior hardware specifications. We used a 3\u2009T Trio MRI scanner for our institute data, while the HCP data is from a Connectome Skyra scanner. We also used a 12\u2010channel head coil, while a 32\u2010channel head coil was used for the HCP data. Our acceleration factor for parallel imaging was 4 (i.e., MB factor of 2 and GRAPPA) compared with 8 for the HCP (MB factor\u2009=\u20098, GRAPPA off). Due to this lower acceleration factor, we increased the TR to 1.44\u2009s for our data (compared to 0.72\u2009s for the HCP) and increased the voxel size to 3\u2009mm isotropic (compared to 2\u2009mm isotropic for the HCP). \n\n\n### Potential weaknesses and further works \n  \nOur study does not include a comparison with recent works on individual identification via FC patterns, which presents a challenge in comparing performance metrics without bias due to variables such as dataset disparities, subject quantity, preprocessing procedures, and variations in temporal window lengths. Previous studies have demonstrated a correlation between individual\u2010specific features in static FC and fluid intelligence levels (Finn et al.,\u00a0 ), and an association between features in dynamic FCs and cognitive task performance (Fong et al.,\u00a0 ). Consequently, a promising avenue for future research is to explore the potential connections between the fpFC identified in our investigation and cognitive or behavioral attributes. Our findings, which indicate a greater similarity among genetically close individuals and inferior classification performance in classifying individuals from the HCP dataset to those from our institution using transfer learning, suggest that our DNN model could facilitate meaningful classifications. For example, a DNN trained to classify a specific clinical cohort might be deployed to determine whether new individuals correlate with that clinical group or control samples, based on the prediction performance of the transfer learning scheme. Alternatively, clustering of the fpFC patterns or hidden representations could reveal potential sub\u2010groups within the training samples, using the DNN designed for individual classification. \n\nWe only utilized BOLD signals from the cerebral cortical areas. The subcortical regions and cerebellum have also been reported to play a range of critical functional roles, from those essential for living to those associated with high\u2010level cognitive processes, such as coordinating thought (Koziol et al.,\u00a0 ). Future research that can systematically evaluate the importance of FNs across the cerebral cortex, subcortex, and cerebellum is thus warranted (Van De Ville et al.,\u00a0 ). The FC of rfMRI data measures simultaneous co\u2010activation across ROIs using BOLD signals that measure neural activity indirectly via neurovascular coupling between brain tissue and blood vessels (Huettel et al.,\u00a0 ). Therefore, it is important to determine whether the main contributor to individual identification using tvFC patterns is distinct and individual\u2010specific characteristics associated with spontaneous neural activity during a resting state or those associated with the hemodynamic response function representing neurovascular coupling (Buxton et al.,\u00a0 ; Gonzalez\u2010Castillo et al.,\u00a0 ; Handwerker et al.,\u00a0 ). \n\nBecause we used nonoverlapping windows in calculating tvFC patterns from BOLD signals, the number of training samples varied by window length since the scan duration was fixed. Despite the expectation that each FC pattern would be less noisy and clearer under a given longer versus a shorter window length, we observed a higher error rate for a 5\u2010min window (0.2%) than for a 3\u2010min window (0.1%). This may potentially be due to overfitting caused by the smaller number of training samples for the 5\u2010min window than for the 3\u2010min window. Thus, an interesting and important question is whether there is an optimal window length for individual identification using DNN if the number of samples is identical across different window lengths. Our motivation for using nonoverlapping rectangular windows was to reduce potential dependency between adjacent tvFC patterns caused by partially overlapping/shared BOLD signals between consecutive temporal windows. Alternatively, it would also be possible to adopt a standard approach of sliding a window with a temporal overlap between the BOLD signal segments since the training, validation, and test samples are separated by the fMRI sessions. This will increase the number of training samples that may work as data augmentation schemes for the DNN training, which may alleviate a potential overfitting issue. Additionally, alternative window shapes such as Tapered Gaussian or Hanning windows can be utilized to further reduce potential discontinuity and/or abrupt changes between tvFC patterns (Preti et al.,\u00a0 ). \n\nWe endeavored to validate the DNN's identification performance and to ascertain reliable edges by iterative training across numerous cycles. It is worth noting, however, that there could still be the risk of underspecification. This suggests that our model may not have found an overarching, generalized solution applicable across various domains, or there may exist alternative solutions capable of addressing the problem (D'Amour et al.,\u00a0 ). In this context, future research aimed at mitigating the risk of underspecification risk is warranted. There are important alternative options beyond our proposed method of weight sparsity optimization to prevent overfitting DNNs and to improve their performance. For example, transfer learning has become feasible owing to the availability of extensive open fMRI datasets such as ABCD (Casey et al.,\u00a0 ), UK Biobank (Sudlow et al.,\u00a0 ), and Healthy Brain Network Biobank (Alexander et al.,\u00a0 ). This approach may involve the direct transfer of task\u2010related knowledge (in our case, individual identity) through supervised pretraining\u2010based transfer learning, or the acquisition and transfer of knowledge unrelated to the downstream task from the source data via self\u2010supervised pretraining\u2010based transfer learning. Studies have shown that implementing transfer learning in deep learning models using fMRI data has enhanced model performance on downstream tasks (Hwang et al.,\u00a0 ; Li et al.,\u00a0 ; Rahman et al.,\u00a0 ). \n\nEach output unit in our DNN represents a distinct individual in our training dataset; therefore, the model cannot incorporate individuals outside of this dataset. Specifically, the FC pattern of an unrecognized individual will be classified as the most similar existing individual in the training set based on the closest matching fpFC. This approach is suboptimal for individual classification across a broader population when the trained DNN is applied. To address this shortcoming, integrating an out\u2010of\u2010distribution (OOD) detection module into the DNN has been proposed, which is in line with recent discussions estimating uncertainty within deep learning models (Hendrycks & Gimpel,\u00a0 ; Yang et al.,\u00a0 ). Enhancing our DNN with an OOD detection module that includes a confidence score could substantially improve the model's reliability for individual identification. Finally, our DNN model was a fully connected NN and thus received 1D vectorized off\u2010diagonal elements of the FC matrix as input. Our model architecture may hence be limited in accommodating the 3D structure of the brain. Future studies using a deep learning model more suitable for dealing with FC data are warranted. For example, a graph neural network incorporating the brain's topological architecture or LSTM/RNN to account for temporal changes could be used to evaluate whether the prediction performance and identified fpFCs can be further optimized (Farahani et al.,\u00a0 ; Kawahara et al.,\u00a0 ). \n\n\n\n## CONCLUSIONS \n  \nOur study offers an insight into the discovery of the intrinsic mode of the human brain using whole\u2010brain resting\u2010state FC and weight\u2010sparsity\u2010controlled DNNs. This study evaluated the identification performance of 100\u2013870 target subjects (depending on the scenario), while mainly focusing on 300 subjects. We demonstrated that a DNN can enhance accuracy by systematically comparing its performance with alternative approaches across different scenarios of window lengths and numbers of target subjects to identify. Moreover, we evaluated the reliability and robustness of the fpFC across the varying configurations and interpreted our findings in the context of functional connectome. We also raised the possibility that genotypical characteristics are reflected in the phenotypical traits of tvFC using stratified groups based on their genetic relatedness. In addition, the feasibility of generalizing a DNN model trained using HCP data to an independent dataset acquired from our institute was demonstrated in the context of transfer learning. Finally, our study is valuable in that it extends the investigation of FC fingerprinting to several hundreds of subjects (870 in this study) compared to the limited number of subjects in the previous works (Finn et al.,\u00a0 ; Liu et al.,\u00a0 ; Van De Ville et al.,\u00a0 ). The code and sample results of fpFC are available at  . \n\n\n## AUTHOR CONTRIBUTIONS \n  \nJuhyeon Lee and Jong\u2010Hwan Lee conceptualized the design of the experiments and methodology. Juhyeon Lee analyzed the data. Juhyeon Lee and Jong\u2010Hwan Lee contributed to the writing of the manuscript. \n\n\n## FUNDING INFORMATION \n  \nThis work was supported by the National Research Foundation (NRF) grant, Ministry of Science and ICT (MSIT) of Korea (NRF\u20102017R1E1A1A01077288, 2021M3E5D2A0102249311, No. RS\u20102023\u201000218987), and in part by the Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean Government (23ZS1100, Core Technology Research for Self\u2010Improving Integrated Artificial Intelligence System). \n\n\n## CONFLICT OF INTEREST STATEMENT \n  \nThe authors have no conflicts of interest regarding this study, including financial, consultant, institutional, or other relationships. The sponsors had no involvement in the study design, data collection, analysis or interpretation of the data, manuscript preparation, or the decision to submit for publication. \n\n\n## Supporting information \n  \n \n", "metadata": {"pmcid": 10789221, "text_md5": "6fe9fa21270c8996ade77212b2d9f9bb", "field_positions": {"authors": [0, 31], "journal": [32, 46], "publication_year": [48, 52], "title": [63, 166], "keywords": [180, 345], "abstract": [358, 1846], "body": [1855, 77675]}, "batch": 2, "pmid": 38096866, "doi": "10.1002/hbm.26561", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10789221", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=10789221"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10789221\">10789221</a>", "list_title": "PMC10789221  Discovering individual fingerprints in resting\u2010state functional connectivity using deep neural networks"}
