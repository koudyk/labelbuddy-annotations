{"display_title": "PMID: 20130756 batch: 9", "list_title": "batch 9 PMID20130756 Neuroplasticity Associated with Tactile Language Communication in a Deaf-Blind Subject ", "metadata": {"batch_nb": 9, "journal": "Frontiers in human neuroscience", "origin_file": "documents_season_1.jsonl", "pmcid": 2805429, "pmid": 20130756, "publication_year": 2010, "text_md5": "6f0dbf10f11c271d129cc2fbac30f9b6", "title": "Neuroplasticity associated with tactile language communication in a deaf-blind subject."}, "text": "PMID20130756        TITLE         Neuroplasticity Associated with Tactile Language Communication in a Deaf-Blind Subject        ABSTRACT           A long-standing debate in cognitive neuroscience pertains to the innate nature of language development and the underlying factors that determine this faculty. We explored the neural correlates associated with language processing in a unique individual who is early blind, congenitally deaf, and possesses a high level of language function. Using functional magnetic resonance imaging (fMRI), we compared the neural networks associated with the tactile reading of words presented in Braille, Print on Palm (POP), and a haptic form of American Sign Language (haptic ASL or hASL). With all three modes of tactile communication, indentifying words was associated with robust activation within occipital cortical regions as well as posterior superior temporal and inferior frontal language areas (lateralized within the left hemisphere). In a normally sighted and hearing interpreter, identifying words through hASL was associated with left-lateralized activation of inferior frontal language areas however robust occipital cortex activation was not observed. Diffusion tensor imaging -based tractography revealed differences consistent with enhanced occipital-temporal connectivity in the deaf-blind subject. Our results demonstrate that in the case of early onset of both visual and auditory deprivation, tactile-based communication is associated with an extensive cortical network implicating occipital as well as posterior superior temporal and frontal associated language areas. The cortical areas activated in this deaf-blind subject are consistent with characteristic cortical regions previously implicated with language. Finally, the resilience of language function within the context of early and combined visual and auditory deprivation may be related to enhanced connectivity between relevant cortical areas.           BODY            Introduction    There is well-documented evidence for extensive crossmodal neuroplastic changes following sensory deprivation in individuals with early onset blindness or deafness. Specifically, it appears that there is functional recruitment of cortical areas normally associated with the processing of the lost sensory modality by the remaining intact senses. These neuroplastic changes can be adaptive and compensatory in nature (Rauschecker,  ; Bavelier and Neville,  ; Pascual-Leone et al.,  ). For example, the occipital cortex is normally associated with processing visual information but in the case of blindness, it is recruited to process tactile information (including Braille reading and recognition of complex tactile forms and shapes), auditory information (for sound localization and speech processing), and verbal memory tasks (for reviews see Theoret et al.,  ; Merabet et al.,  ; Collignon et al.,  ). A corollary situation also appears to exist in the deaf whereby cortical regions normally associated with auditory and language processing are activated in response to vibro-tactile stimuli and observing sign language (for reviews see Bavelier and Neville,  ; Neville and Bavelier,  ). The functional relevance of crossmodal sensory cortical recruitment is supported by evidence of task specific impairment of performance when activity in these regions is disrupted either transiently by repetitive transcranial magnetic stimulation (rTMS) (Cohen et al.,  ; Amedi et al.,  ) or permanently by circumscribed lesions of these cortical areas (Hamilton et al.,  ; Hickok et al.,  ).    There is a paucity of evidence describing crossmodal neuroplasticity and function following the combined and early loss of both vision and hearing. Interestingly, results from one study has demonstrated that deaf-blind individuals actually show superior performance on tactile verbal memory recall tasks compared to sighted-hearing controls (assessed by time taken and number of items recalled) (Arnold and Heiron,  ). This superior performance has been attributed to more efficient encoding of tactile information rather than storage and retrieval (Arnold and Heiron,  ). This later finding is somewhat surprising and given the highly interdependent importance of both vision and hearing, one might expect that their combined loss would have a devastating impact on higher-order cognitive functions such as language. An alternative view would be to consider the possibility that language function is \u201cpre-wired\u201d or \u201cinnate\u201d (Hauser et al.,  ; Pinker and Jackendoff,  ). Thus, the combined early loss of both hearing and vision would not necessarily preclude an individual from developing high-level language faculties given the existence of genetically pre-determined brain circuits devoted to linguistic attributes such as basic grammar and word skills. The neural substrates associated with cognitive functions within the context of early onset and combined sensory loss remain unknown. One neuroimaging study in a late deaf-blind subject has reported that identifying words using a relatively simple form of tactile language was associated with the activation of a cortical network including inferior frontal, temporal and occipital areas (Osaki et al.,  ). However, given the subject's late onset of both deafness and blindness, it is difficult to disentangle between activation patterns related to the tactile language task performed from underlying crossmodal neuroplastic changes related to sensory deprivation itself.    Certainly, deaf-blind individuals are able to make use of numerous tactile and manual-based methods of language and communication to share information and thoughts. These modes of tactile language can differ in terms of their complexity as well as their relative difficulty to learn and master. Braille for example, is a writing and reading system featuring varied arrangements of spatially organized raised dot patterns representing letters, words, and numerals that can be identified through touch. Print on palm (POP), is characterized by the sequential tactile spelling of words (i.e. using the corresponding letter forms of the alphabet) with the index finger on the surface of the palm. Finally, American Sign Language (ASL) is a visual-gestural form of communication that is linguistically different from spoken English and has its own distinct grammatical and syntactic structure. This method of communication relies on the use of signs made with the hands, their interactions and orientation, as well as corporal movements and facial expressions. It incorporates both specific gestures as well as finger spelling of words using a manual alphabet. Deaf-blind individuals are able to communicate through the use of a haptic variant of ASL (haptic ASL or hASL) in which a receiver places their hand on those of the sender to haptically capture their form, orientation and movement so as to interpret the semantic meaning conveyed by the sign.    In this study, we used functional magnetic resonance imaging (fMRI) to compare the neural correlates associated with three forms of tactile language communication (Braille, POP and hASL) in an early blind and congenitally deaf individual. To help disentangle the neural correlates associated with tactile communication from the crossmodal changes related to multiple sensory deprivation, we also investigated hASL language processing in an individual highly proficient in this form of communication but with normal visual and auditory development. Finally, we carried out diffusion tensor imaging (DTI)-based tractography to uncover differences in brain connectivity between these two subjects that may potentially underlie neuroplastic changes associated with language function. We hypothesized that language processing in this early blind-congenitally deaf subject would be associated with activation of peri-sylvian areas within the left hemisphere that have been extensively described in normally sighted and hearing individuals. This finding would support the notion that cortical areas sub serving language function develop within the context of early visual and hearing sensory deprivation. We further hypothesized that given the early onset of his sensory deprivation, activation of early visual and auditory-language cortical areas would also be evident in response to the processing of tactile information. This latter finding would be consistent with the results of prior studies revealing cross modal plasticity in blind and deaf individuals.      Materials and Methods     Study participants    At the time of study, subject A.B. was a pre-lingually deaf and early blind, 37-year old right-handed male. Profoundly deaf due to hereditary deafness, he never developed fluent articulate speech. At the age of six, he suffered bilateral ocular trauma that rendered him profoundly blind (documented no light perception in either eye by the age of eight). He wore prosthetic eyes for cosmetic purposes. Inspection of his structural MRI did not reveal any neural abnormalities apart from bilateral ocular phthisis bulbi (i.e. end-stage atrophic degeneration secondary to the trauma). At the age of two, he was taught a few words by his mother in his native Bengali through associating the vibration sensations generated by her throat. Following his vision loss, he remained confined at home for 4\u2009years but later enrolled at the Perkins School for the Blind (Watertown, MA, USA) where he received formal instruction including the English alphabet, Braille and haptic ASL (hASL). He graduated from high school and later received a bachelor's degree in political science and currently works as an adaptive technology instructor for the deaf-blind. In his daily activities, he routinely uses Braille and communicates with others through hASL and\u2009POP.    Subject S.W. was a 24-year old right-handed female with normal corrected vision and hearing and fluent articulate speech. Three years prior to this study, she met subject A.B. while training to become an English to ASL interpreter for the deaf-blind. Since that time, she has had extensive experience with hASL as well as communicating with subject A.B. At the time of study, she did not know how to read Braille nor receive POP words proficiently. It is notable that similar behavioral studies have employed sighted individuals to serve as control subjects. However, training these controls to carry out the hASL task at a high and comparable level of proficiency was not feasible. Thus, we felt that subject S.W. would serve as an ideal control subject in order to draw appropriate inferences regarding activation related to the task of hASL compared to the effect of sensory deprivation alone.    Both subjects provided written informed consent before the participating in the study and the experiments were carried out in accordance to the Declaration of Helsinki and approved by the institutional review board of Beth Israel Deaconess Medical Center, Boston, MA, USA.      Behavioral tasks and study design    Three conditions were presented following a randomized blocked design for each of the tactile language modalities investigated; (1) presentation of words, (2) presentation of non-words, and (3) rest. Conditions were signaled by a specific and pre-determined cue (i.e. \u201cword\u201d was signaled by a tap on the palm, \u201cnon-word\u201d by a light squeeze of the finger, and \u201crest\u201d by a light squeeze of the hand). The task commands and order were presented auditorilly through headphones worn by either the experimenter or the signing interpreter (depending on the task to be performed). Each stimulus presentation lasted 3\u2009s with six presentations per block (total of 18\u2009s/block). Each block was repeated four times per run for a total run length of 216\u2009s. Four runs of each tactile language modality (hASL, POP, and Braille) were presented. Behavioral data was also\u2009collected.    English Braille text (grade II) was presented on a series of cards and prepared using a standard Braille text embosser. The Braille reading task was carried out by presenting a series of common words (a mixture of both concrete and abstract terms) containing four or five letters. Subject A.B. was instructed upon receiving the appropriate cue to use his left index finger (his preferred hand for reading and tactile communication) and sweep the raised dot patterns once in order to read the word displayed by the experimenter. For words presented with Print on palm (POP), an interpreter (subject S.W.) held subject A.B.\u2019s left index finger and upon cue, presented a series of English words by spelling out the letters sequentially by stroking the surface of her palm. Words presented by hASL were carried out by having subject A.B. rest his left hand on the signing hand of the interpreter (subject S.W.) so as to receive a series of signed words presented on cue. To monitor behavioral performance and maintain attention throughout the scanning runs, the subject was instructed to indicate whether the word presented started with a consonant (index finger) or vowel (second finger) using a two-key button box placed under the right hand.    On a second day of experimentation, we repeated the hASL task with control subject S.W.; the signing interpreter used in the previous set of experiments. Note that this subject was unable to read Braille and receive words with POP thus only the hASL task was performed. Individuals who are able to receive words by hASL alone (i.e. without the benefit of sight) and at a highly proficient level are not easily accessible. Thus, to facilitate the experiment, we employed an experienced signer for the deaf to deliver words by hASL following the same procedure as was used with subject A.B. The control subject was blindfolded and wore earplugs throughout the entire experiment and carried out the same behavioral task.      Neuroimaging and data acquisition    Blood Oxygen-Level Dependent (BOLD) fMRI measurements were performed using a whole-body Achieva Intera 3T scanner equipped with a eight channel SENSE head coil (Phillips Medical Systems, Bothell, WA). 3D anatomical volumes were collected using a T1 SPGR sequence at a resolution of 1.2\u2009\u00d7\u20091.0\u2009\u00d7\u20091.0\u2009mm. Functional data was acquired using a multi-slice gradient echo (GE) echo-planar imaging (EPI) sequence with the following parameters: TR\u2009=\u20092\u2009s, TE\u2009=\u200928\u2009ms, flip angle\u2009=\u200990\u00b0, imaging matrix\u2009=\u200980\u2009\u00d7\u200980 zero filled to 96\u2009\u00d7\u200996, FOV\u2009=\u200923\u2009cm. Twenty-five slices with a thickness of 4\u2009mm and 0.5\u2009mm gap were oriented axially and covering the whole brain. Functional data collection was conducted using a block design with alternating conditions (words, non-words and rest for each form of tactile communication). Four separate functional runs for each tactile language modality were used in order to simplify the task instructions for subject A.B. and improve the power for fMRI analysis. Diffusion tensor imaging (DTI) was acquired in 15 independent directions with a b factor of 1000\u2009s/mm2using a spin-echo EPI sequence. In both study subjects, three runs of 51 axial slices with in-plane resolution of 1.875\u2009\u00d7\u20091.875\u2009mm and 3-mm slice thickness were acquired and averaged together to create a single volume for each subject for later analysis.      Analysis    Analysis of functional neuroimaging data was carried out using Brain Voyager QX 1.9 software package (Brain Innovation, Maastricht, Netherlands) and Freesurfer image analysis suite (Dale et al.,  ; Fischl et al.,  ) combined with the FS-FAST functional analysis stream (Massachusetts General Hospital, Charlestown, MA, USA). A high-resolution anatomical T1-weighted anatomical volume was used to reconstruct a cortical surface mesh and then inflated to allow viewing of sulcal activation. Functional preprocessing included 3-D motion correction, high pass filtering to remove temporal linear trends, correction for slice time acquisition and spatial smoothing (Gaussian kernel, 5.0\u2009mm FWHM). Neither subject exceeded a maximum motion tolerance of 3\u2009mm in any direction. Following co-registration of the preprocessed functional image with the high-resolution anatomical, data was transformed into standard talairach space (Talairach and Tournoux,  ). Voxel time courses for each subject were fit using a general linear model (GLM) for further statistical analysis. Each experimental condition was modeled by a boxcar regressor matching the condition time course. Boxcar regressors were then smoothed by a canonical hemodynamic response function (Boynton et al.,  ). Individual subject maps for each contrast of interest (described in results section) were generated by projecting the volume of significance values resulting from the GLM onto the reconstructed cortical surface mesh for each hemisphere. A Bonferroni-corrected statistical threshold criterion of   p   \u2009<\u20090.01 was used. For the region of interest (ROI) analysis, areas were anatomically selected on each subject's inflated cortical surface (see inset of Figure  ). The voxels which intersected these regions of interest were averaged together at each time point and subjected to the same GLM analysis that was performed for the voxel-wise analysis. The resulting parameter estimates were normalized to percent signal change units for graphical display. For the conjunction analysis, positive significance values from each experiment (Braille, Print on Palm and hASL) were overlaid on subject A.B.\u2019s reconstructed cortical surface using custom software to color-code each task (Swisher et al.,  ).      DTI probabilistic tractography    Cortical reconstruction and volumetric segmentation was performed with Freesurfer image analysis suite (see above). The Freesurfer suite provides an automated labeling of the cerebral cortex into units based on gyral and sulcal structure (Fischl et al.,  ; Desikan et al.,  ). Diffusion weighted images were analyzed using FMRIB's Diffusion Toolbox (Behrens et al.,  ,  ,  ) following initial averaging of runs carried out by the scanner image acquisition software. Data were corrected for eddy currents, fit to tensors and probabilistic diffusion model and registered to the Freesurfer anatomical space. Calcarine sulcus regions of interest (Fischl et al.,  ; Desikan et al.,  ) were selected and projected 3\u2009mm into white matter along each ROI vertex normal to generate DTI seeding points. Probabilistic streamline tractography was then performed on these seeds (default \u201cprobtrackx\u201d settings: 5000 samples, 0.2 curvature threshold, 2000 steps, 0.5-mm step length, terminating pathways which loop back on themselves). Probabilistic tractography results at the grey/white matter boundary were then rendered on inflated Freesurfer surfaces for visualization.       Results    Both the deaf-blind (A.B.) and control (S.W.) subjects were able to carry out the tactile language tasks designed for this study. Behavioral performance (correct identification of presented words) for subject A.B. was 86.7%, 94.8%, and 78.1% correct for hASL, Braille and POP respectively and 78.1% correct for the control subject on the hASL task. During post-experiment debriefing, subject A.B. reported that he was able to identify all the words easily and, even when explicitly asked, he denied visually imagining the individual characters or gestures that comprised the words presented.    Tactile identification of hASL words (in contrast to rest) for subject A.B. was associated with robust bihemispheric activation that included multiple sensory, motor and language areas (see Figure  C and Table  ). This activation included occipital areas extending ventrally to the fusiform gyrus, laterally along the inferior temporal gyrus, and dorsally along the intraparietal sulcus. The occipital activation encompassed the calcarine sulcus and extrastriate areas, including parts of the lateral-occipital complex (LOC), the fusiform gyrus, and middle-temporal areas (MT+). Parietal activation spread along the intraparietal sulcus (IPS) and into the superior parietal lobule (SPL). Activation was also found in peri-sylvian regions anatomically corresponding to auditory and language-associated areas of the posterior superior temporal lobe, including the superior-temporal gyrus (BA 22,42) and Wernicke's area (BA 39,40) as well as inferior frontal cortex including Broca's area (BA 44,45). Finally, robust activation was also observed in primary motor, premotor and primary somatosensory cortices bilaterally. The contrasts of Braille words v.s. rest (Figure  A) and print on palm (POP) words v.s. rest (Figure  B) revealed broadly similar patterns of activation across occipital, temporal and frontal areas but with varying degrees of activation along posterior superior-temporal regions. In the hearing-sighted control subject (S.W.), identifying words through hASL (compared to rest) revealed bilateral activation of ventral occipital-temporal areas, motor and somatosensory regions, as well as within the inferior frontal cortex including Broca's area (BA 44,45). However, activation was notably less robust within regions of the occipital pole, intraparietal sulcus, and areas along the superior-temporal sulcus (Figure  D).    In order to address whether the observed patterns of activation were specific to the linguistic content of the words presented (as opposed to non-specific task activation due to haptic or tactile exploration), we carried out a further analysis of a word vs. non-word contrast for the hASL task in both subjects (Binder et al.,  ). In subject A.B., this contrast confirmed strong occipital activation as well as a left-lateralized activation of posterior superior-temporal areas, including superior-temporal gyrus and Wernicke's area and Broca's area within inferior frontal cortex (Figure  and Table  ). However in this contrast, activation was less apparent along more anterior regions of the superior-temporal gyrus including auditory association area (BA42). The same hASL word v.s. non-word contrast in control subject S.W. confirmed strong activation of ventral occipital-temporal areas and left-lateralized activation within inferior frontal cortex. Again, in comparison to deaf-blind subject A.B., activation within the occipital pole, intraparietal sulcus, and along the posterior superior-temporal cortex was notably absent (Figure  ).    We carried out a conjunction analysis to further characterize areas of common and differential activity associated with each form of tactile communication in subject A.B. (Figure  ). Areas common to all three forms of tactile communication (Figure  ; areas colored in white) included early occipital visual areas, inferior frontal cortex, intraparietal sulcus, superior parietal lobe, primary somatosensory and premotor areas bilaterally. Activation along the middle and superior-temporal sulcus was largely associated with hASL and POP tasks, but less so for Braille.    To compare the magnitude of activation across cortical areas, we next ROI analysis for each form of tactile communication (Figure  ). To facilitate comparisons, cortical regions of interest were selected based on their known association with the visual, somatosensory, auditory and language systems. Subject A.B. showed activation along the calcarine sulcus across all three forms of tactile communication. Middle-temporal areas (\u201cMT+\u201d) were activated during the hASL and POP tasks, but less so by Braille reading. Finally, higher-order visual areas along the fusiform gyrus (\u201cfusiform\u201d) and in lateral occipital (LO) cortex (\u201clateral visual\u201d) were strongly activated across all three tasks (with the exception of the middle-temporal area for the Braille task). In contrast, the hASL task in the control subject S.W. did not lead to robust activation of primary visual, lateral occipital, nor fusifom areas. However, activation was evident within middle-temporal areas (MT+) in both study subjects. A trend towards left-lateralized activation within Broca's area was evident across all three forms of tactile communication in subject A.B. as well as for hASL in the control subject S.W. In subject A.B., areas within posterior superior-temporal cortex (including BA 22) and Wernicke's area showed activation for the hASL and POP tasks, but only minimal activation for the Braille task. Control subject S.W. showed left-lateralized activation of Wernicke's area, but superior-temporal activation (BA 22) was absent.    Finally, we completed a probabilistic diffusion tensor tractography analysis in both study subjects. Using the calcarine sulcus as a seeding region in both hemispheres, evidence of enhanced connectivity with posterior superior-temporal regions of the left hemisphere (particularly along the supramarginal gyrus) was observed in subject A.B. as compared with subject S.W. (Figure  ). The probabilistic connectivity profile was otherwise largely similar in both subjects, revealing connections along the ventral temporal surface to the temporal pole as well as medial occipital lobe\u2009connections.      Discussion    In this study, we employed fMRI to identify and compare networks of cortical activation associated with multiple methods of tactile communication in a pre-lingual, deaf-blind subject who has developed a high level of language function. Studying the neural correlates associated with tactile language processing in this individual provided a unique opportunity to examine neural systems supporting language function within the context of early onset and profound vision and hearing loss.    In deaf-blind subject A.B., identifying words through three forms of tactile communication (Braille, POP and hASL) was associated with robust activation of occipital cortex (including calcarine-striate and extrastriate regions), left-lateralized activation within posterior superior-temporal areas (including Wernicke's area and superior-temporal gyrus; BA 22), and inferior frontal cortical areas (including Broca's area). Furthermore, this left-lateralized pattern was evident despite the fact that subject A.B. read and received the presented words using his left hand. The findings are consistent with the general dominant role of the left hemisphere for language function (Price,  ). Common areas of activation across the three tactile forms of communication were localized primarily within the occipital lobe, along the ventral surface of occipital-temporal cortex and intraparietal sulcus, and inferior frontal cortex.    The patterns of activation observed in subject A.B. are in agreement with reports of crossmodal sensory recruitment associated with tactile and language related tasks following sensory deprivation. For example, activation of the occipital cortex in the blind has been associated with Braille reading (Sadato et al.  ,  ; Burton et al.,  ), fine spatial tactile discrimination (Stilla et al.,  ), tactile moving stimuli (Goyal et al.,  ), and haptic object identification (Amedi et al.,  ; Pietrini et al.,  ). In the deaf, activation of auditory and language related areas has also been reported with viewing sign language (Bavelier et al.,  ; Neville et al.,  ; Nishimura et al.,  ; Fine et al.,  ) including Broca's area (MacSweeney et al.,  ,  ) as well as in response to vibro-tactile stimuli (Auer et al.,  ). It is also of note that activation of auditory and language areas has been reported for the mental rehearsal of words and inner speech (McGuire et al.,  ; Paus et al.,  ).    By comparison, identification of words presented by hASL in the sighted control subject S.W. was also associated with a left-lateralized pattern of activation within inferior frontal language areas (Broca's area; BA 44,45) and to a lesser extent, Wernicke's area (BA 39,40). However, in contrast to what was observed in the deaf-blind subject, activation within occipital cortical areas was largely absent (with the exception of MT+; see below). Thus, the common areas of activation implicating frontal and posterior superior temporal language-associated areas may underlie a common network for the processing of words, even in the context of combined early onset visual and auditory sensory deprivation. The role of Broca's and Wernicke's areas in language processing in hearing and deaf subjects is well established (Price,  ; Price et al.,  ; Emmorey et al.,  ). However, the additional recruitment of cortical areas for tactile communication observed in this deaf-blind subject (specifically, occipital and associative auditory cortical areas) may reflect crossmodal neuroplastic changes related to his early onset sensory deprivation and in a broad sense, consistent with previous findings in both the blind and deaf population.    Areas activated across subject A.B.\u2019s three tactile forms of communication likely include components of a general linguistic processing network as well as regions implicated in non-linguistic crossmodal tactile information processing. Within the occipital cortex, Braille, POP and hASL were all associated with robust activation implicating primary visual (i.e. calcarine-striate), LO and fusiform areas. Activation of primary visual areas has previously been reported during Braille reading (Sadato et al.,  ,  ; Burton et al.,  ) while LO cortex has been implicated in tactile and haptic object recognition and has been described as a multi-modal cortical area responsible for general shape processing (Amedi et al.,  ,  ). Finally, the fusiform gyrus has also been previously shown to be activated in studies investigating Braille reading in the blind (Sadato et al.,  ; Burton et al.,  ; Sadato  ). Interestingly, this region also appears to play an important role in reading function in the intact brain (see  on \u201cword form area\u201d, Gaillard et al.,  ).    Differential patterns of activity across all three modes of tactile communication were found primarily with regards to area MT+ and nearby superior-temporal areas. These differences are possibly related to differing degrees of tactile motion processing associated with each task. Unlike POP and hASL, Braille reading was not associated with strong activation within these areas consistent with previous studies investigating Braille reading in the blind (Sadato et al.,  ,  ; Burton et al.,  ). Conversely, the comparatively robust activation found within these areas with POP and hASL may be explained by the fact that both these modes of communication are associated with more complex tactile-related movements to form letters and signs. Indeed, activation of MT+ has been previously reported in the blind during active processing of tactile motion (Goyal et al.,  ). As with language-associated areas, there also appeared to be a left-lateralized activation within MT+ for identifying words. Interestingly, previous reports have shown that both deaf and fluent hearing signers are better at detecting a motion stimulus within the right visual field. This right field advantage has been explained by the proximal association of contralateral motion-sensitive areas to language processing areas within the left hemisphere (Bosworth and Dobkins,  ,  ).    The underlying mechanisms associated with crossmodal neuroplasticity remain largely unknown. Neuroplastic processes may follow a combination of rapid changes such as an unmasking of existing connections followed by more long-term structural modifications including dendritic growth and arborization that potentially culminate in the establishment of novel pathways (Pascual-Leone et al.,  ). This \u201cre-wiring\u201d reflects ongoing changes at both cortical and sub-cortical levels responding to shifts in afferent input and efferent demand (Pascual-Leone et al.,  ). While the data presented here is only from one pair of study subjects, the use of DTI-based imaging may help to shed light on this issue. Probabilistic tractography in the deaf-blind subject A.B. revealed greater connectivity between calcarine-striate and temporal cortical areas within the left hemisphere which was not observed in the control subject. These enhanced occipital-temporal cortical connections may reflect shared spatial-temporal computations being performed between these areas. It is notable that this pattern of connectivity is in agreement with recent anatomical evidence describing direct occipital-temporal cortical connections in non-human primates (Rockland and Van Hoesen,  ; Cappe and Barone,  ; Schroeder and Foxe,  ).    To our knowledge, only one other study has investigated the neural correlates of language processing in a deaf-blind individual. Osaki et al. (  ) employed a combined neuroimaging approach using magnetoencephalography (MEG) and positron emission tomography (PET) to localize cortical areas associated with identifying words using a relatively simple tactile language in a subject who lost both sight and hearing late in his lifetime. Tactile presentation of three-letter Japanese words (using a system of touching and stroking the fingertips of the subject to represent different consonants and vowels) was associated with activation of a network of cortical areas, including left postcentral gyrus, bilateral inferior frontal gyri, left posterior temporal lobe, right anterior temporal lobe, and bilateral middle occipital gyri (Osaki et al.,  ). The reported areas of activation are largely consistent with those identified in this study, as is the lack of occipital activation observed during tactile linguistic processing in normally sighted volunteers (Osaki et al.,  ). However, there are methodological differences that may help to further clarify functional role of the areas activated during tactile language processing. First, the subject of the Osaki study was a post-lingual deaf and late-blind individual. Thus, there remains the possible confound that the activation patterns reported may be related to mental imagery given his prior and long-standing visual and auditory experience (as the authors themselves conclude Osaki et al.,  ). Indeed, previous studies have demonstrated occipital cortical activation during tasks involving mental imagery (Kosslyn et al.,  ; Ganis et al.,  ) as well as frontal language and temporal auditory activation during mental word rehearsal (McGuire et al.,  ; Paus et al.,  ). However, the early blind and congenitally deaf status of our subject A.B. help to rule out visual or auditory imagery as potential confounders in explaining the observed recruitment of occipital, temporal and frontal areas. In future studies, establishing the functional role of the recruited cortical areas during tactile communication in blind-deaf subjects like A.B. might be accomplished by using transcranial magnetic stimulation (TMS) to target and transiently disrupt the function of these areas and observe their effect on behavior (Pascual-Leone et al.,  ). Second, the lack of observed activation in normally sighted-hearing individuals in the Osaki study may be simply related to the fact that these control subjects were not proficient in the tactile language task employed (Osaki et al.,  ). In our study, we compared activation associated with tactile presentation of words using hASL in a normally sighted and hearing interpreter who is highly proficient with this form of tactile communication. The fact that our subjects were closely matched in terms of behavioral performance helps to disentangle patterns of activation associated with crossmodal recruitment (following sensory deprivation) and networks associated with the language task in question. In the control subject of this study (subject S.W.), robust activation was evident within a left-lateralized network implicating Broca's area but not within occipital and posterior superior-temporal areas compared to the deaf-blind subject. This suggests that the functional recruitment of these areas may indeed be related to specific crossmodal recruitment related to the development of language skills.    Our study provides additional evidence regarding the neural correlates associated with different types of tactile language processing within the context of combined visual and auditory deprivation. However, we recognize that there are shortcomings that need to be addressed. Recruiting additional deaf blind and control subjects may contribute to the overall generalizibility of the results however, it is important to recognize that deaf-blind individuals represent a very heterogeneous population in terms of etiology given that hearing and vision loss are often of different causes and not co-incident. Furthermore, early onset hereditary causes of deaf-blindness (such as Usher syndrome) can be associated with cognitive and developmental delays (Ronnberg and Borg,  ). Second, it is very difficult to find deaf-blind individuals with high language function and documented history of early sensory deprivation as well as interpreters to serve as control subjects who are highly proficient with hASL (i.e., able to not only sign, but also receive words in hASL without the use of vision). Typically, ASL is interpreted through sight and most signing interpreters are not familiar with the haptic form of ASL. In this study, we chose subjects A.B. and S.W. given their long-standing relationship and facility in conversing through haptic ASL. Third, we acknowledge that the conclusions drawn from the DTI study are also limited. While not the main focus of the investigation, the differences in connectivity patterns observed generate interesting hypotheses that can be contrasted with the overall patterns of activation obtained by fMRI. Certainly, further studies in larger populations and employing this parallel functional and connectivity data approach are warranted. Fourth, in subject A.B., it was noted that the word v.s. non-word contrast for the Braille and POP tasks did not register an overall significant activation compared to the hASL. This issue may be related to the nature and the strategies employed to carry out the different tasks. One likely explanation is that subject A.B. is able to carry out the behavioral task in Braille and POP simply by recognizing the first letter of the word rather than their \u201cholistic capture\u201d as required for signs using hASL. Again, specifically designed studies implementing careful control tasks (particularly word vs. non-word comparisons) are likely to clarify this issue in future studies (Binder et al.,  ). Finally, while subject A.B. is congenitally and profoundly deaf, his profound vision loss occurred later in his life, between the ages of six and eight. It is therefore possible that early in his childhood he may have acquired some vocabulary through visual association. Thus, we cannot exclude with certainty the possibility that his prior visual experience may have contributed to the development of his language function. However, it is known that his knowledge of the English language, Braille, POP and hASL were all acquired   after   his vision loss. This issue is relevant within the context of crossmodal plasticity as it relates to the acquisition of language. The crossmodal recruitment of auditory and language related areas in the deaf has been attributed to the fact that ASL is highly visual in nature (Fine et al.,  ). Certainly, ASL relies heavily on facial expressions to provide emphasis and to signal lexical and syntactic structures. Thus, picking up on these facial expressions is crucial for learning sign language (Neville et al.,  ; Fine et al.,  ; MacSweeney et al.,  ). Furthermore, it has been suggested that acquiring proficient language function from lip-reading, sign language and from a cochlear implant all benefit from mutual reinforcement between occipital and temporal auditory-language areas (Giraud et al.,  ; Giraud and Truy,  ). However, the deaf-blind subject studied here did not learn the haptic form of ASL (nor the English language) through visual nor auditory associative links with tactile spatial patterns or hand gestures and formations. One might suspect that learning ASL without the benefit of sight might preclude a deaf-blind individual from understanding linguistic nuances and subtleties. Yet, he functions at a very high level and is highly proficient, capturing the subtle emotional inflections of language. Given that he acquired language essentially through touch alone and without visual and auditory reinforcement, we propose that his language abilities reflect the recruitment of occipital and frontal-temporal cortical areas and the exploitation of intrinsic language processing within these regions.    The resilience of language function and involvement of classical language areas (including left-lateralized frontal and peri-sylvian regions) during tactile communication in subject A.B. is consistent with hypotheses of an innate language function (Hauser et al.,  ). Noppeney et al. (  ) have specifically investigated whether the lack of visual experience in individuals with early onset blindness alters underlying neural systems related to the retrieval of semantic information. Similar to the results reported here, these authors found fMRI activation of a \u201ccore\u201d language network of left-lateralized frontal-temporal areas in both early blind and sighted subjects with blind subjects showing additional activation of extrastriate regions of occipital cortex (Noppeney et al.,  ). These authors further suggest that innate and possibly epigenetic mechanisms may explain the observed resilience of specific language functions in the context of early onset visual deprivation (Noppeney and Price,  ; Noppeney et al.,  ). In this study, we investigated the neural correlates associated with processing words presented through three modes of tactile communication (Braille, POP and hASL) in an early blind, pre-lingually deaf subject. Extrapolating these findings across all deaf-blind subjects would certainly be premature. However, this issue notwithstanding, the findings obtained from this unique individual and contrasted with a sighted-hearing control allows several important observations. First, robust patterns of activation (including occipital, posterior superior temporal and inferior frontal language areas) were observed for all tactile forms of communication (including common and differential areas they may reflect subtleties associated with each mode of tactile communication used. Second, activation in a normal hearing and sighted interpreter proficient with hASL showed common areas of activation implicating frontal language areas (Broca's area; BA 44,45) yet robust occipital cortical recruitment was not evident. Finally, the recruitment of sensory-motor processing areas previously identified following sensory deprivation and potential differential patterns of connectivity between these areas help strengthen the conclusion that occipital, parietal, temporal and frontal areas (normally attributed with visual, spatial, auditory and language processing respectively) are recruited for tactile language processing in response to early onset and profound combined visual and auditory deprivation.      Conflict of Interest Statement    The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.", "utf8_text_md5_checksum": "6f0dbf10f11c271d129cc2fbac30f9b6"}
