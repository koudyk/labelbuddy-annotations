{"text": "Heitmann, Stewart and Breakspear, Michael\nNetw Neurosci, 2018\n\n# Title\n\nPutting the \u201cdynamic\u201d back into dynamic functional connectivity\n\n# Keywords\n\nDynamic functional connectivity\nNonlinear dynamics\nMetastability\nMultistability\n\n\n# Abstract\n \nThe study of fluctuations in time-resolved functional connectivity is a topic of substantial current interest. As the term \u201cdynamic functional connectivity\u201d implies, such fluctuations are believed to arise from dynamics in the neuronal systems generating these signals. While considerable activity currently attends to methodological and statistical issues regarding dynamic functional connectivity, less attention has been paid toward its candidate causes. Here, we review candidate scenarios for dynamic (functional) connectivity that arise in dynamical systems with two or more subsystems; generalized synchronization, itinerancy (a form of metastability), and multistability. Each of these scenarios arises under different configurations of local dynamics and intersystem coupling: We show how they generate time series data with nonlinear and/or nonstationary multivariate statistics. The key issue is that time series generated by coupled nonlinear systems contain a richer temporal structure than matched multivariate (linear) stochastic processes. In turn, this temporal structure yields many of the phenomena proposed as important to large-scale communication and computation in the brain, such as phase-amplitude coupling, complexity, and flexibility. The code for simulating these dynamics is available in a freeware software platform, the Brain Dynamics Toolbox. \n   Author Summary  \nThe study of network fluctuations in time-resolved functional connectivity is a topic of substantial current interest. However, the topic remains hotly disputed, with both positive and negative reports. A number of fundamental issues remain disputed, including statistical benchmarks and putative causes of nonstationarities. Dynamic models of large-scale brain activity can play a key role in this field by proposing the types of instabilities and dynamics that may be present. The purpose of the present paper is to employ simple dynamic models to illustrate the basic processes (\u201cprimitives\u201d) that can arise in neuronal ensembles and that might, under the right conditions, cause true nonlinearities and nonstationarities in empirical data. \n \n\n# Body\n \n## INTRODUCTION \n  \nThe brain is a dynamic machine par excellence, tuned through the principles of self-organization to anticipate the statistics and movement of the external milieu (K. Friston,  ; Skarda & Freeman,  ). Its unceasing dynamics and cycle of prediction-action-perception mark it as distinct from even the most advanced deep learning platforms despite impressive advances in machine learning. Systems neuroscience is likewise incorporating dynamic algorithms into its core methodologies (Breakspear,  ; K. J. Friston, Harrison, & Penny,  ), in the design of hierarchical models of perception and inference (Mathys, Daunizeau, Friston, & Stephan,  ); dynamic approaches to clinical disorders (Roberts, Friston, & Breakspear,  ); dynamic models of functional neuroimaging data (Stephan et al.,  ; Woolrich & Stephan,  ); and dynamic frameworks for the analysis of resting state fMRI data (Deco, Jirsa, & McIntosh,  ). Dynamic models are at the heart of the distinction between functional connectivity and effective connectivity (see  ; K. J. Friston,  ) and can help disambiguate correlated activity due to mutual interactions from that caused by input from a common source. \n\n  \n\nResearch into the dynamics of resting-state fMRI data is currently very active, and takes its form largely through the study of nonstationarities in time-resolved functional connectivity (Chang & Glover,  ; Hutchison et al.,  ; Zalesky, Fornito, Cocchi, Gollo, & Breakspear,  ). However, the topic remains hotly disputed, with both positive (Abrol et al.,  ; Nomi et al.,  ; Zalesky et al.,  ) and negative (Laumann et al.,  ) reports. In addition, fundamental statistical issues continue to be contested, including the utility of sliding-window analyses (Hindriks et al.,  ; Leonardi & Van De Ville,  ; Zalesky & Breakspear,  ) as well as core definitions of stationarity (Liegeois, Laumann, Snyder, Zhou, & Yeo,  ). Another issue of substance pertains to the   causes   of putative nonstationarities (assuming they exist); in particular, whether nonstationarities reflect subtle cognitive processes (random episodic spontaneous thought, i.e., \u201crest\u201d; Breakspear, Williams, & Stam,  ); whether they are slower processes that nonetheless retain cognitive salience (such as drifts in attention and arousal; Kucyi, Hove, Esterman, Hutchison, & Valera,  ); or whether they are nuisance physiological and head motion covariates that have been inadequately removed from fMRI   (Laumann et al.,  ). Regardless of these debates, the overarching motivation of the field is that resting-state brain activity is endowed with functionally relevant complex neuronal dynamics\u2014either as the substrate for ongoing \u201cthought,\u201d or to prime the cortex for perception and action (K. Friston, Breakspear, & Deco,  ). So, the central question seems not whether such neuronal dynamics exist, but to what extent they can be detected in functional neuroimaging data. \n\nDynamic models of large-scale brain activity can play a key role in this field by proposing the types of instabilities and dynamics that may be present (Cabral, Kringelbach, & Deco,  ; Deco, Jirsa, McIntosh, Sporns, & K\u00f6tter,  ; Gollo, Zalesky, Hutchison, van den Heuvel, & Breakspear,  ; Hansen, Battaglia, Spiegler, Deco, & Jirsa,  ; C. J. Honey, K\u00f6tter, Breakspear, & Sporns,  ). The purpose of the present paper is to employ simple dynamic models to illustrate the basic processes (\u201cprimitives\u201d) that can arise in neuronal ensembles and that might, under the right conditions, cause true nonlinearities and nonstationarities in empirical data. In doing so, we also aim to disambiguate some key terms in the field: first, the differences between nonstationarity and nonlinearity\u2014both can herald underlying dynamics, cause rejection of common nonparametric nulls, and (as we will see) occur individually or together; second, the distinctions between key terms in dynamic systems theory, especially the catchphrase terms of   and   (which are often used interchangeably). Hopefully this is a constructive step toward a more definitive resolution of the uncertainties in the field. \n\n\n## METHODS \n  \n### Coupled Dynamical Systems \n  \nTo illustrate the breadth of synchronization dynamics, we study an autonomous, nonlinear system of coupled  . This model has been previously employed to study whole-brain dynamics (C. J. Honey et al.,  ; Zalesky et al.,  ). The system is composed of local subsystems (\u201cneural mass,\u201d or nodes) coupled together to form a larger ensemble (for a review, see Breakspear,  ). Each local node comprises a population of excitatory neurons and a slow variable incorporating the (simplified) response of a local inhibitory pool of neurons. Inhibitory activity is driven by local excitatory activity, to which it feeds back via a slow inhibitory current. The dynamics of neural masses are determined by a conductance-based process, with fast (instantaneous) sodium membrane currents and slower potassium currents. The dynamics within each node takes the form of a low-dimensional nonlinear differential equation, where   X   is a vector of the system\u2019s variables (cell membrane potentials, firing rates, membrane channel currents). The system has a number of physiologically derived time-invariant parameters   a  , such as synaptic connection strengths, membrane channel conductances, and neural gain (Breakspear, Terry, & Friston,  ). Depending upon the choice of these parameters, single-node dynamics may range from a steady-state fixed-point attractor, to fast periodic oscillations and  . Here we choose the parameters so that the autonomous behavior of each neural mass is described in mathematical terms as a nonlinear dynamical system with a chaotic attractor. This chaotic regime arises from the intrinsic constants and variables within the local (uncoupled) neural population\u2014specifically from the mixing of the fast timescales of the pyramidal cells and the slow responses of the inhibitory population. These dynamics do not depend upon the coupled interactions. \n\nA mesoscopic neural ensemble is constructed by permitting two or more of such local neural masses {  X  ,   X  , \u2026} to interact through a coupling function (Breakspear & Stam,  ). These interactions are parameterized by the matrix of internode coupling   C   = {  c  }, where   i   is the source node and   j   is the receiver node. Connections may be reciprocal but asymmetric (  c   \u2260   c  ). Hence each node\u2019s dynamics are governed by where   i   indexes the node and the coupling function   H   embodies the nature of the internode influences among all nodes in the system, that is, the model of effective connectivity. Internode coupling in this framework is classically composed of excitatory-to-excitatory connectivity. However, there are no restrictions on the general nature of   that prohibit inhibitory internode coupling parameterized in   C  . \n\nFor simulations of empirical macroscopic network behaviors, the connectivity   C   between neural masses can be defined by   (Sporns, Tononi, & K\u00f6tter,  ) representing the network of white matter fiber tracts mediating internode connectivity in the brain. The structural connectomes can be obtained from postmortem tracing studies (Stephan et al.,  ) or from in vivo human MRI-based tractography. Because of technical limitations of current state of the art tractography, connectivity matrices derived from MR-based tractography are symmetric   c   =   c  . \n\nAlthough we employ a particular model to illustrate synchronization dynamics, many of the underlying principles hold for any local system with chaotic dynamics (Pikovsky, Rosenblum, & Kurths,  ). Periodic dynamics permit a narrower range of dynamic scenarios. For most of our simulations, we focus on dyads (pairs) of coupled nodes. Complex dynamics on motifs with three or more nodes derive from the principles of two nodes, but add an additional layer of complexity, depending on their connectivity as well as the nature of axonal time delays (Atay,  ; Cabral, Luckhoo, et al.,  ; Deco et al.,  ; Gollo & Breakspear,  ; Gollo, Mirasso, Sporns, & Breakspear,  ). For the moment, we do not consider the role of time delays in the resulting synchronization dynamics. We return to these issues below. \n\nAll simulations in this paper are performed using the Brain Dynamics Toolbox ( ), an open-source Matlab-based toolbox for interactive simulations of neuronal dynamics (as described in Supplementary Information II, Heitmann & Breakspear,  ). The Brain Dynamics Toolbox allows scaling up to simulate large ensembles, the employment of other local node dynamics, the introduction of local stochastic influences, and the treatment of internode time delays. Readers may also wish to explore The Virtual Brain (Leon et al.,  ; Sanz-Leon, Knock, Spiegler, & Jirsa,  ), an open-source Python-based toolbox specifically designed for simulating whole-brain dynamics according to the principles explored here. \n\n\n### Quantifying and Testing Time Series Dynamics \n  \nThe detection and quantification of the linear correlations or nonlinear interdependence in time series data rests upon two related steps: (a) the employment of a metric that captures these (linear or nonlinear) properties; and (b) the application of a statistical test to ascertain whether the value of this metric is statistically significant according to an appropriate null hypothesis. The second of these steps recognizes the fact that such metrics are never exactly zero when applied to noisy time series of finite length. In this paper, we use the method of surrogate data to achieve the latter goal. Both steps are now described in further detail. \n\n#### Dynamic metrics. \n  \nWe employ two metrics of internode interactions: the traditional Pearson\u2019s correlation coefficient, and a measure of nonlinear interdependence based upon time series forecasting methods (Schiff, So, Chang, Burke, & Sauer,  ; Terry & Breakspear,  ). These are sensitive to stationary linear correlations (traditional time-averaged functional connectivity) and stationary nonlinear interdependence, respectively. The latter estimates a (normalized) prediction error based upon forward projections of each system\u2019s dynamic trajectory: It approaches 0 for highly structured, completely predictable nonlinear time series and diverges quickly toward a maximum error of 1 when the time series have no structure. Crucially, the measure is sensitive to nonlinearities in the time series, possessing higher values for nonlinear time series than for random time series with the same (cross- and autocorrelation) linear properties. There are two versions: Self-predictions are sensitive to nonlinearities within a time series, whereas cross-predictions are sensitive to nonlinear interdependences between subsystems. \n\nEstimates of dynamic, instantaneous interactions are obtained by examining the behavior of phase differences between time series. The Hilbert transform is first applied to each system\u2019s time series, allowing an estimate of the instantaneous phase (and amplitude) of a signal (Tass et al.,  ). The Hilbert transform of a time series   x  (  t  ) is given by which can be used to compose the analytic signal, which uniquely defines the instantaneous amplitude   A  (  t  ) and phase   \u03c6  (  t  )of the signal   x  (  t  ). Phase dynamics between two signals   x  (  t  ) and   x  (  t  ) are then given by \n\n\n#### Surrogate algorithms. \n  \nIn finite length, autocorrelated time series, measures of (linear and nonlinear) sample correlations are generally not 0, even for uncoupled, independent systems. Measures of correlation taken from large numbers of samples do center at 0, but the variance across individual samples can be substantial. To perform statistical inference on the typically modest number of data available, it is thus necessary to compare empirical measures of coupling to a null distribution derived from ensembles of surrogate data: These are pseudo time series derived from empirical data by resampling methods that preserve the time series length, autocorrelation structure, and amplitude distribution but have had the property of interest (nonstationarity, nonlinearity) destroyed. If the empirical measure falls outside of the null distribution, then the data can be inferred to contain that property of interest. \n\nFor the present study, we employ a nonparametric phase-randomization method (Theiler, Eubank, Longtin, Galdrikian, & Doyne Farmer,  ). Briefly, multivariate data are mapped into the frequency domain by application of the Fourier transform. The phase of each frequency is then independently rotated by a random increment between 0 and 2  \u03c0  . The data are then transformed back to the time domain. By leaving the amplitude of each frequency untouched, this process preserves the power spectrum of the time series and hence the linear autocorrelations. By rotating the phases of different time series (in a multivariate stream) by the same random increment, the cross-correlations are also preserved (Prichard & Theiler,  ). An additional step restores the amplitude distribution of the original time series, which is otherwise rendered Gaussian (Schreiber & Schmitz,  ). This resampling approach can be adapted for complex three-dimensional data enclosed within a bounded spatial domain, such as whole-brain fMRI, by using the wavelet transform (Breakspear, Brammer, Bullmore, Das, & Williams,  ). \n\nPhase randomization works because trajectories in smooth continuous dynamical systems ( ) generate time series with highly structured phase relationships across frequencies. To test for significant linear cross-correlations, we simply shift the time series relative to one another (thus preserving auto- but destroying cross-correlations) and test the original against the correlations from the time-shifted surrogate data. To test for nonlinearities within a single time series, we perform phase randomization and compare the nonlinear self-prediction errors of the original time series to the ensuing surrogate distribution. Finally, to establish nonlinear interdependence, we apply a multivariate phase randomization and compare the nonlinear cross-predictions of original and surrogate ensemble. \n\n\n\n\n## RESULTS \n  \nWe first explore the emergence of dynamic synchrony between two interacting neural masses, each with three dynamical variables   X  (  t  ) = {  V  ,   W  ,   Z  } exhibiting local chaotic dynamics. Specifically, we examine the dynamics of two uncoupled nodes, then two nodes with strong and weak coupling. We plot and analyze the time series corresponding to the average membrane potential of each system,   V   and   V  . In later sections, we consider the principles underlying larger ensembles and the translation of these dynamics into the setting of noisy experimental data. \n\n### Uncoupled Systems \n  \nIn the absence of coupling   c   =   c   = 0, the two coupled neural subsystems evolve independently ( ). Because of their intrinsic aperiodic dynamics, the two systems evolve in and out of phase even if their parameters are identical. Plotting the time series of one system   V   directly against the other   V   reveals the lack of any underlying synchronization structure ( ). As a result, the difference between the two systems\u2019 phase (modulus 2  p  ) unwinds ( ). It is, however, important to note that because of the autocorrelations within each time series, the linear correlation coefficient is often not close to 0 for any particular finite length sample: The correlation coefficient for the time series shown in   is 0.08. However, the distribution of the linear correlation coefficient from an ensemble of repeated realizations of the time series is centered at 0 ( ). This is a reminder that anecdotal observations of nonzero correlations can easily be misinterpreted as functional connectivity in the data, where there is none. \n   Uncoupled systems. (A) Time series for two uncoupled neural masses (  V   is black,   V   is gray) in the chaotic regime. (B) The same time series with   V   plotted against   V  . Transients (  t   < 100) have been omitted. (C) Hilbert phase of   V   relative to   V  . Plotted in cylindrical coordinates with unit radius. (D) Distribution of linear correlations between   V   and   V   for multiple simulation runs with random initial conditions. (E) Amplitude-adjusted surrogates for the time series from panel A. (F) Distribution of linear correlations between surrogate data drawn from the same instances of   V   and   V   (i.e., one simulation run, multiple shuffles of the surrogate data). (G) Nonlinear self-prediction of V1 from itself (black) and from surrogate data (red). Note that both errors grow toward one with longer prediction horizons, but the original data falls well below the null distribution. (H) Nonlinear cross-prediction of   V   from   V   (black) and from surrogate data (red). Here the empirical data falls within the surrogate distribution, reflecting the absence of intersystem coupling.    \nSurrogate data generated from the time series in   by (multivariate) phase randomization are shown in  . The distribution of linear correlations between time series generated by repeated application of phase randomization are shown in  : It can be seen that the empirical correlation (0.08) falls within the surrogate distribution. This observation confirms that the ensemble of surrogate data does adequately represent the null distribution of trivial linear correlations that arise because of the finite sample length. \n\nDo these data contain further (i.e., nonlinear) structure? This can be tested by studying the nonlinear prediction errors, specifically how forward projections of one system\u2019s orbits predict the actual evolution of either that same system (nonlinear self-prediction error) or the other system (nonlinear cross-prediction error; Schiff, So, Chang, Burke, & Sauer et al.,  ; Terry & Breakspear,  ). Because this approach is based upon a low-dimensional phase space reconstruction, it is sensitive to nonlinear, as well as linear, correlations within the data. Here we see that such forward predictions (of one system predicting itself,  , and of one system predicting the other,  ) are less than their theoretical maximal value of 1 (black lines). The nonlinear (self-) prediction errors fall well below the forward predictions arising from surrogate data (red lines), because the original time series have internal nonlinear structure, arising from the local chaotic dynamics. However, the nonlinear cross-prediction errors fall within the null distribution, because there is no coupling and thus no nonlinear interdependence. \n\nIn sum, uncoupled chaotic neuronal dynamics give rise to autocorrelated time series with trivial linear cross-correlations that distribute around 0. Nonlinear self-prediction errors lie outside the null distribution, confirming that each time series contains nonlinear (chaotic) structure. However, nonlinear cross-prediction errors fall within the null distribution generated by surrogate data that contain the same linear correlations. That is, these data arise from independent (uncoupled) stationary nonlinear processes. \n\n\n### \n  \nIn the presence of strong unidirectional coupling, such as   c   = 0.6,   c   = 0, two neural subsystems with identical parameters exhibit a rapid convergence to complete synchrony; that is, the second (slave) system rapidly adjusts its dynamics to match those of the first (master) system ( ). Thereafter the two systems pursue identical orbits; that is, they exhibit identical synchronization, evidenced by their rapid convergence to perfect phase synchrony ( ), and their states approach the hyperdiagonal in phase space,   V   =   V  ,   W   =   W  ,   Z   =   Z  . For simplicity, we plot a two-dimensional cross section through the full dimensional phase space spanned by   V   and   V   ( ). It can be seen that the initial transient (gray line) rapidly converges onto the hyperdiagonal (black line). \n   Generalized synchrony. (A) Time series for two coupled identical neural masses (  V   is black,   V   is gray) exhibiting identical synchronization. (B) Time series for two coupled nonidentical neural masses (  V   is black,   V   is gray) exhibiting generalized synchronization. (C) Hilbert phase of   V   relative to   V   for the case of identical synchronization. Note the rapid approach to stable 1:1 phase synchrony. (D) Hilbert phase of   V   relative to   V   for the case of generalized synchronization. Brief, but incomplete, phase slips continue to occur following the transient. (E)   V   plotted against   V   for the cases of identical synchronization. After a brief transient, the system approaches the diagonal. (F)   V   plotted against   V   for the cases of generalized synchronization. Transients have been omitted. (G) Nonlinear self-prediction of   V   from itself (black) and from surrogate data (red). (H) Nonlinear cross-prediction of   V   from   V   (black) and from surrogate data (red).    \nThe onset of identical synchrony occurs for much weaker internode coupling if it is bidirectional,   c   = 0.05,   c   = 0.05. This is because both systems are able to simultaneously adjust their internal dynamics according to the state of the other system, leading to a more stable, integrated system. \n\nBiological systems are obviously not composed of identical subsystems because some degree of asymmetry is inevitable. However, two neural masses with modestly mismatching parameters continue to exhibit strong, rapid, and stable synchrony if the internode coupling is sufficiently strong, for example,   c   = 0.6,   c   = 0 ( ). These dynamics are accompanied by stable 1:1 phase locking between the two systems ( ). That is, following an initial transient of phase unwinding (until   t   = \u223c150 ms), the phase difference remains close to 0, although it shows brief, bounded excursions. Rather than contracting onto the (hyper-) diagonal linear subspace, the orbits of this system converge toward a smooth manifold that lies just off the diagonal ( ). This phenomenon, known as generalized synchronization, arises in a broad variety of coupled asymmetric chaotic systems (Afraimovich, Verichev, & Rabinovich,  ; Hunt, Ott, & Yorke,  ; Pecora & Carroll,  ; Rulkov, Sushchik, Tsimring, & Abarbanel,  ). The smooth surface onto which the orbits converge is known as the  . \n\nThe time series generated in this scenario embody several instructive properties. The presence of synchrony gives rise to linear correlations that are close to unity. After a brief transient of less than 150 ms, the correlation coefficient is above 0.99 for all successive time windows. That is, the system has stationary linear cross-correlations. In the presence of static measurement noise, such a system would give rise to stationary functional connectivity (that is, the ensemble linear statistics are stationary over successive time windows). However, these time series also contain deeper structure than multivariate surrogate data that possess the same linear (auto- and cross-) correlations. That is, the nonlinear prediction error ( ) and nonlinear cross-prediction ( ) of the original data are both smaller than prediction errors of the corresponding linear null distributions. This arises because the system traverses phase space on the highly structured and smooth synchronization manifold. \n\nIn addition to the presence of stationary linear statistics, these data thus contain nonlinear correlations previously termed \u201cdynamic connectivity\u201d (Breakspear,  ). This property of the data permits rejection of the null hypothesis represented by the multivariate surrogate data, namely that the time series are generated by a stationary multivariate linear process. Since trivial analysis of the stable and very high linear correlations shows that the linear statistics are stationary, then the preceding analyses point to the (true) alternative hypothesis that the data are generated by a stationary multivariate nonlinear process. \n\n\n### Metastability \n  \nWe next study the development of generalized synchrony in the presence of increasingly strong unidirectional coupling   c   > 0,   c   = 0, that is, as the second system gradually adjusts its dynamics to those of the first. Increasing coupling   c   from 0 leads to a monotonic increase in the time-averaged correlation coefficient until the onset of stable generalized synchronization. However, the accompanying dynamic behavior is quite complex (Ashwin, Buescu, & Stewart,  ). When the coupling is not sufficiently strong, the two systems show instances of desynchronization, evident as a separation of the states of each system (see example in  ) and a complete unwinding of the relative phase. For weak levels of unidirectional coupling (e.g.,   c   = 0.1), brief periods of generalized synchrony (and corresponding phase locking) appear among longer intervals of phase unwinding ( ). If the coupling is increased, the duration of synchronous epochs lengthens, and the instances of phase unwinding become confined to brief, erratic bursts ( ). Even in the presence of reasonably strong coupling (e.g.,   c   = 0.5),such bursts continue to (infrequently) appear if one waits for a sufficiently long period of time (e.g., a single burst over a 20-s duration,  ). Meanwhile, as the coupling increases, the synchronization manifold contracts toward the hyperdiagonal, with asynchronous bursts corresponding to brief, disorganized, large amplitude excursions ( ). \n   Metastability. (A) Time series for two weakly coupled neural masses (  V   is black,   V   is gray) showing a single instance of desynchronization. (B) Hilbert phase of   V   relative to   V   with relatively weak coupling. Periods of generalized synchronization are interspersed by erratic desynchronization. The gray shaded region (bottom of panel) shows the point-wise correlations between   V  (  t  ) and   V  (  t  ) smoothed over a 1-s moving window. (C) Hilbert phase of   V   relative to   V   with medium coupling. The instances of desynchronization have become relatively infrequent and briefer. (D) With strong coupling, instances of desynchronization are relatively rare. (E) Plot of   V   versus   V   for the case of strong coupling. The desynchronization is seen as a brief, erratic excursion from the synchronization manifold. (F) Nonlinear self-predictions of   V   from itself (black), and (G) nonlinear cross-predictions of   V   from   V   (black). Predictions of   V   from surrogate versions of   V   are shown in red. The time series retain nonlinear structure despite the instances of desynchronization.    \nThe occurrence of such bursts corresponds to a dynamical phenomenon known as metastability. In brief, for strong coupling, the system possesses a single, low-dimensional chaotic attractor that is embedded within the synchronization manifold: Although the dynamics of this chaotic attractor are reasonably complex (Supplementary Information I, Heitmann & Breakspear,  ), both systems converge onto the same manifold, corresponding to stable (and stationary) generalized synchronization (Ashwin,  ). The dynamics considered within the full (six-dimensional) space spanned by both systems become relatively simple. However, if the coupling is slowly weakened from this scenario, there appears a critical value   c   below which instabilities appear within the synchronization manifold and the system \u201cblows out\u201d into the full phase space for brief instances (this is formally called a blowout bifurcation; Ashwin, Aston, & Nicol,  ; Ott & Sommerer,  ). In the vicinity of this blowout bifurcation   c   \u2248   c  , the intervals between asynchronous bursts can be very long, following a heavy-tailed process (Ott & Sommerer,  ). \n\nMetastability is perhaps better known when there are multiple competing states (Cocchi, Gollo, Zalesky, & Breakspear,  ; M. Rabinovich, Huerta, & Laurent,  ; M. I. Rabinovich, Huerta, Varona, & Afraimovich,  ). Such a system cycles between such states, exhibiting a broad variety of synchronous behaviors (such as a variety of cluster solutions; Ashwin & Field,  ). In the present setting, there is only one such unstable state and the system hence jumps away, then returns back toward the same synchronous state. This specific type of behavior is known in the literature as itinerancy (Kaneko,  ; Kaneko & Tsuda,  ; Tsuda,  ). In more technical parlance, it is an example of homoclinic itinerancy (\u201chomo\u201d referring to a single system that is both attracting and repelling). \n\nItinerancy endows the time series with highly nonstationary properties: The unstable bursts yield a loss of phase synchrony and a corresponding local decrease in the linear correlation coefficient, both of which return to high values during the longer periods of generalized synchronization. As a result, fluctuations in time-dependent linear correlations from the original time series are greater than those arising from multivariate (stationary) surrogate data. Nonlinear prediction errors and cross-prediction errors both remain outside the null distributions (from multivariate surrogate data) even if these are obtained from long windows that contain several of the bursts (  and  ). \n\nA final summary description of these data is therefore quite nuanced. Recall that they are generated by a coupled nonlinear dynamic system whose parameters are all constant and, in particular, do not depend upon time. These data are hence generated by an autonomous, multivariate nonlinear process. They yield data whose nonlinear properties (for example, phase locking) are highly dynamic. The linear properties of these dynamics are also highly nonstationary; that is, they possess fluctuating time-resolved functional connectivity. Moreover, because the itineracy has long-tailed (non-Poisson) statistics, these properties cannot be captured by a classic finite state Markov model and hence may, in certain circumstances, violate formal definitions of weak-sense stationarity (Liegeois et al.,  ). \n\nThe term \u201cdynamic functional connectivity\u201d is arguably a poor term to summarize these properties and to disambiguate metastability from the stationary but nonlinear properties that arise in the setting of generalized synchronization, both of which permit rejection of the stationary, linear null. We return to this issue in the   section. \n\n\n### Multistability \n  \nWe consider one further dynamical scenario that yields nontrivial, dynamical interdependence between two or more systems, namely multistability. In a multistable system there exist two or more stable attractors. That is, there are dynamical regimes that, in the absence of noise, trap the behavior of a system indefinitely. Spontaneous switching between the coexisting states then arises when there is noise   \u03b6   added dynamically to the states, where   \u03b6   is a stationary zero mean stochastic process scaled in amplitude by the parameter   b  . When the noise is of sufficient amplitude, a multistable system is able to escape the basin of each attractor, and jump from one to the other. This is a subtle, albeit important, difference between multistability and metastability. A metastable system is composed of only unstable nodes, and the evolution of the system cycles from one to the other (or back to itself) even if there is no noise   \u03b6   = 0. In contrast, a multistable system will settle onto one stable attractor unless external noise is injected   \u03b6   > 0. The difference may seem subtle but the mechanisms, emergent system behavior, and resulting statistics are quite distinct (for a review, see Cocchi et al.,  ). \n\nIn an array of coupled systems such as we are considering, multistability can arise when each individual node has multiple attractors. It can also emerge when the individual nodes are monostable, but the internode interactions introduce multiple types of synchronization dynamics (Ashwin & Field,  ). In the system considered above, there is only one (chaotic) attractor per node but the coupled ensemble can exhibit multistable attractors, for example when there are three or more nodes and their interactions have axonal time delays (Gollo & Breakspear,  ). \n\nThe emergence of multistability through the interactions of monostable elements is very interesting, but also rather complex. For reasons of relative simplicity, we will thus illustrate a system of coupled nodes where each single node has two attractors; a fixed point and a co-occurring periodic limit cycle. That is, each individual node can exhibit either steady state or oscillatory behavior, depending on the state to which it is initially closest. A simple\u2014or \u201ccanonical\u201d\u2014form of this system has been used to model the human alpha system (Freyer, Roberts, Ritter, & Breakspear,  ) and is a mathematical approximation to a complex neural field model (Freyer et al.,  ). The equation for the amplitude dynamics of a single node according to this simplified model are given by where   r   is the amplitude,   \u03bb   and   \u03b2   are parameters that control the size and depth of the fixed-point and limit-cycle attractor basins. The parameters   b   and   b   control the influence of the additive   \u03b6  and multiplicative noise   \u03b6   x  , respectively (see Supplementary Information III for full details; Heitmann & Breakspear,  ). \n\nWhen the attractor basins of each system are large (i.e., the basin boundaries are distant from the attractors) and the noise has low amplitude, the two coupled systems exhibit either noise-driven low-amplitude fluctuations ( ) or high-amplitude oscillations ( ). When the noise is of sufficient strength or the attractor basins are shallow, the dynamics at each node jump from one attractor to the other. In the absence of internode coupling, these transitions occur independently ( ). The introduction of intersystem coupling increases the coincidence in the timing of the state transitions ( ). However, because of the presence of system noise, these do not always co-occur, even for relatively strong coupling. \n   Multistability. (A) Time series of the noisy subcritical Hopf model with one node. With   \u03b2   = \u221210 the system exhibits a stable (noise perturbed) fixed point at   r   = 0. (B) With   \u03b2   = \u22126 the system exhibits a stable limit cycle with amplitude   r   = 2. Oscillations are shown in gray. Black represents the noise-driven amplitude fluctuations, with close-up shown in panel D. (C) With   \u03b2   = \u22127.5, the system exhibits bistability with noise-driven switching between the fixed point and limit cycle. For simplicity, the (gray) oscillations are not shown. (E) System with two nodes and   \u03b2   = \u22127.5 but zero coupling (  c   = 0). The systems jump between the fixed point and limit cycles independently. (F) Histogram of the linear correlations between the time series generated by the two nodes from panel E. The simulation was repeated for   N   = 200 trials with random initial conditions for each trial. The correlations center at 0 but with substantial intertrial variability. (G) System with two nodes and   \u03b2   = \u22127.5 and strong coupling (  c   = 1). The jumps between the fixed point and limit cycles occur in similar time windows. (F) Histogram of the linear correlations between the time series generated by the two nodes from panel E. The correlations center well above 0 with reduced intertrial variability.    \nTo illustrate the corresponding interactions between two coupled multistable nodes, we focus on their amplitude fluctuations and ignore their phases. In the absence of coupling (  c   =   c   = 0), linear correlations between the amplitude fluctuations converge toward 0 for sufficiently long samples. However, linear correlations taken from brief samples do fluctuate considerably. Locally, the noise-driven amplitude fluctuations are highly incoherent because the noisy inputs are independent (i.e.,   \u03b6   \u2260   \u03b6  ). However, if the two systems do transition, by chance at similar times, then the local linear correlations are driven by these large amplitude changes in the variance, giving rise to large (but spurious) correlations (both positive and negative). Over time, these fluctuations center on 0 ( ), although they have high variance (  SD   = 0.17) as a consequence of coincidental state switches. Moreover, the distribution of sample correlations (taken from short time windows) is not substantially influenced if one of the time series is randomly shifted in time compared with the other: The distribution of values is thus a reflection of the stochastic timing of the erratic amplitude jumps within each system, and whether both systems happen to switch within the same time window. \n\nIn the presence of coupling, the local fluctuations remain uncorrelated. This is due to the independence of the noise sources,   \u03b6   \u2260   \u03b6  . Even though the function   f   is nonlinear, the system evolves in a largely linear fashion within each attractor, and the intersystem coupling is overwhelmed by the independent noisy perturbations around each attractor. However, if one system jumps between basins, it then exerts a strong pull on the other system, until it too jumps to the corresponding attractor. The ensuing coincidence of such large-amplitude state changes then skews the sample linear correlation toward the right (i.e., positively) so that they center at a value greater than 0 ( ). Linear correlations from long time series converge to a positive value that is typically larger than the average of the sample correlations, because such long windows are increasingly dominated by the large-amplitude state changes. Notably, the average of the sample correlations and the long-term correlation coefficient converge toward 0 if one time series is independently rotated in time with respect to the other, underscoring the effect of intersystem coupling on sample correlations. \n\nAs raised above, the local (very short-term) fluctuations are dominated by the independent noise sources, even in the presence of coupling. These data do not contain additional nonlinear structure (both the nonlinear prediction errors and cross-prediction errors fall within the null). Between state transitions, the data resemble stationary stochastic fluctuations. Only when considered on lengthy time series data do the sample statistics reflect the presence of the underlying nonlinear multistable attractor landscape. \n\nThe time series generated by a coupled (noise-driven) multistable system hence show multivariate statistics that are   locally   stochastic, independent, and stable, but are   globally   highly correlated and fluctuate substantially. If the noise term is independent of the state of the system (as per  ), then the switching between attractors is Poisson (Freyer et al.,  ). The statistics of the time series can then be closely approximated by a finite state Markov process, with a fixed likelihood \u039b of jumping states at any time, thus generating Poisson statistics with an exponential distribution of dwell times. Despite the erratic nature of the state transitions, this result theoretically renders the statistics weak-sense stationary (WSS) because the expected correlation and cross-correlations are independent of time (Liegeois et al.,  ). \n\nHowever, there is one final nuance that is conceptually important. In many situations, the influence of the state noise   \u03b6   is state dependent, in which case a more general differential equation pertains: where the influence of the state noise   \u03b6   is dependent on the states   X   via the function   G  . When the noise is state dependent, (e.g.,   G  (  X  ,   \u03b6  ) =   b   X  .  \u03b6  , as in the case of  ), then the system typically gets   trapped   near each of the attractors in a nonstationary manner (Freyer et al.,  ). More technically, in a setting of purely additive noise, transitions probabilities are time invariant and follow a stationary Poisson process. But with multiplicative noise, the chance of a state transition decreases as the time since the last transition increases. This nonstationarity gives rise to a heavy-tailed (stretched exponential) distribution of dwell times (Freyer et al.,  ). Long dwell times are more likely than in the case of purely additive noise. More crucially, the dwell time is dependent on the history of the system. As a consequence, sample statistics cannot be well approximated by a standard finite state Markov process. This is a system for which the covariance between the two nodes is not time invariant and the process is thus   not   weak-sense stationary. \n\nIn sum, the system governed by   for   c   > 0 yields stochastic (linear) time series that fluctuate considerably. However, the statistics are only nonstationary in the strict sense if the noise is multiplicative (state dependent) so that system gets trapped within each state and the ensuing statistics are non-Poisson, which better resemble the statistics of physiological fluctuations. \n\n\n### Complex Dynamics in Larger Ensembles \n  \nWe have thus far restricted our analyses to coupled dyads in order to illustrate dynamic \u201cprimitives\u201d\u2014generalized synchronization, metastability, and multistability. However, cognitive function inevitably involves exchanges between a substantial number of cortical regions\u2014certainly more than two (Fr\u00e4ssle et al.,  ; Razi & Friston,  ; Seghier & Friston,  ; Sporns,  ). To what extent do dynamics in dyads inform our understanding of dynamics in larger ensembles, particularly as time delays (  \u03c4  ) between nodes become an indispensable part of modeling larger systems? \n\nIn some circumstances, the complex dynamics that occur between two nodes are inherited \u201cupwards\u201d when a large array of nodes are coupled together using the same principles of coupling. Thus, a system expressing multistability during the interaction between two nodes will often exhibit noise-driven multistable switching when more nodes are added. In this situation, nodes may cluster into \u201cup\u201d and \u201cdown\u201d states; that is, nodes may cluster into similar states within the same attractor basin, likewise segregated from other clusters which co-occupy a distinct state. In fact, in many coupled oscillator systems, such multistable clustering is quite generic (Hansel, Mato, & Meunier,  ) and can theoretically encode complex perceptual information (Ashwin & Borresen,  ). \n\nOn the other hand, introducing more nodes can lead to additional complexities and dynamic patterns that are not possible with two nodes. A classic example is the nature of phase relationships between nodes in the presence of time-delayed coupling: With two nodes, the time delays cause a phase lag between the coupled nodes\u2019 oscillations. However, when three nodes are coupled in an open chain (or \u201cV\u201d) formation, then the outer nodes can exhibit stable zero-lag synchrony, with the middle node jumping erratically between leading and lagging the outer nodes (Vicente, Gollo, Mirasso, Fischer, & Pipa,  ). Although first described in arrays of coupled lasers (Fischer et al.,  ), considerable work has since shown that such zero-lag configurations arise in small V-shaped motifs of coupled neural systems, including spiking neurons (Vicente et al.,  ) and neural mass models (Gollo, Mirasso, Sporns, & Breakspear,  ). Importantly, stable zero-lag synchrony between the outer nodes of a V motif can survive immersion into larger arrays, where they increase the stability of the system as a whole (Gollo et al.,  ). Such observations support the notion that these coupled triplets underlie the emergence of zero-lag correlations that have been observed in diverse neurophysiological recordings (Gray, K\u00f6nig, Engel, & Singer,  ; Singer & Gray,  ). However, closing the three-node motif by adding a link between the outer nodes (hence turning the V into a cycle) destroys stable zero-lag synchrony, instead promoting \u201cfrustrated\u201d metastable dynamics (Gollo & Breakspear,  ). \n\nTime delays can generate many complex phenomena at the network level\u2014especially when the time delays are heterogeneous\u2014even when the uncoupled (individual) nodes have linear or limit-cycle behaviors (Atay,  ). In addition to the emergence of metastability (Hansel et al.,  ), time delays can also introduce slow collective frequencies (i.e., ensemble oscillations that are much slower than the frequencies of the uncoupled individual units; Cabral, Kringelbach, et al.,  , Niebur, Schuster, & Kammen,  ). Other complex dynamics that can emerge through the influence of time delays include traveling waves (Breakspear, Heitmann, & Daffertshofer,  ) and chimera states\u2014ensemble dynamics whereby there is a domain of coherent nodes and a separate domain of chaotic, incoherent nodes (Abrams & Strogatz,  ; Breakspear, Heitmann, & Daffertshofer,  ; Laing,  ; Shanahan,  ). \n\nWhile considerable progress has been made in this area, the full armory of complex dynamics in large systems of coupled neural subsystems is far from understood. For illustrative purposes, we consider a number of candidate scenarios that arise in larger arrays, specifically when simulating neural mass dynamics on a matrix of 47 cortical regions derived from CoCoMac (Stephan et al.,  ), a compilation of tracing studies from the macaque brain that yields a sparse (22%) binary directed graph (available in the Brain Connectivity Toolbox, Rubinov & Sporns,  ). We employ a coupling function   that mimics a competitive (agonist) feedback between local self-excitation and input from distant regions such that as the influence of external nodes is scaled up by a coupling constant   c  , local recurrent feedback is correspondingly scaled down by (1 \u2212   k  ). All internode influences occur through the coupling matrix   C  . \n\nThe neural mass model employed here possesses two timescales\u2014a fast local field oscillation of approximately 100 Hz nested within a slower timescale of approximately 10 Hz (due to the slow inhibitory feedback; see Supplementary Information I, Heitmann & Breakspear,  ). When parameterized with strong internode coupling (e.g.,   c   = 0.75) and a time delay that approaches the period of the fast oscillations of the neural mass model (  \u03c4   = 6\u201310 ms), the ensemble dynamics break into a number of phase-coupled clusters ( ). \n   Complex dynamics in larger ensembles. (A) Stable partitioning of ensemble dynamics into four phase-coupled clusters with   \u03c4   = 10 ms and coupling   c   = 0.75. (B) Partitioning of ensemble dynamics into six phase-coupled clusters with   \u03c4   = 6 ms and coupling   c   = 0.75. There is slightly greater disorder in some of the clusters compared with those in panel A. (C, D) With weaker coupling and/or shorter time delays (  \u03c4   = 5.5 ms,   c   = 0.45), there are brief phase slips, leading to a reorganization of the cluster configuration. (E) With briefer time delays (  \u03c4   = 5 ms), clustering does not occur. Instead the system shows instances of global synchrony interspersed among spatiotemporal chaos.    \nEach cluster is constituted by phase entrainment to a common beat of the faster rhythm. The full array of cluster states then recur over the course of the slow oscillation. Note that the number of clusters may differ according to the time delay (four clusters for   \u03c4   = 10 ms,  ; and six clusters are apparent for   \u03c4   = 6 ms,  ). In this scenario, the nodes within clusters show stable, stationary generalized synchronization. Nodes in different clusters also show generalized synchronization, albeit with a constant phase offset. This is an ensemble equivalent of stable generalized synchrony in coupled dyads. \n\nThis example illustrates the self-organization of coupled neural systems into dynamic communities, an example of functional segregation. Of interest, if the coupling is weaker (e.g.,   c   = 0.45) or the time delay shorter (  \u03c4   \u223c 5\u20136 ms), the ensemble dynamics show brief instances of desynchronization, such that the global coherence of regions into clusters decreases and nodes switch alliances between clusters ( ). Similar occasions of desynchronization can herald a reconfiguration from a poorly organized state to highly clustered dynamics ( ). In these settings, the ensemble shows some dynamic flexibility in addition to segregation. Such instances render the ensemble statistics nonstationary around the point of transition. \n\nIf a shorter time delay (  \u03c4   \u223c 5 ms) is incorporated into the model, then a clean demarcation into distinct phase-coupled clusters does not arise. The ensemble rather shows instances of near global synchrony interspersed by longer periods of relatively dispersed dynamics ( ). During the epochs of high global synchrony, zero-lag synchrony emerges, and as a result, the ensemble has highly ordered (low entropy) dynamics: Outside of these windows, the amount of order among the nodes is low. The ensemble statistics in this setting are both nonlinear and nonstationary. \n\nIf the time delays approach 0, the state of global synchronization becomes increasingly stable, even for very weak couplings\u2014resembling the scenario for coupled dyads ( ). Conversely, if the time delays are increased, the synchronized cluster solutions become unstable. So, there is a finite range of delays (related to the fast dynamics of the neural masses) for which clustering and metastable dynamics occur. In simple systems, such as coupled Kuramoto (phase) oscillators, this relationship can be derived analytically (Yeung & Strogatz,  ). In systems with nontrivial local dynamics and highly asymmetric connectivity matrices, such as the CoCoMac graph employed here, there are added layers of complexity that remain poorly understood. \n\nThese scenarios illustrate the capacity of neuronal ensembles to exhibit a diversity of dynamic behaviors that yield nonlinear and nonstationary statistics. In some scenarios, dynamical primitives that characterize coupled pairs (generalized synchronization, metastability, and multistability) dominate the ensemble dynamics, yielding their characteristic dynamic fingerprints. New phenomena also appear, including zero-lag synchrony (despite the presence of time delays) and clustering. Typically, these new behaviors compliment the basic dynamics present in coupled dyads, hence metastable bursts yielding spontaneous reconfiguration of cluster states. \n\n\n\n## DISCUSSION \n  \nThe growth of interest in \u201cdynamic\u201d resting-state functional connectivity motivates a deeper understanding of synchronization dynamics in neural systems. Our objective was to illustrate the breadth of synchronization dynamics that emerge in pairs and ensembles of coupled neural populations, and to propose these as putative causes of empirical observations. To recap, coupled dyads exhibit several basic forms\u2014dynamic \u201cprimitives\u201d\u2014that yield nontrivial statistics in the ensuing time series. Generalized synchronization yields stationary nonlinear time series. Metastable dynamics, which arise when the intersystem coupling is below a critical threshold, yield nonstationary and nonlinear statistics. Multistability yields a nonstationary process that is locally linear (i.e., on short timescales) but evidences strong nonlinear properties globally (over long timescales). When such pairs are integrated into a larger ensemble and the coupling is imbued with time delays, then these basic primitives combine with new phenomena, such as phase clustering, to cause complex dynamics that spontaneously switch between different network configurations. This yields time series whose statistics violate the assumptions of a stationary stochastic process and which hence yield nontrivial fluctuations in time-resolved functional connectivity. The dynamics primitives of generalized synchronization, metastability, or multistability may thus account for the spontaneous fluctuations observed in resting-state fMRI data. \n\nIt is also interesting to consider the computational potential of these synchronization dynamics. Neural mass models describe the local average of neural activity, namely local field potentials and average spike rates, not individual spikes. It has been proposed that coherent oscillations in the local field potentials of disparate brain regions promotes information transfer (Palmigiano, Geisel, Wolf, & Battaglia,  ) and spike time\u2013dependent plasticity (Fries,  ). Accordingly, the dynamics illustrated in this paper would allow such neuronal binding (Engel, Fries, K\u00f6nig, Brecht, & Singer,  ) to occur across multiple timescales, and among dynamic cortical assemblies that form and dissolve through active, nonlinear processes (Breakspear, Williams, et al.,  ; Kirst, Timme, & Battaglia,  ). Dynamic synchronization, and desynchronization, could also underlie the spontaneous shifts in attention that co-occur with changes in neuronal oscillations (Jensen, Kaiser, & Lachaux,  ; Womelsdorf & Fries,  ) and in the absence of changes in task context (Kucyi et al.,  ). The nesting of a fast oscillation in a slower one\u2014as occurs for our neural mass model\u2014yields phase-amplitude and phase-phase interactions, which have been proposed supporting cognitive processes requiring complex spatial and temporal coordination (Canolty et al.,  ; Miller et al.,  ; Mormann et al.,  ; Tort, Komorowski, Manns, Kopell, & Eichenbaum,  ). More deeply, the presence of weak instabilities (such as brief desynchronizations) in cortical dynamics has been proposed as a means by which cortex can be primed to respond sensitively to sensory perturbations and thus to optimize perceptual performance (Cocchi et al.,  ; K. Friston et al.,  ). Future work is required to elucidate the effects of exogenous stimulation and endogenous parameter modulation on neural dynamics, and thus to more directly address these issues (Deco & Kringelbach,  ). \n\nThere are several important caveats of the present study. Most notably, functional connectivity denotes correlations between neurophysiological recordings (see  ): We have interrogated the time series of simulated neuronal states   X  . Neural states are not directly evident in empirical functional imaging data, which rather arise from noisy and indirect observation processes. We can somewhat crudely represent this as where   Y   is the empirical signal in channel/voxel   v  ,   M   is a complex measurement process (a nonlinear convolution over a set of regions   i   \u2208   I   and time   \u03c4   \u2208   T  ), and   \u03b7   is the added measurement noise. Functional connectivity is defined as   Cov  (  Y  ,   Y  ) not   Cov  (  X  ,   X  ). In the case of fMRI, the BOLD signal arises from the summed effect of neuronal activity signaling slow changes in blood flow, mediated by neurovascular coupling (Buxton, Wong, & Frank,  ). The net effect of this hemodynamic response function (HRF) is a broad low-pass filter (K. J. Friston, Mechelli, Turner, & Price,  ), which also includes some spatiotemporal mixing if sampled at sufficiently high spatial resolution (Aquino, Schira, Robinson, Drysdale, & Breakspear,  ). Empirical functional connectivity in rs-fMRI experiments thus reflect slow changes (<0.1 Hz) in synchronization dynamics plus the effect of local spatial mixing. Although we do not explicitly model the observation function, it is worth noting that both meta- and multistability yield fluctuations that are substantially slower than the timescales of the single-node neural dynamics (see  \u2013 ), clearly extending into the slow timescales of the HRF. Explicitly incorporating an observation function into a computational framework is crucial to any definitive resolution of fMRI fluctuations. Recent work, using neural mass models that capture the essential internal connectivity and timescales of cortical microcircuits, driving an observational HRF, mark an important step in this direction (Bastos et al.,  ; K. J. Friston et al.,  ). However, the appearance of slow fluctuations in synchrony from fast dynamics lies at the core of the body of work using neural mass models to study fMRI (Cabral, Hugues, Sporns, & Deco,  ; Deco et al.,  ; Deco & Jirsa,  ; Gollo, Zalesky, et al.,  ; C. J. Honey et al.,  ; Zalesky et al.,  ). \n\nIn comparison, EEG and MEG directly detect field fluctuations and do not suffer the same temporal filtering as fMRI. Analyses of resting-state EEG (Breakspear,  ; Breakspear & Terry,  ; C. Stam, Pijn, Suffczynski, & Da Silva,  ) and MEG (C. J. Stam, Breakspear, van Cappellen van Walsum, & van Dijk,  ) data using nonlinear time series techniques have shown that the human alpha rhythm is imbued with substantial nonlinear structure. Integrating these findings with use of biophysical models has shown that the alpha rhythm arises from multistable switching between a fixed point and periodic attractor in the presence of multiplicative noise (Freyer et al.,  ; Freyer et al.,  )\u2014precisely the scenario illustrated in  . This process yields fluctuations in alpha power whose timescales clearly extend into those of the HRF (Freyer, Aquino, Robinson, Ritter, & Breakspear,  ). Crucially, the presence of multiplicative noise in the model (and non-Poisson dwell times for each attractor) imply, as discussed above, that the system statistics are history dependent and are not (weak-sense) stationary according to formal, quite restrictive definitions (Liegeois et al.,  ). By this we can infer that the statistics of resting-state cortex   are   nonstationary. \n\nDespite their superior temporal fidelity, EEG and MEG sensor data involve substantial spatial summation of source activity. Although \u201coff the shelf\u201d (Oostenveld, Fries, Maris, & Schoffelen,  ; Tadel, Baillet, Mosher, Pantazis, & Leahy,  ) source reconstruction methods are now available, they inevitably incorporate assumptions about the covariance structure of the sources and the measurement noise (Baillet, Mosher, & Leahy,  ). As such, and despite post hoc unmixing steps (orthogonalization; Hipp, Hawellek, Corbetta, Siegel, & Engel,  ), there does not yet exist a definitive account of the contribution of synchronization dynamics to source-level M/EEG activity. Given recent accounts of complex multinetwork switching in such data (Baker et al.,  ), substantial steps toward this end seem within reach. \n\nIn addition to spatial and temporal filtering, empirical data are also corrupted by extraneous \u201cnoise,\u201d including physiological effects (EMG, respiratory confounds) and measurement noise (thermal fluctuations, etc.). These external (nuisance) noise sources (  \u03b7   in  ) are conceptually distinct from intrinsic system noise (  \u03b6   in  ), which are an essential part of neuronal dynamics. Prior assumptions regarding the amplitude and temporal correlation structure of measurement noise are a crucial prelude to a formal Bayes-based model inversion scheme (Stephan et al.,  ). While resolving the contribution of measurement noise to resting-state fluctuations is largely a methodological and empirical issue (Laumann et al.,  ; Uddin,  ), computational modeling can also assist. As we have seen above, multistable and metastable processes yield specific heavy-tailed statistics. Most nuisance confounds either have a specific timescale (such as respiratory effects; Chang & Glover,  ), or have very short-range correlations (such as thermal effects). The hope here is to use the statistical fingerprints of synchronization dynamics to help disambiguate true from spurious fluctuations in observed data. Given the salience of brain-body interactions (as indexed by physiological and behavioral interdependences; Allen et al.,  ), it should also be considered that   some   physiological correlates will index true neuronal dynamics (Nguyen, Breakspear, Hu, & Guo,  ) and not simply artifacts. Computational models that incorporate somatic and physiological dynamics\u2014which thus embody and not merely eschew these signals\u2014may be required here (Petzschner, Weber, Gard, & Stephan,  ). \n\nComplex network dynamics are a topic of substantial interest (Boccaletti, Latora, Moreno, Chavez, & Hwang,  ), particularly in computational and network neuroscience (Ashourvan, Gu, Mattar, Vettel, & Bassett,  ; Khambhati, Sizemore, Betzel, & Bassett,  ; Sizemore & Bassett,  ; Stitt et al.,  ). Network fluctuations co-occur with a variety of fluctuating cognitive processes within and across resting-state fMRI sessions (Shine, Koyejo, & Poldrack,  ; Shine & Poldrack,  ). To understand how basic dynamics between pairs of coupled systems scale up, we simulated network dynamics on a structural connectome using connectivity data from CoCoMac. This approach\u2014of dissecting network activity into basic synchronization dynamics among coupled dyads (Gollo & Breakspear,  ; Gollo et al.,  )\u2014contrasts with the engagement of emergent network dynamics without recourse to the dynamics among the basic elements (Cabral et al.,  ; Deco et al.,  ; Deco & Jirsa,  ; C. J. Honey et al.,  ; Zalesky et al.,  ). Our simulations showed how dynamical primitives mix with new ensemble phenomena to inform global dynamics, including the presence of clustering and global synchronization. We did not explore the specific role of the CoCoMac connectome network topology in shaping these dynamics, nor correlations between functional and structural connectivity, which has been the subject of substantial prior work (C. Honey et al.,  ). However, modeling work in this field\u2014mirroring empirical resting-state research\u2014has focused on structural correlates of time-averaged functional connectivity. Future work is required to build upon the early forays in this direction by examining multiple timescales in more detail (Cabral, Kringelbach, & Deco,  ; Deco & Jirsa,  ; Gollo et al.,  ). \n\nAlthough it has intuitive appeal, the term \u201cdynamic functional connectivity\u201d is arguably a clumsy one as it suggests, perhaps naively, that dynamic processes exist within the stream of observable data. Dynamics occur in the underlying neuronal system. If they extend into the temporal and spatial aperture of a particular functional neuroimaging modality (in sufficient strength to survive corruption by measurement effects), then they cause nontrivial statistics in time-resolved data samples. From this perspective, it would be preferable to use simple descriptive terms to capture the fluctuating properties of these time-resolved data and reserve the notion of dynamics to refer to their underlying causes. \n\n\n## AUTHOR CONTRIBUTIONS \n  \nStewart Heitmann: Conceptualization; Formal analysis; Methodology; Resources; Software; Visualization; Writing \u2013 original draft; Writing \u2013 review & editing. Michael Breakspear: Conceptualization; Formal analysis; Methodology; Supervision; Visualization; Writing \u2013 original draft; Writing \u2013 review & editing. \n\n\n## FUNDING INFORMATION \n  \nThis manuscript was supported by the National Health and Medical Research Council (118153, 10371296, 1095227) and the Australian Research Council (CE140100007). \n\n \n", "metadata": {"pmcid": 6130444, "text_md5": "451106fdec2cd8a0fcbbeaa7b6d8254e", "field_positions": {"authors": [0, 41], "journal": [42, 55], "publication_year": [57, 61], "title": [72, 135], "keywords": [149, 229], "abstract": [242, 2386], "body": [2395, 65317]}, "batch": 1, "pmid": 30215031, "doi": "10.1162/netn_a_00041", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6130444", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6130444"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6130444\">6130444</a>", "list_title": "PMC6130444  Putting the \u201cdynamic\u201d back into dynamic functional connectivity"}
