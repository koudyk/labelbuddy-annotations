{"text": "Vidaurre, Diego and Woolrich, Mark W. and Winkler, Anderson M. and Karapanagiotidis, Theodoros and Smallwood, Jonathan and Nichols, Thomas E.\nHum Brain Mapp, 2018\n\n# Title\n\nStable between\u2010subject statistical inference from unstable within\u2010subject functional connectivity estimates\n\n# Keywords\n\ndynamic functional connectivity\nfunctional connectivity\nhidden Markov model\nhypothesis testing\nmultiple replications\npermutation testing\nstatistical testing\ntest combination\n\n\n# Abstract\n \nSpatial or temporal aspects of neural organization are known to be important indices of how cognition is organized. However, measurements and estimations are often noisy and many of the algorithms used are probabilistic, which in combination have been argued to limit studies exploring the neural basis of specific aspects of cognition. Focusing on static and dynamic functional connectivity estimations, we propose to leverage this variability to improve statistical efficiency in relating these estimations to behavior. To achieve this goal, we use a procedure based on permutation testing that provides a way of combining the results from many individual tests that refer to the same hypothesis. This is needed when testing a measure whose value is obtained from a noisy process, which can be repeated multiple times, referred to as replications. Focusing on functional connectivity, this noisy process can be: (a) computational, for example, when using an approximate inference algorithm for which different runs can produce different results or (b) observational, if we have the capacity to acquire data multiple times, and the different acquired data sets can be considered noisy examples of some underlying truth. In both cases, we are not interested in the individual replications but on the unobserved process generating each replication. In this note, we show how results can be combined instead of choosing just one of the estimated models. Using both simulations and real data, we show the benefits of this approach in practice. \n \n\n# Body\n \n## INTRODUCTION \n  \nSuppose that we are interested in testing hypotheses about variables, or set of variables, which we can observe on multiple occasions such that we may obtain a number of noisy measures of the same underlying (unobserved) feature or process. This can happen when we replicate a measurement on multiple occasions for each subject, or if the design of the experiment is such that the repetitions are independent of each other (which would not be the case, for example, if there is a strong effect of learning or habituation across runs). This can also happen when we are modeling data using an approach that is complex enough that inferences about the model parameters can be slightly different every time we estimate the model, for example, with different arbitrary initializations. This is the case, for example, for independent component analysis (ICA, Hyvarinen & Oja,  ; Beckmann, DeLuca, Devlin, & Smith,  ) and Hidden Markov models (HMM, Rabiner,  ; Vidaurre et al.,  ). \n\nIn nondeterministic approaches such as ICA and HMM, the degree to which different initializations will lead to different estimates (i.e., different local minima) of the model parameters depends on elements such as the signal\u2010to\u2010noise ratio, training parameters, and amount of available data (Himberg, Hyv\u00e4rinen, & Exposito,  ). Successive runs of the algorithm may find local minima that are equally good or equally likely, or it may find suboptimal local minima. While in some settings an appropriate figure of merit (e.g., residual sum of squares or model evidence) can adjudicate between these different estimates, sometimes no practical or definitive model comparison score is available; furthermore, even when a score is available, this is typically an approximation or a heuristic, and it is possible that many models with very similar scores will be found. Here we claim that all models are potentially useful and that an effective combination can be more powerful than choosing a single model. More specifically, in this work, we take up the issue of making inference on these noisy replicate estimates, relating the estimates on a group of subjects to variables such as demographics, behavior or personality scores. For this, we are not interested in whether each score relates to each individual replicate; rather, we aim to assess, based on a single global test over the pool of estimates, whether there is evidence that each score holds a significant association with the estimated measure. \n\nBased on the principles of permutation testing, this article presents a simple approach where we use the   non\u2010parametric combination   NPC algorithm (Pesarin & Salmaso,  ; Winkler et al.,  ) to combine results from multiple functional connectivity (FC) estimations, regardless of whether the replications are at the level of data acquisition or model inference. This approach is useful in estimating effects that explain the underlying data that is the focus of the analysis. We demonstrate the validity of this method on the HMM, using simulations and data from the Human Connectome Project (Smith et al.,  ), where we test a measure of (resting state fMRI) dynamic FC over 100 different HMM runs against a number of behavioral variables measured across hundreds of subjects. \n\n\n## METHODS \n  \n### Background \n  \nWe refer to the noisy samples or parameter inference runs as   R replications  , to be distinguished from the   P observed variables   against which we aim to test. (Replications are not to be confused with   realizations  , which we will use to refer to the multiple instances of the synthetic experimental scenario carried out below). We have one hypothesis per observed variable and wish to combine the tests across multiple replications, with no particular interest in assessing each replication in isolation. For   N   subjects, let us denote replications as    Y    (  N   by   R  ), and observed variables as    X    (  N   by   P  ). For reference, we will consider each column of    Y    (referred to as    y   ) as a noisy sample of the certain unobservable variable of interest   Y  . \n\nFor each column of    Y    and each column of    X    (referred to as    x   ), we can use permutations (Nichols & Holmes,  ) to test the null hypothesis that there is no association between the model and the observed data. From this procedure, we obtain a (1 by   R  ) vector of   p   values per observed variable, say    p   . A simple approach could combine these   R   values with a simple statistic such as the mean or the median of    p    to assess the significance: if the mean   p   value is small (e.g., below 0.01), this would suggest that there is a significant relationship between   Y   and    x   . In what follows, we will refer to this summarised   p   value as   p  , similar to Edgington's   p   value combining method comprised of the sum of   p   values (Edgington,  ). A more effective approach is to use the geometric mean, equivalent to exponentiating the average of the log   p   values; this is related to Fisher's   p   value combining method (Fisher,  ) and amplifies the importance of values near zero. Denoting the individual   p  values for a given observed variable of interest as   p  , we have \n\nAgain, if   p   is below a certain level, we can state there is a significant relationship between the replications and the examined observed variable. Note that neither   p   or   p   are   p   values because they do not distribute uniformly in [0,1] under the null. \n\n\n### Example case for a single pair of variables \n  \nBefore coming to a complete description we consider a toy example to make the point above more concrete. We wish to assess if there is a linear relationship between two variables,    a    and    b   . The first one,    a   , with values   a  , is Gaussian distributed (mean 0, standard deviation [  SD  ] 1); the second one,    b   , is a corrupted version of    a    by the introduction of random noise: \n\nwhere   \u03b5   are independent, Gaussian distributed random variables (mean 0,   SD   = 1), and   \u03ba   \u2265 0  .   We generate replicates of    b    based on independent realizations of noise   \u03b5   and   \u03ba  , where   \u03ba   is randomly sampled from a uniform distribution between 0 and   c.   We choose   c   to define the expected strength of the relationship between    a    and    b   . We then run permutation testing on each data set. We evaluate the power of the permutation combining method to detect a relationship between    a    and    b    for different values of   c   > 0. Even when   \u03ba   is randomly small on some replicates, it may be large on others (allowing to detect the underlying relationship in these cases). \n\nFor the purpose of illustration, we generated 1,000 data sets using   N   =\u2009100, each with a different value of   \u03ba   sampled from a uniform distribution and performed permutation testing for each of them. We repeat this for three different values of   c  : 0.0, 0.1, and 0.2. Figure   shows histograms of correlation coefficients between    a    and    b    across data sets (top), and histograms of   p   values (bottom). If the empirical distribution of   p   values is basically flat, as is the case when   c   = 0.0, then there is no evidence of a relationship between    a    and    b  .   However, when   c   = 0.1 or   c   = 0.2, then the distribution of   p   values gets increasingly skewed toward zero despite the generally low correlations. Therefore, if    a    and    b    were experimental replications of some pair of unobserved processes, we could intuitively say that there are signs of correlation between these processes in the   c   = 0.1 and   c   = 0.2 cases. However, neither   p   or   p   (data not shown in the figure) are below 0.05; they are higher than 0.2 in all cases, emphasizing again the point that   p   or   p   are not   p   values and, thus, the need for a permutation procedure to learn their null distribution. \n  \nDistribution of correlation and (first\u2010level)   p   values for the toy example. Simulated examples where we generated 1,000 data sets, where maximum regression coefficient,   c  , is systematically varied. When   c   > 0.0, the mean correlation across data sets is higher than zero (top), and the distribution of   p   values is skewed toward 0.0 (bottom). However, both   p   and   p   are higher than .05 [Color figure can be viewed at  ] \n  \n\n### The NPC algorithm \n  \nGiven a data set with   N   subjects, we are interested in the relationship between the underlying variables (of which the replications are noisy observations), and not on the individual replications. Since   p   or   p   cannot be interpreted as   p   values, we require a method to estimate actual   p   values, that is, distributed uniformly under the null hypothesis. For this, we use the NPC algorithm on   p   (Pesarin & Salmaso,  ; Winkler et al.,  ). In the case when there is only one variable in the model (  p   = 1), referred to as    x   , NPC (on   p  ) proceeds as follows:   \nRun statistical tests (e.g.,   t   tests) between each replication    y    and    x    to obtain an (  R   by 1) vector of   p   values    p   . We summarise    p    using the geometric mean, which, using Equation  , yields   p  . This corresponds to the first\u2010level permutation testing. \n  \nUnder the null hypothesis that each replication    y    and    x    are not associated, we randomly permute    x    a number of times   K.   For each permutation   k  , we produce an (  R   by 1) vector of parametric   p   values    p    analogously to the previous step. We summarise    p    using the geometric mean, obtaining a surrogate   p   value   p   per permutation. \n  \nAt the second level, we obtain a final value by computing the proportion of surrogate   p   values   p   that are equal to or lower than the unpermuted summary   p   value   p  : \n  \n\nFor the   p   > 1 case, that is, when there is more than one observed variable of interest, this procedure can be repeated for each variable, using Equation   on the    x    separately. Crucially, we would use the same exact same permutations\u2014that is, with the permutations happening in synchrony for all observed variables. This way, the dependence between the tests across variables is implicitly accounted for; in Winkler et al. ( ), this is referred to as \u201cmultiple models\u201d. This will yield a final   p   value per observed variable, say   p  . We can obtain a summary, family\u2010wise error corrected   p   value (Nichols & Hayasaka,  ) for each variable of interest   j   by computing where   p   is the null surrogate   p   value obtained with Equation   for the   j   variable of interest and   k   permutation. Alternatively, we can use false\u2010discovery rate (FDR; Benjamini & Hochberg,  ; Nichols & Hayasaka,  ) on the uncorrected   p   values   p   to obtain FDR\u2010corrected   p   values   p  . \n\nIn summary, this procedure draws statistical power from both working in logarithmic space (i.e., promoting the importance of   p   values closer to zero), and simultaneously relaxing the alternative hypothesis from the highly conservative \u201c  all   of the replications bear a relationship with the corresponding observed variable\u201d to the less conservative \u201c  at least   some of the replications bear a relationship with the corresponding observed variable\u201d. In the above example, for instance, this scheme of permutation testing produced a   p   value higher than 0.5 when   c   = 0.0, and   p   values lower than 0.001 for both the   c   = 0.1 and   c   = 0.2 cases, exhibiting both sensitivity and robustness to nonnormality (given that no distributional assumptions are made). \n\nMATLAB scripts for the NPC algorithm and the simulations below can be found in Github. \n\n\n### Regression\u2010based permutation testing \n  \nFor comparison with the NPC, we briefly outline here an alternative also based on the principles of permutation testing, but where we use multivariate regression in order to integrate over replications. That is, instead of performing univariate statistical testing between each replication and each behavioral variable and then combining the resulting first\u2010level   p   values using the geometric mean (Step I in the NPC algorithm outlined above), now we use multivariate regression where we predict each behavioral variable using all replications as predictors; we used regularised ridge regression (using a minimal penalty) to alleviate overfitting in the regression and to avoid algebraic indeterminacies when   R   >   N.   Instead of a   p   value combining function with NPC, an   F  \u2010test is used to summarise all the regression coefficients (i.e., to integrate across replications), and this F score is converted to a   p   value parametrically. We embed this estimation into a standard permutation testing procedure. The final   p   value is eventually computed as in Step III. We shall refer to it as   p  . \n\n\n\n## SIMULATIONS \n  \nTo illustrate the power of combining FC estimations using NPC, we simulated synthetic data sets emulating a scenario in which we are interested in testing whether FC between a pair of brain regions holds a relation to certain behavioral trait in a set of   N   subjects. In this situation, we have the following variables:   \nA subject\u2010specific FC coefficient   \u03b2  , which we cannot observe directly. \n  \nA behavioral variable hypothesized to be related to FC and encoded by a (  N   by 1) vector    x  ,   that can be observed directly. \n  \nSome neural process modulated by   \u03b2   denoted as    S   , which we cannot observe directly. We can consider    S    to be some archetypical, noiseless brain activity controlled by   \u03b2.  \n  \nThe observed (e.g., neuroimaging) data sets    D   , which are noisy measurements of    S    and have a dimension (  T   by 2). This measurement can be repeated up to   R   times per subject. \n  \nAn (  N   by   R  ) matrix    Y   , such that   Y   contains the estimated FC value for the   n   subject and   j   experimental replication (i.e., the correlation coefficient between the channels of the corresponding measured data    D   ). \n  \n\nA schematic of this experimental case is presented in Figure   for clarity. As explained in detail below, the value of   \u03b2   is specific for each subject, and its mean over subjects is zero by design. The hypothesis that we are here testing, therefore, is not whether   \u03b2   is different from zero, but whether there is an association between   \u03b2   and behavior (represented by    x   ). The objective of this simulation is then to assess whether the proposed approach can uncover such relationship, mirroring real data situations often found in the literature where the interest is relating functional connectivity to subject phenotypes (e.g., Smith et al.,  ). Note that, regardless of the generating model for    Y   , the final goal is to test the relation between    x    and    Y   , and the NPC algorithm could have been applied similarly to other generative models. \n  \nSchematic of the model used for the simulations analysis. The dotted arrow represents the correlation we are testing \n  \nWe next provide details about the generating process for    x    and    Y   . In this specific context, the noise in the observations (or replications) stems from the imperfect measurement of    S   , which we can measure multiple times (  R  ). Therefore, there is a relation between FC (  \u03b2  , which we cannot observe but we can estimate) and behavior (   x   ), but this relationship is noisy and weak for some replications. In detail, we generated data from this setting as follows. \n\nWe have   N   =\u2009200 subjects. We uniformly sampled a value   \u03b2   between \u22120.2 and\u2009+ 0.2 for each subject   n.   For each subject, also, we sampled two vectors with 10,000 values each: the first,    s   , is Gaussian distributed (mean = 0,   SD   = 1), whereas the second is set as \n\nwhere    \u03b5    is also Gaussian\u2010distributed. The vectors    s    and    s    constitute the unobserved neural process    S   . The correlation between    s    and    s    can be analytically computed from   \u03b2   as \n\nWe set the value of the observed behavioral variable for each subject to be where   \u03b7   is Gaussian distributed (mean = 0,   SD   = 1). Now, to sample the observed data sets    D    for each subject, we randomly sampled   T   =\u2009100\u2010time points from    S    (whose columns are    s    and    s   ) and added some Gaussian noise with mean = 0 and   SD   =   \u03c3.   We did this   R   times per subject, obtaining one (100 by 2) noisy data set    D    =\u2009[   d   ,    d   ] each time. We then set the observed replication values to where we applied the   z  \u2010transformation on the resulting correlation to make appropriate for parametric testing. \n\nNote that, as illustrated in Figure  , the (unobserved) value   \u03b2  n is involved in both the generation of    Y    and    x   . With both    Y    and    x    in hand, we ran the described permutation testing algorithm on the noisily estimated FC matrix    Y    and the behavioral variable    x   . By controlling   \u03c3   (which defines how noisy are individual time series samples    d    and    d   ), we could make the detection more or less difficult. \n\nWe used a range of 30 values for   \u03c3   between 0.25 and 1.5, and repeated the experiment, that is, data generation and testing, 100 times per value of   \u03c3.   For each repetition of the experiment, standard permutation testing resulted on   R   =\u2009100   p   values (one per replication). Since   p   = 1, there was no need to control for familywise error rate across observed variables (Equation  ). \n\nAlongside the NPC, we also ran for comparison the regression\u2010based permutation testing approach described above, denoted as   p  . Figure  a shows   p  /  p  /  p  /  p   (respectively from left to right) averaged across the 100 realizations of the experiment as a function of   \u03c3  , together with 95% confidence intervals (minus/plus twice the standard error). We ran 10,000 permutations in each case. Thanks to the effect of the logarithm, the   p   values are lower than   p   values, but neither of them ever reached significance provided the weak and volatile relationship between    Y    and    x   . The individual per replication   p   values (shown in Figure  b for one example repetition, per value of   \u03c3  , together with the corresponding correlation coefficients) illustrate this point: although there were some significant   p   values, the average is condemned to fail due to the frequent bad   p   values associated to some too noisy replications. The   p   values did not reach significance either, probably because of a loss of statistical power due to overfitting in the regressions (given that   N   =\u2009200 is not much higher than   R   =\u2009100). However, most of the   p   values from the NPC permutation approach turned out to be significant despite the low magnitude of the signal across replications, with the average of   p   across realisations of the experiment leaving the zone of significance only for the highest values of   \u03c3   (i.e., for the hardest instantiations of the problem). \n  \nResults from the simulated data with   N   =\u2009200, where there is a relationship between the tested variables: FC and behavior. (a)   p   Values obtained from combining tests using the mean (  p   and   p  ),   p   values from the regression\u2010based permutation testing approach (  p  ), and   p   values from the described permutation testing approach (  p  ), as a function of   \u03c3  , which controls the noise in the replications (i.e., higher values of   \u03c3   produce more difficult instantiations of the problem); 95% confidence intervals are computed across realizations of the experiment. (b)   p   Values before test combination for a given repetition (per value of   \u03c3  ), together with the estimated correlation coefficients [Color figure can be viewed at  ] \n  \nSupporting Information Figures S1 and S2 show additional simulations for   N   =\u200950 and   N   = 1,000 subjects, respectively. In the most difficult case,   N   =\u200950, both   p   and   p   were far from any level of statistical significance, and   p  , although exhibiting lower   p   values than   p   and   p  , reached significance only occasionally (but more often than   p  ). In the easiest   N   = 1,000 cases,   p   was under 0.05 for the lowest levels of noise, and   p   reached values under 0.05 for half of the range of   \u03c3  ;   p  , however, stayed most of the time at the minimum levels allowed by the number of permutations (i.e., 1/10,001), clearly outperforming   p   and   p  . Comparatively,   p   also reached significance for the entire range, but less strongly than   p  . As observed, the NPC outperformed this alternative in every case. This was expected because univariate calculations are more robust to overfitting that multivariate regression, which hinders the latter's statistical power. \n\nNext, we repeated the same analysis but forcing a fixed value of   \u03b2   for all subjects (in particular, we set   \u03b2   = 0). In this case, there is not a relationship between behavior and FC. Figure   shows that NPC, as well as the other methods, is robust to Type I errors. \n  \nResults from the simulated data, where there is   not   a relationship between FC and behavior. The description of the panels is equivalent to Figure  . In this case, however, the 95% confidence intervals do not overlap with the region of statistical significance no relation was found between FC and behavior, that is, there was no Type I errors [Color figure can be viewed at  ] \n  \n\n## DYNAMIC FUNCTIONAL CONNECTIVITY IN REAL DATA \n  \nHaving demonstrated the utility of the NPC approach to relate FC to behavior in a synthetic scenario where the estimation was very noisy, we next evaluated it using real data by applying the Hidden Markov model (HMM) to resting state fMRI data from the Human Connectome Project (HCP). The HMM assumes that the data can be described using a finite number of states. Each state is represented using a probability distribution, which in this case is chosen to be a Gaussian distribution (Vidaurre, Smith, & Woolrich,  ); that is, each state is described by a characteristic pattern of BOLD activation and a certain FC profile (we use the same configuration as in Vidaurre, Smith, and Woolrich ( ), to which we refer for further details). As the HMM is applied at the group level, the estimated states are shared across subjects; however, the state time courses that indicate the moments in time when each state is active are unique to a given individual. For the purposes of this analyses, we set the HMM to have 12 states. Note that, as discussed in former work (Vidaurre et al.,  ), there is no specific biological significance in the chosen number of states, and a different number of states just provide different levels of detail in the HMM decomposition. Here, we chose 12 states simply to be consistent with our previous work on this data set (Vidaurre, Smith, & Woolrich,  ). Using the inferred state time courses, the amount of   state\u2010switching   for each subject was calculated, which corresponds to a metric of how frequently subjects transition between different brain states (more specifically, given that the state time courses are probabilistic assignments, we compute the mean derivative of the state time courses for each subject). We used state\u2010switching as a summary metric of dynamic functional connectivity (DFC). \n\nIn order to infer the HMM at reasonable cost in spite of the large amount of data (820 subjects by four sessions by 15\u2009min, TR\u2009= 0.75 s), we used a stochastic learning procedure (Vidaurre et al.,  ), which involved performing noisy, yet economical, updates during the inference. Since stochastic inference brings an additional layer of randomness into the HMM estimation but is not costly to run, we repeated the HMM inference 100 times and computed state\u2010switching for each run. In this context, each HMM estimation constitutes a replication. Following the paper notation, we denote the state\u2010switching measure for subject   n   and replication   j   (averaged across the four sessions) as   Y  . \n\nAlthough stochastic inference adds additional randomness to the estimation, the HMM has have previously been reported to perform very robustly in this data set (Vidaurre, Smith, & Woolrich,  ), possibly as a consequence of the large number of subjects (  N   =\u2009820), the length of the scanning sessions, and the general high quality of the data. For this reason, the different HMM runs were quite consistent, which in turn means that the tests produce relatively similar results across replications (as shown below). To illustrate the effect of greater noise, we created a second set of replications where we permuted the state\u2010switching measure between subjects randomly for half of the HMM runs (i.e., half of the HMM runs, or replications, are potentially related to behavior whereas the other half are noise, and all of them are included in the analysis). We refer to this as the   perturbed   data, as opposed to the   original   data where the HMM estimations are left intact. \n\nFurthermore, each subject has a number of behavioral measures, including psychological and sociological factors and several health\u2010related markers. We used a total of 228 behavioral variables, after discarding those with more than 25% of missing values, to test against DFC as measure by state\u2010switching. We included age, sex, motion, and body\u2010mass\u2010index (the latter two usually considered as confounds). We also discarded those subjects without family information and those with a missing value in any of the behavioral variables. We denote the (  N   by   P  ) matrix of subject traits as    X   . \n\nWe tested for significance in the correlation between switching rates across replications (   Y   ) and each of the subject traits, contained in the columns of    X,    for both the original and the perturbed data set. We used 10,000 permutations, respecting the family structure of the HCP subjects (Winkler, Webster, Vidaurre, Nichols, & Smith,  ). \n\nFigure   compares the results of applying the NPC approach described above with the mean and geometric mean of the   p   values (  p   and   p  ) as well as with the alternative regression\u2010based permutation testing outlined above (  p  ). Figure  a shows the mean   p   value (averaged across replications) reflecting the subject\u2010wise correlation of state\u2010switching (as measured by the HMM) with the different behavioral variables, with the behavioral variables being ordered from more to less significant; for purposes of illustration, dots represent individual   p   values for some randomly chosen replications. On the left, the   p   values obtained from standard permutation testing on the original HMM runs are quite consistent across replications; on the right, for the perturbed set of HMM runs, given that half were randomly ordered over subjects, the mean   p   value reflects the reduced effect strength. \n  \nAnalysis of the relation between behavior and DFC (state switching) as measured by the HMM, where replications correspond to HMM runs. (a) Mean   p   values (averaged over replications, with dots representing   p   values for a randomly chosen subset of 10% of the individual replications), reflecting the subject\u2010wise correlation of DFC with the different behavioral variables. On the   X  \u2010axis, behavioral variables are ordered from more to less correlated. On the left, this is shown for the original data set; on the right, this is shown for the perturbed data set (a noisier version of the original data set). (b) Histograms of   p   values, indicating that   p   and   p   generally outperform   p   and   p  . (c) The   p   values are robust to perturbation only for NPC, where the correlation between perturbed and original   p   values is close to 1.0 \n  \nIn Figure  b, we examine the histograms of   p   values for each of the four alternatives (with a loose use of the term \u201c  p   value\u201d when referring to   p   and   p  ). On top, where all the HMM runs were used normally, the difference between methods is somewhat subtle. At the bottom, no variable was under significance level for   p  , and only 30 variables were under significance level for   p  ; in contrast, over 60 variables turned out to be significant for   p   and   p  . The difference between   p   and   p   conveys the benefits of working on logarithm space, whereas the difference between   p   and   p   reflects the transformation needed to convert   p   to quantities interpretable as conventional   p   values. According to the small differences between   p   and   p  , the latter factor seemed to make the biggest difference in this data set. Regarding the regression\u2010based permutation method (  p  ), given that we have 100 replications in this case and a large number of good\u2010quality subjects, the regressions did not suffer from overfitting as much as in the simulations above. \n\nFigure  c shows, for each of the methods, the (combined across replications)   p   values for the original data versus the perturbed data, reflecting that only the NPC approach was robust to having corrupted replications (i.e., the   p   values are almost identical between the original and the perturbed data set). \n\nFigure   presents the behavioral variables for which we found significance using the NPC procedure. Interestingly, although motion is a significant predictor it does not explain the greatest variance in this analysis, suggesting that DFC on resting state fMRI, as estimated by HMM, can be meaningfully related to behavior beyond the influence of motion. Most of the traits that were found significant were health\u2010related, with fewer higher\u2010level psychological traits than were found by Smith et al. ( ), which focused on functional connectivity instead of any dynamic aspects of the data (such as the state\u2010switching rate). Due to the relatively large number of observed variables, only a few were found to be significant after FWE correction (i.e., in Equation  , the minimum of the surrogate   p   values across observed variables can be small if there are many observed variables to choose from). In contrast, FDR (Nichols & Hayasaka,  ), allowed the identification of up to 25 variables. When we randomly corrupted the entire data set (instead of half of the subjects as in the perturbed data set), all methods, including NPC, were able to satisfactorily control for Type I errors (data not shown). \n  \nFor the observed variables considered to be significant (out of 228), (a)   p   values using the NPC on   p   approach (  p  ), with FWE significance indicated on the top left; and (b) FDR\u2010corrected   p   values (  p  ) [Color figure can be viewed at  ] \n  \n\n## DISCUSSION \n  \nIn this article, we show that the stochastic nature of FC estimations often considered a hindrance, can be effectively integrated to provide valid and sensitive inferential procedures. If the differences between the estimations are not only due to random noise but contain different elements of information, such integration can be largely beneficial. If these differences are just pure noise, the presented procedure can approximate the accuracy of a single, noise\u2010free estimation. \n\nOn these grounds, we describe a permutation testing approach based on previous work (Pesarin & Salmaso,  ; Winkler et al.,  ) that can be used to test for the relationship between a set of observed variables and an unobserved (FC\u2010based) variable for which we have a number of noisy estimations. The crucial point is that we are not interested in finding the relationships as described by a particular FC estimation, but instead would like to understand the relationship of the   true   FC with the observed variables. We took as a concrete example the relationship between covert patterns of intrinsic brain connectivity, as they occur at rest, and patterns of cognitive and demographic variables measured outside of the scanner, using data from the Human Connectome Project. \n\nAlthough we focused on univariate observed variables and replications, the described method can straightforwardly be extended in a number of ways. First, although we focused on linear relationships between variables, it can easily be extended to multivariate statistics, such as multivariate linear regression, or canonical correlation analysis. This is important in that it allows studies in which the mapping between cognitive function and the data is not univariate in nature. It can also be extended to situations when we have replications on both sides of the correlation, such as when both the observed and nonobserved behaviors are measured on multiple occasions. In this case, each pair of replications could be tested individually (for each element of the corresponding Cartesian product), and we would proceed similarly. \n\nMoving forward, these types of approaches are likely to be particularly important in the domain of neuroscience given recent shifts toward the use of intrinsic connectivity at rest as a method of evaluating structural features of cognition. Intrinsic connectivity, as measured at rest, is a powerful tool for exploring the structure of neural organization since it is able to reveal similar patterns of neural organization as emerge during tasks (Smith et al.,  ). In addition, the simple noninvasive nature of the use of resting state as a method for assessing neural function means that it can be applied to multiple different populations, even those for whom task\u2010based measures of neural function or psychological measurements may be problematic (such as children or populations with cognitive problems). Measuring neural organization at rest is also easy to implement across centers making it amenable to the creation of large multicentre data sets, a shift that is likely to be increasingly important as neuroscience faces up to the challenges of reproducible science. \n\nDespite the promise that assessing neural function at rest holds, many of the same features that make it an appealing tool for the cognitive neuroscience community are also at the heart of many of its limitations. For example, the power that is gained by the unobtrusive nature of the measure of neural function at rest also leads to concerns regarding what the measures actually represent: it is unclear which aspects of the neural signal reflect the intrinsic organisation of neural function, which reflect artefacts that emerge from physiological noise or motion (Power et al.,  ), and which reflect the patterns of ongoing experience that frequently emerge when individuals are not occupied by a demanding external task (Gorgolewski et al.,  ; Vatansever et al.,  ). In this context, because the underlying ground truth is unknown, an effective way to integrate estimations will help the researcher to identify which aspects of a given neural pattern are expressed in a robust way in relation to neurocognitive function. \n\nAlthough dynamic approaches to understanding functional connectivity space are growing in popularity (Chang & Clover,  ; Vidaurre, Smith, & Woolrich,  ), different approaches have specific limitations. For example, sliding window approaches depend upon an apriori selection of the window length, which limits the granularity of neurocognitive states that can be identified. While approaches such as HMM circumvent this problem by allowing the data to determine the temporal duration of the underlying states, these analyses are inherently probabilistic and parameter inference can introduce noise into the analysis. In this context, NPC allows dynamic approaches to cognition to be compared to observed data in a systematic manner. This could help pave the way to formally evaluate how different descriptions of the underlying dynamics at rest best predict variables with well\u2010described links to cognitive function. This way, NPC can become a useful tool in resolving the state\u2013trait dichotomy that currently hinders the development of the science of how neural function evolves at rest. \n\n\n## Supporting information \n  \n \n", "metadata": {"pmcid": 6492297, "text_md5": "fdedb8364923396f7047169cb1d7c6c0", "field_positions": {"authors": [0, 141], "journal": [142, 156], "publication_year": [158, 162], "title": [173, 280], "keywords": [294, 468], "abstract": [481, 2026], "body": [2035, 38132]}, "batch": 1, "pmid": 30357995, "doi": "10.1002/hbm.24442", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6492297", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6492297"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6492297\">6492297</a>", "list_title": "PMC6492297  Stable between\u2010subject statistical inference from unstable within\u2010subject functional connectivity estimates"}
