{"text": "Kumar, Ashutosh and Sharaff, Aakanksha\nSN Comput Sci, 2023\n\n# Title\n\nPubExN: An Automated PubMed Bulk Article Extractor with Affiliation Normalization Package\n\n# Keywords\n\nArticles extraction\nAffiliation normalization\nBulk extraction\nNCBI PubMed\nPython package\nBiomedical text mining\n\n\n# Abstract\n \nBiomedical article extraction is the preliminary step for every biomedical application. These applications are helpful in finding the gene, disease, chemical, drugs, protein entities. Finding entities relation such as gene\u2013gene entities, drug-disease interaction, and chemical protein relation the PubExN can be helpful for these types of biomedical applications. In most cases, domain experts do this extraction process on their own. Human interference makes this process time-consuming and there is a high probability, that documents can be missed during the extraction process. To get rid of these complicated processes a python package is introduced to automate the process of bulk extraction from the PubMed database. The extraction process covers all the citation information with the associated abstract. The batch approach is used to extract the bulk extraction. The motivation for the development of PubExN was to provide flexibility for the extraction process of biomedical article\u2019s text data from NCBI\u2019s PubMed database. Basically, NCBI\u2019s PubMed database article contains the article id or can say PubMed-id (PMID), the title of the article, abstract, authors information, etc. This package will benefit many biomedical texts mining research including biomedical named entity recognition, biomedical relation extraction, literature discovery, knowledgebase creation, and various biomedical Natural Language Processing (NLP) tasks. In addition, it could be used in the author name disambiguation problems and new drug discoveries. This package will help save time and extra effort for the extraction and normalization process of PubMed articles. \n \n\n# Body\n \n## Introduction \n  \nImprovement in the healthcare systems is a basic need for patients as well as for researchers [ ]. The most complicated task is to improve it continuously. The improvement can only be made if we keep measuring the deficits in the system through appropriate metrics and use quality indicators for the following system. The Research methods are broadly classified into two categories qualitative and quantitative [ ]. The use of methods such as interviews, observations, and data analysis in qualitative research has shown to be an efficient way of answering some of these complex problems [ ]. Over time, data are drastically exploding in almost every field. Text data brings new opportunities in various domain. Scientific and technical publications where the new research is going on. Figure\u00a0  shows the statistics of country-wise publication produce a volume of scientific and technical journal from 2000 to 2018. Biomedical text mining is currently a trending topic nowadays due to the increasing new opportunities in the field. When a researcher works on any problem statement, their primary issue is retrieving the structured data, e.g., patient-id, prescription, lab results. The maximum percentage of data available is unstructured or freely written without context [ ]. Researchers cannot directly utilize the data produced and stored on the internet for modeling or any extensive scale application. Retrace, annotating or transforming unstructured data into structured data is extensive and laborious work, and it is not worth the time [ ]. The development of an information extraction package, which we have coined PubExN, is an essential component in making more complex quantitative research on the quality of healthcare possible. This is because there are now no widely accessible tools available for biomedical researchers, despite having already discussed the significance, expense, and arduous nature of this activity [ ]. Such tools would be straightforward in an ideal world, even for researchers who had no prior knowledge of natural language processing (NLP) or software engineering.   \nPublication over the year 2000\u20132018 \n  \n\nThe environment and another requirement to successfully fetch the data are done using the entire API [ ]; the personal NCBI key is required to access the database and maintain the authentication while extracting the data [ ]. The approach our package utilizes is batch extraction. The package can be installed in any operating system having python 3.8 or above versions. For python 3.8, we need to create a conda environment. \n\nThe package can directly be installed using the standard python-pip command. However, the library we have developed can be used as a python package; There is a similar platform named Canary developed by [ ], which is an NLP Based platform and software for data extraction. It was designed to match the above criteria and developed to process and extract the data from clinically supported documents. The package we have developed is similar in some ways, but at the same time, it is implemented as a python package and fulfill the objectives described. Python programing language is used to develop this package and we have used the python 3.9 version for developing our wheel and final package. As we have developed this package from the scratch, we have plugged some existing web scraping dependencies which reduced our efforts up to some extent. The major reason for using this. \n\nThere is always a high probability of missing out on the information relevant to their area of interest and essential for implementation. At that time, it is necessary to refer to all the relevant resources to gain a 360-degree of view (all the possible information on a particular topic), means while extracting the biomedical articles from the PubMed database all the possible information will be able to create a knowledge source. One of the most positive aspects of having software-based data extraction, for now, is that the researchers are now able to collect the pool of evidence systematically, structure their research and study their area of interest to create systematic Literature Reviews (SLR) along with systematic Mapping (SM). Whenever the goal is to precisely collect information that researchers need to have in line with the research questions posed by the SM/SLR study and to extract the necessary data from the primary studies, according to multiple studies, the SLR is not very accurate in their objective study and increases the chances of errors. Hence this issue can be tackled better with the introduction of this software. When young researchers and juniors start to work on research, as they are not very experienced, it might increase the chances of deviating from the objective and learning irrelevant information; hence, it requires heuristics and predefined guidelines for efficient and effective data extraction. Here, effectiveness means the extraction of accurate and quality data from studies. Meanwhile, efficiency implies data extraction speed, which can only be made possible through specific and targeted packages like PubExN. The software has drastically reduced the search time for researchers to find relevant studies on the internet. Previously all types of researchers had to spend much time on the internet to find out the relevant studies of their domain; now, they can directly access content by typing out the keywords. Apart from it, they can have complete information about the topic with a reduced risk of missing out on anything important or relevant. The human utility factor is a significant aspect of explaining the success of any software mainly; there are two significant challenges: low team synergy and language barriers. People new to research find it challenging to implement software usage into their work. However, after gaining some user experience, they get over their hesitation in utilizing the package. Apart from that, the people doing any research have very little or less knowledge of the programming language in which the software package is etched. Second, for the non-native English speaker, the package is bounded to be used for English speakers and etched to extract the English content. However, we need to restructure the package to retrieve the data in other major languages. The package we have created focuses on the extraction of related medical purposes. However, the utilization of our package majorly is adopted by the pharmaceutical industries for their in-lab research; they can fetch the data in bulk to understand any disease, issue or any field of research without wasting their time, the unique selling point for the pharma industry will be there will be lack of chances to miss out any vital information if they switch to automated package for the retrieval of data. \n\nUnique features of the package are given as follows:   \nSome methods are available scientific community, but the uniqueness of the PubExN is the bulk extraction of articles. \n  \nThe PubExN can provide the facility to extract as much as or required article information from the PubMed database which is not provided by the other methods. \n  \nThe PubExN gives the flexibility of normalizing the extracted data which is not present in the other methods. \n  \nPubExN, further normalize the affiliation details which will be helpful for knowledge base creation. \n  \nPubExN uses parallelization for faster execution or parallel extraction process. \n  \n\n\n## Methodology \n  \nHere in this section, we will describe the high-level software architecture; Fig.\u00a0  illustrates the high-level building components used in the creation of PubExN.   \nExplain end to end pipeline development of the package \n  \n\nThe process starts with the NCBI\u2019s PubMed database. The reason of chosen the PubMed index biomedical database because the PubExN is made for mostly biomedical research and only required abstract-level information. PubMed provides the abstract, author and citation information. This information can easily extract from the PubMed index database without any consent or permission and it will be useful in various biomedical applications like biomedical entity extraction, biomedical entities relation extraction, literature base discovery, knowledge graph creation and many more. PubMed stores this information in a systematic order. The researches based on the targeted data need to extract these data from the biomedical database. PubExN can take \u201ckeyword\u201d or \u201csingle PMID\u201d or \u201cList of PMIDs\u201d to extract data from the PubMed database. Extracted PubMed data in the XML format which provides \u201cPMID\u201d, \u201carticle title\u201d, \u201cauthor first name\u201d, \u201cauthor middle name\u201d, \u201cauthor last name\u201d, \u201caffiliation details\u201d and \u201cabstract of the articles\u201d. Full-text affiliation details passes to the affiliation parser for further normalization of the \u201cCountry\u201d, \u201cDepartment name\u201d, \u201cEmail\u201d, \u201cLocation\u201d, \u201cZip code\u201d. PubMed also holds many other information like other citation information like no. of citation, cited paper,publication data, PMCID (PubMed Central) information, etc. The PubExN can also be used for \u201cPubMed Central\u201d which stores almost full detail of the article (Title, abstract, Full text of the article(missing in PubMed database) and author citation information), but in most use cases \u201cPMID\u201d, \u201carticle title\u201d, \u201cauthor first name\u201d, \u201cauthor middle name\u201d, \u201cauthor last name\u201d, \u201caffiliation details\u201d and \u201cabstract of the articles\u201d are the most important component of any natural language processing task in biomedical domain [ ,  ]. \n\n High-level software architecture description:    \n Purpose:   By utilising a variety of architectural views to illustrate various facets of the system, this article presents a thorough architectural overview of the entire system. The purpose of this document is to record and communicate the major architectural choices made during the development of the system. \n  \n Scope:   A high-level overview of how the software behind article extraction and normalization is structured is provided in this Software Architecture Document. The text mining research team of NIT Raipur is working on PubExN to aid in the article extraction process in biomedical text mining. This document was made using the PubExN Analysis Design Model found in PubMed. The bulk of the content was culled from PubMed utilising the E utility API and the format of the Software Architecture Document. \n  \n Definitions, Acronyms and Abbreviations:   The PubExN is a software to designed to reduce the cost and time of the biomedical literature extraction process. We have defined the acronyms and abbreviations throughout the research paper. \n  \n Architectural Representation:   The architecture is laid out in this document from several different perspectives, including the use case view, the logical view, the process view, and the deployment view. Specifically, this document does not detail an implementation view. All of these are perspectives on a model created in the Unified Modeling Language (UML) \n  \n Architectural Goals and Constraints:   Explanation of the software architecture from the perspective of its use cases. The scenarios and/or use cases that are prioritised during an iteration often depend on the information provided by the Use Case View. It specifies the scenarios and/or use cases that model crucial core features. It also details the collection of use cases and/or scenarios that either provide extensive architectural coverage (by exercising many architectural elements) or highlight a particularly nuanced aspect of the architecture. \n  \n Use-Case View:   Fig.\u00a0  shows the Use-case view of the proposed architecture. \n    \nExplanation of Use-Case View \n  \n\nArticles can be searched by PubExn but the affiliation of the author or the citation information is required when you are creating a knowledge base for any biomedical application. So that is the reason we are extracting article information along with the affiliation of the author. Basically, for clustering the domain of the particular area (suppose when we are interested that how many articles are published by a particlcular author then that time it will help to find the domain expert of the particular field).\n   \n Process View   In Fig.   explaining the architectural process view. \n  \n Size and Performance  \n    \nExplanation of Process View of the proposed architecture \n  \n\n  \nThe framework can endorse N number of simultaneous users against the central database or local servers. \n  \nThe client portion shall require memory disc space according to the articles download size and at least 32\u00a0GB of RAM required for smooth operation. \n  \nNo more than a 10-s delay in accessing the original course catalogue database is allowed. \n  \n\n  \n Quality  The performance indicators mentioned in the Additional Standard are sup- ported by the software architecture. \n  \n\n  \nThe desktop user interface shall be windows, linux or any other platform. \n  \nSystem should be python supported. \n  \nInternet should be mandated throughout the process. \n  \n\n### Article Extraction Process \n  \nAlgorithm 1 will be explaining the workflow for the extraction of data. PubExN can be extracting PMID, title, first name, middle name, last name, affiliation, and abstract of the paper. The package utilizes an API that permits access to NCBI\u2019s dataset and downloads the data without any blockage. The data is fetched through research (research is mainly used to determine the total number of documents present.) within the batches of 300 approx. The NCBI website does not permit downloading approx more than 300 papers at a time. Key and API tackle this fetching hindrance to make sure the fetching operation keeps on running and does not stop in the middle. Let us try to understand the architecture with the help of an example. Suppose we aim to fetch the entire collection of papers written on covid\u201319; we will use the command to input our keyword covid \u2013 19, and the algorithm will use research to search for all the papersstored in the PubMed database [ ,  ]. Based on a request. Get started putting together our search answer right now. When we are finished with this search answer, make search soup. After finding all the papers of the targeted keyword or field, the package will start to fetch the papers in batches with all the details like PMID, title, author name, and affiliation (it is the information related to the source of the paper, e.g., university, journals), abstract of the paper in a structured [ ]. The papers will be fetched into batches of 300 papers at once with the help of the APIs we have defined. The response for the fetch is in XML. The final response is collected after the response is in the dictionary format. However, we programmed the package to normalize the affiliations of the paper because affiliation is in the full-text format. By increasing the size of the dataset, we may transform the response into a data frame. After the data had been converted into a data frame, the normalized author function was implemented, which helped eliminate the ambiguity in the author\u2019s name\u2014after that, finally used the affiliation normalization technique to normalize the affiliation into the different sections (department, school, institute and country, email, location, zip code) [ ]. \n\n\n\n#### Affiliation Extraction Code Explanation \n  \nThe explanation of the affiliation extraction code is as follows.   \n Import Libraries  : Code starts with the essential importing packages like BeautifulSoup, requests, XML, dotenv, JSON, pandas, etc. BeautifulSoup is a python package mainly used to fetch data from HTML and XML files. It integrates with your preferred parser to provide you idiomatic access to the parse tree for navigation, searching, and editing. Typically, it saves programmers hours or days of effort. We have used the \u201crequests python package\u201d for request HTTP requests using python. XML documents can be parsed in Python with the help of two modules: XML.etree.ElementTree and Minidom (Minimal DOM Implementation). To \u201cparse\u201d an XML file, you must first read the file and then divide the data into individual tokens based on the characteristics of the XML document. To store the system environment variable, we have used the python dotenv package. JSON (JavaScript Object Notation) is used to encode and decode JSON data. \n  \n Driver Function  : In the driver function, create a \u201cfunction\u201d to take \u201cPMID\u201d or \u201cList of PMID\u201d or \u201cKeywords\u201d or \u201cauthor name\u201d as input. Then inside this function, declared another function called the author detail in which the pmid item (single pmid, list of pmid and keyword) passes in an iteration and stores the output in the database in a structural manner. \n  \n Main Function  : In the author detail function pmid as a keyword, then research variable that stores the access of the NLM databases. PubExN uses NLM\u2019s NCBI PubMed database through eutils API. Then prepare a search response with a combination of research, keyword, and API key. Then create a search soup by search response and XML features. if search soup has any value function, the process goes further; otherwise, it terminates the program. Once we got the value in the \u201cwebenv\u201d to find the text from the search soup. Then create a batch to fetch all the article information which we have given in the pmid item. Because there is a limit of 300 articles at once, so we have created maximum of 2000 batches. When the 2001 batch comes next iteration will be started. Then with efetch PubExN fetch all the information one by one. From the efetch we have extracted \u201carticle title\u201d, \u201cpmid\u201d, \u201cauthorlist\u201d, \u201cabstract\u201d, \u201cauthor\u201d, \u201cforename\u201d, \u201clastname\u201d, \u201cinitials\u201d and, \u201caffiliation\u201d. In each iteration it goes through try and except; if it matches it goes to try otherwise it goes to except block. Finally, the value which we have gotten from the process is stored in the dictionary. After that dictionary convert into pandas data frame to store in the normalized form in Mongo or postgres SQL database. \n  \n\n\n\n### Affiliation Normalization Process \n  \nAlgorithm 2 shows step wise step process of the affiliation normalization process. The algorithm starts with downloading the GRID database. Global Research Identifier Database (GRID), an open database of one-of-a-kind research-related organisation identifiers that the company had developed in-house over the course of several years and made available for public use by the research community. If the Grid dataset is available, it shows function will continue with the further process otherwise first it downloads the dataset and then move to the further functionality. In the second step, package preprocess the full-text affiliation by applying the RegEx methods (A regular expression technique has been with the help of the python RegEx method. We did not use any context-sensitive grammar for designing the PubExN. A simple rule-based approach has been used for exact matching to word matching has been applied). In the third step, it extracts various components by a parsing method. \n\nAlgorithm 3 is the driver function of the functions defined in Algorithm 2. \n\n\n\n#### Code Explanation of Affiliation Normalization \n  \n  \n Importing Packages:   The program starts with important packages such as re, string unidecode, and NumPy required in the affiliation parser. \u201cre\u201d stands for a regular expression known as RegEx can be used to check if a string contains the specified search pattern. Unidecode. This library provides functions to transliterate Unicode characters to an ASCII approximation. Unidecode. Decoder. This module takes care of transliterating a single grapheme. It\u2019s only documented so you can use a better strategy to transliterate larger texts. NumPy (short for \u201cNumerical Python\u201d) is a package that contains objects representing multidimensional arrays and a set of procedures for working with those arrays. NumPy performs mathematical and logical array operations. \n  \n Important Function Explanation:   Affiliation parser required important function like \u201creplace institution abbreviation\u201d, \u201cappend institute city\u201d, \u201cclean text\u201d, \u201cfind country\u201d, \u201ccheck country\u201d, \u201cparse email\u201d, \u201cparse zipcode\u201d and \u201cparse location\u201d to complete the parsing process of affiliation parser. In \u201creplace institution abbreviation\u201d, the abbreviation is replaced by its full-text institution name. Suppose we have an abbreviation \u201cNITRR\u201d so it replaces NITRR to \u201cNational Institute of Technology Raipur\u201d. We used GRID database for the exact matching of abbreviations. Then in \u201cappend institution city\u201d function we append the city to the university that has multiple campus if exist. In \u201cclean text\u201d cleans the text of the affiliation text with the abbreviation. If there is any abbreviation i.e., \u201cDept.\u201d it will replace it by \u201cDepartment\u201d similarly if the parser \u201cUniv.\u201d so it replaces with \u201cUniversity\u201d and so on. \u201cfind country\u201d function is used to find the country from the string. suppose in the full-text article like \u201cIndian Institute of Technology Delhi, India\u201d so this function will separate the country name from the full text. Country function check if any states sting from USA or UK. \u201cparse email\u201d function parse email and separate from the full-text article. \u201cParse zipcode\u201d function parses the zipcode from the full-text affiliation. \u201cparse location\u201d function parse state and country from the full-text affiliation. \n  \n Driver Function:   \u201cparse affil\u201d is the driver function and collectively call all the above-defined function and create a dictionary once the dictionary is completed then convert it to the pandas dataframe to store the normalized data in the MongoDb or any SQL databases. \n  \n\n\n\n\n## Results \n  \nThe package we have developed, PubExN, for extracting data from NCBI\u2019s PubMed database for retrieving the research article, has an immense impact on Biomed, medical researchers and students. Although applying this batch approach will help them directly extract the entire text surrounding their area of interest by just using the keyword. \n\nIn addition, the result obtained using the PubExN package is underlined in this subsection. \n\n### Article Extraction Results \n  \nThe article extraction process starts with importing the package into the pro gram\u2019s main module. Select the option based on user requirements to get the desired output. In the first option, if the use-case needed is \u201csearch by keyword,\u201d then pass the keyword and can get all the associated results from that particular keyword. \n\nSelect the option \u201ca\u201d then pass any keyword. We take \u201ccovid-19\u201d as a keyword example first, it gives the total number of articles available with the associated keyword in the PubMed database, and then it produces output in the form of tabulation. Figure\u00a0  shows the extracted abstract from the following PMIDs [ \u2013 ].   \nOutput of the keyword search \n  \n\nUsers or researcher can also search for single article information by passing a single \u201cPMID\u201d of the article. In that case user import the PubMed extract and select option \u201cb\u201d. The function asks to take input in the form of PMID; if the PMID is valid, it will produce the output. \n\nThe output was generated in the form of tabulation, by taking the example PMID [ ] and the results appear as shown in Fig.\u00a0 :   \nPubMed article count by \u201ccovid-19\u201d keyword \n  \n\nIf the requirement of extraction of multiple PMIDs, then choose the option \u201cc\u201d. It takes multiple PMIDs in a list format and passes the list items one by one to take multiple inputs. Multiple PMIDs article information and author details are produced in the tabulated format. The results seem as by taking the example PMIDs: \n\nThe compiled results of multiple \u201cPMID\u201d entries [ \u2013 ] shows in Fig.\u00a0 :   \nPubMed article count by \u201ccovid-19\u201d keyword \n  \n\n\n### Affiliation Normalization Results \n  \nAffiliation normalization module plays an important role in generating structured data. Whenever any use-case requires affiliation normalization in the module, they can easily install and import the pubmed extract package and can use the functionality of affiliation normalization in their module. \n\nAfter successful import of the package then select the choice \u201cd\u201d and enter the full text affiliation. It will generate the normalized affiliation from the full text. Normalized full-text affiliation break into \u201cdepartment\u201d, \u201cinstitution\u201d, \u201cemail\u201d, \u201czip code\u201d, \u201clocation\u201d, \u201ccountry\u201d and it store in the dictionary with the same keys. The final output seems like this: \n\n\n### Reducing Batch Timing \n  \nSome significant delays we have noticed in the PubExN because the proposed software package uses a batch technique, and these methods suffer delay when the process is increases. We have tested the package using one parallelization technique called RAY. Ray is an open-source parallel distributed method. We have tested our package with ray and found a drastic time reduction in the extraction process. Undoubtedly batch approach is a time-consuming process, but if we use it with parallel or distribution methods, it performs well. \n\nModern applications use parallel and distributed computing. To make efficient or to run them on a larger scale, we will need to make use of numerous cores on different machines. The infrastructure that is responsible for crawling the biomedical literature responding to search queries is not a collection of single-threaded applications that are executed on someone\u2019s rather a collection of services that communicate and interact with one another. \n\nRay has the following advantages which are as follows.   \nRay provides the facility to run the same code with multiple machines. Although we have not run the code on multiple machines, we have tested PubExN on a single machine with multiple cores, and for 5 million records, it took around 13\u00a0h. \n  \nBuilding micro-services and actors that have a state and can communicate. \n  \nIt can also handle network or service failure. In case of any failure, if the code is stopped in between whenever the services resume, it starts from that point only. \n  \nIt can efficiently handle large objects and numeric data. \n  \n\n\n### Comparison with the Existing Methods \n  \n \nTable   shows the comparison of the proposed work with existing techniques used to extract the raw data from biomedical databases. The proposed package provided the facility to normalize the raw biomedical text, and also PubExN provides the facility to further normalize author-named and affiliation normalization. \n   \nProposed model comparison with the existing method \n  \n\n\n### Execution Time \n  \nExecution time and the search time both gradually increase as the number of articles increases in PubExN. Table   shows the execution time of the PubExN for a particular number of articles. The execution time includes searching the article information from the PubMed database, normalization process and dumping the extracted data into local database (MySQL/SOLR).   \nExecution Time of the PubExN for the given articles \n  \n\n\n### Searching Time \n  \nTable   shows the time taken by the PubExN for searching the articles from the PubMed database. The time taken by searching the articles is very less because PubExN used API based searching technique which reduces the searching time of the article from the PubMed database.   \nSearching time of the article from the PubMed database \n  \n\n\n### Application of the Proposed Model \n  \nThe PubExN is an essential biomedical text-mining tool that can be used in almost every biomedical application. The following are the most used application by PubExN.   \n Biomedical Named Entity Recognition:   Extraction of a biomedical entity from biomedical literature is a significant task of biomedical named entity recognition (BioNER). Training a BioNER model requires substantial biomedical literature. This bio literature is fetched through the biomedical literature databases (PubMed, PubMed Central). Extraction from these databases is a complicated and hectic process. The PubExN provides flexibility to extract as much raw data from these biomedical literature databases. \n  \n Biomedical Entity Relation Extraction:   Similarly, to find the relation between the biomedical entities large volume of the biomedical structural database is required. PubExN brings down the extra efforts done in the extraction process and is also helpful in reducing the time of the extraction process. \n  \n Author Name Disambiguation Problem:   The Author Name Disambiguation problem as one of the challenging tasks of biomedical text mining. Some- times an author writes intentionally or unintentionally a different name in the articles that time identification of the authorship might be difficult. To solve this problem Microsoft research team is currently working on the Author Name Disambiguation problem. In this problem, the researcher extracts data from a biomedical database like PubMed, PubMed Central, and Some Patent, and Grant databases. Basically, the researcher extracted author information, citation information, a keyword from the abstract, and material methods to check the similarities between the articles to identify the exact author. \n  \n Literature-Based Knowledge Discovery:   In biomedical text mining, literature-based knowledge discovery (LBD) is one of the significant applications of the biomedical domain. In LBD, the title, author information, citation information, abstract and material, and method section are extracted from biomedical databases like PubMed, then tried to find the relations between similar articles. \n  \n\n\n\n## Conclusion \n  \nThis package has been designed to automate the PubMed article data extraction process with affiliation-level normalization. It also follows open-source development and rigorous operational testing and validation standards. We have designed comprehensive documentation with step-wise step guidance to use this package to further aid researchers or users. We envision that this package will benefit a wide range of biomedical text mining development and could aid in the further development of the PubMed-based bulk extraction process. In the future, a deep learning model can be trained to normalized the affiliation information using the Named Entity Recognition model (NER) so that affiliation information can be extracted from these databases. The latest version of the python package is available on PyPI Website  . You can easily install the package using the pip command locally accordingly to your data extraction need. \n\n \n", "metadata": {"pmcid": 10132428, "text_md5": "36b1b760a91d595145e0ae79d2ac9955", "field_positions": {"authors": [0, 38], "journal": [39, 52], "publication_year": [54, 58], "title": [69, 158], "keywords": [172, 284], "abstract": [297, 1958], "body": [1967, 32464]}, "batch": 1, "pmid": 37128512, "doi": "10.1007/s42979-023-01687-3", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10132428", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=10132428"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10132428\">10132428</a>", "list_title": "PMC10132428  PubExN: An Automated PubMed Bulk Article Extractor with Affiliation Normalization Package"}
